[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nujoum Unus",
    "section": "",
    "text": "Nujoum Unus is an MS Business Analytics graduate from UC San Diego who designs analytics and GenAI solutions with real business impact. She translates complex data into actionable insights through machine learning, NLP, and thoughtful product design."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Nujoum Unus",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego | San Diego, CA MS Business Analytics | Aug 2024 - Dec 2025\nA.P.J. Abdul Kalam Technological University | Kerala, India B.Tech in Computer Science and Engineering | July 2019 - June 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Nujoum Unus",
    "section": "Experience",
    "text": "Experience\nBriteCap Financial | Data Science Capstone | April 2025 - June 2025\nAutomation Engineer | TATA ELXSI | Nov 2023 - May 2024"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "WELCOME Y‚ÄôALL"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nto do: expand on the description of the experiment.\n Background \nCharitable organizations often rely on fundraising letters to solicit donations, but little rigorous evidence has been available to guide how those letters should be designed. In a groundbreaking field experiment, economists Dean Karlan and John List set out to test how different framing strategies and financial incentives affect individual donation behavior.\nThe experiment, conducted in collaboration with a politically-oriented nonprofit organization, involved mailing 50,083 fundraising letters to previous donors. Crucially, the recipients were randomly assigned to different treatment groups, allowing the researchers to measure causal effects rather than mere correlations.\n Purpose of the Study \nKarlan and List aimed to answer a simple but important question:\n&gt; Do people give more when their donation is matched? And if so, does the size of the match matter?\nThey also explored additional behavioral levers commonly used in fundraising, such as challenge framing, suggested donation amounts, and goal-based appeals.\n Experimental Design \nThe letters fell into three broad treatment types:\n\nStandard Fundraising Letter (Control)\n\nA typical letter requesting support for the organization, with no additional incentives or matching language.\n\nMatching Grant Letter\n\nIncluded a paragraph stating that a leadership donor would match any contribution at one of three possible ratios:\n\n1:1 (every dollar given is doubled)\n\n2:1 (every dollar is tripled)\n\n3:1 (every dollar quadrupled)\n\n\nMatching offers also varied by threshold, i.e., the maximum amount the leadership donor would match:\n\n$25,000, $50,000, $100,000, or unstated.\n\nSuggested donation levels were tailored based on each recipient‚Äôs previous giving history:\n\nTheir highest previous gift\n\n1.25√ó their highest gift\n\n1.5√ó their highest gift\n\n\nChallenge Grant Letter\n\nFramed the offer as part of a collective effort or campaign challenge, appealing to urgency and social impact rather than pure match mechanics.\n\n\nBecause each component (match ratio, threshold, suggested donation amount) was randomized independently within the matching grant group, the experiment had a factorial design ‚Äî allowing the researchers to isolate and measure the effects of each variable.\n Why This Matters \nAt the time of the study, fundraisers often relied on rules of thumb and anecdotes, lacking hard data on what actually drives giving. Karlan and List‚Äôs approach brought scientific rigor to the domain of nonprofit fundraising by:\n\nLeveraging random assignment to establish causality\nTesting commonly used marketing strategies under real-world conditions\nGenerating insights with practical implications for organizations seeking to raise more money\n\n Contribution to the Literature \nThis study represents one of the first large-scale natural field experiments in charitable giving. It moved beyond lab settings and survey experiments to observe real decisions involving real money. The results helped bridge the gap between behavioral economics and fundraising practice, offering evidence-backed recommendations on:\n\nThe efficacy of matching offers\n\nHow much match ratios influence behavior\n\nWhether people respond to thresholds or suggested amounts\n\nThe heterogeneous effects by donor characteristics and geography\n\n Project Overview \nIn this replication study, we use the same dataset provided by Karlan and List to:\n\nReproduce their key findings\nValidate the statistical robustness of their claims\nExplore new visualizations and simulations that illuminate the behavioral mechanisms at play\nReflect on what this experiment teaches us about human motivation, social framing, and economic incentives in the context of public goods\n\nThis report follows a structured analysis of donation likelihood, donation size, and how different dimensions of the match offer (ratio, threshold, framing) influence both."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nto do: expand on the description of the experiment.\n Background \nCharitable organizations often rely on fundraising letters to solicit donations, but little rigorous evidence has been available to guide how those letters should be designed. In a groundbreaking field experiment, economists Dean Karlan and John List set out to test how different framing strategies and financial incentives affect individual donation behavior.\nThe experiment, conducted in collaboration with a politically-oriented nonprofit organization, involved mailing 50,083 fundraising letters to previous donors. Crucially, the recipients were randomly assigned to different treatment groups, allowing the researchers to measure causal effects rather than mere correlations.\n Purpose of the Study \nKarlan and List aimed to answer a simple but important question:\n&gt; Do people give more when their donation is matched? And if so, does the size of the match matter?\nThey also explored additional behavioral levers commonly used in fundraising, such as challenge framing, suggested donation amounts, and goal-based appeals.\n Experimental Design \nThe letters fell into three broad treatment types:\n\nStandard Fundraising Letter (Control)\n\nA typical letter requesting support for the organization, with no additional incentives or matching language.\n\nMatching Grant Letter\n\nIncluded a paragraph stating that a leadership donor would match any contribution at one of three possible ratios:\n\n1:1 (every dollar given is doubled)\n\n2:1 (every dollar is tripled)\n\n3:1 (every dollar quadrupled)\n\n\nMatching offers also varied by threshold, i.e., the maximum amount the leadership donor would match:\n\n$25,000, $50,000, $100,000, or unstated.\n\nSuggested donation levels were tailored based on each recipient‚Äôs previous giving history:\n\nTheir highest previous gift\n\n1.25√ó their highest gift\n\n1.5√ó their highest gift\n\n\nChallenge Grant Letter\n\nFramed the offer as part of a collective effort or campaign challenge, appealing to urgency and social impact rather than pure match mechanics.\n\n\nBecause each component (match ratio, threshold, suggested donation amount) was randomized independently within the matching grant group, the experiment had a factorial design ‚Äî allowing the researchers to isolate and measure the effects of each variable.\n Why This Matters \nAt the time of the study, fundraisers often relied on rules of thumb and anecdotes, lacking hard data on what actually drives giving. Karlan and List‚Äôs approach brought scientific rigor to the domain of nonprofit fundraising by:\n\nLeveraging random assignment to establish causality\nTesting commonly used marketing strategies under real-world conditions\nGenerating insights with practical implications for organizations seeking to raise more money\n\n Contribution to the Literature \nThis study represents one of the first large-scale natural field experiments in charitable giving. It moved beyond lab settings and survey experiments to observe real decisions involving real money. The results helped bridge the gap between behavioral economics and fundraising practice, offering evidence-backed recommendations on:\n\nThe efficacy of matching offers\n\nHow much match ratios influence behavior\n\nWhether people respond to thresholds or suggested amounts\n\nThe heterogeneous effects by donor characteristics and geography\n\n Project Overview \nIn this replication study, we use the same dataset provided by Karlan and List to:\n\nReproduce their key findings\nValidate the statistical robustness of their claims\nExplore new visualizations and simulations that illuminate the behavioral mechanisms at play\nReflect on what this experiment teaches us about human motivation, social framing, and economic incentives in the context of public goods\n\nThis report follows a structured analysis of donation likelihood, donation size, and how different dimensions of the match offer (ratio, threshold, framing) influence both."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\nimport pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\ndf.info()\ndf.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows √ó 48 columns\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\nWe tested whether the treatment and control groups differed in prior donor behavior by comparing the number of months since last donation (mrm2). Both a two-sample t-test (t = 0.120, p = 0.905) and a linear regression of mrm2 ~ treatment (Œ≤ = 0.0137, p = 0.905) confirm no statistically significant difference. This supports the randomization mechanism and matches Table 1 in Karlan and List (2007).\n\nfrom scipy import stats\n\n# Clean data: drop NAs\ndf_clean = df[[\"mrm2\", \"treatment\", \"control\"]].dropna()\n\n# Split groups\ntreat = df_clean[df_clean['treatment'] == 1]['mrm2']\ncontrol = df_clean[df_clean['control'] == 1]['mrm2']\n\n# Perform Welch's t-test (no assumption of equal variances)\nttest = stats.ttest_ind(treat, control, equal_var=False)\n\n# Print results\nprint(f\"T-test result: t = {ttest.statistic:.3f}, p = {ttest.pvalue:.3f}\")\n\nimport statsmodels.formula.api as smf\n\n# Regression of mrm2 on treatment\nmodel = smf.ols(\"mrm2 ~ treatment\", data=df_clean).fit()\nmodel.summary()\n\nT-test result: t = 0.120, p = 0.905\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmrm2\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.01428\n\n\nDate:\nWed, 23 Apr 2025\nProb (F-statistic):\n0.905\n\n\nTime:\n22:29:36\nLog-Likelihood:\n-1.9585e+05\n\n\nNo. Observations:\n50082\nAIC:\n3.917e+05\n\n\nDf Residuals:\n50080\nBIC:\n3.917e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.9981\n0.094\n138.979\n0.000\n12.815\n13.181\n\n\ntreatment\n0.0137\n0.115\n0.119\n0.905\n-0.211\n0.238\n\n\n\n\n\n\n\n\nOmnibus:\n8031.352\nDurbin-Watson:\n2.004\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n12471.135\n\n\nSkew:\n1.163\nProb(JB):\n0.00\n\n\nKurtosis:\n3.751\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n Interpretation \nWe assess balance between treatment and control groups using both statistical methods:\n\nT-Test:\n\nt = 0.120, p = 0.905\n\nResult: Not statistically significant\n\nRegression:\n\nCoefficient on treatment ‚âà 0.014\n\np-value ‚âà 0.905\n\n95% Confidence Interval: Includes zero\n\n\nThese results confirm no significant difference in mrm2 across groups, supporting the randomization mechanism. This aligns with Table 1 in Karlan & List (2007), where the group means were:\n\n\n\nGroup\nMean Months Since Last Donation\n\n\n\n\nTreatment\n13.012\n\n\nControl\n12.998"
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\n\nimport matplotlib.pyplot as plt\ngave_by_group = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ngave_by_group[\"group\"] = gave_by_group[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\nplt.figure(figsize=(6, 4))\nplt.bar(gave_by_group[\"group\"], gave_by_group[\"gave\"], width=0.5)\nplt.title(\"Proportion Who Donated by Group\")\nplt.ylabel(\"Proportion Gave\")\nplt.ylim(0, gave_by_group[\"gave\"].max() + 0.01)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe compare the response rate (i.e., whether a donation was made) between treatment and control groups.\nThe bar plot below shows that the treatment group, who received a matching grant offer, donated at a higher rate than the control group, who received a standard letter.\nThis visual confirms the core finding in Karlan & List (2007): matching donations increased participation in charitable giving.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Ensure binary outcome is correctly typed\ndf['gave'] = df['gave'].astype(int)\n\n# T-test: response rate (gave) between treatment and control groups\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_control = df[df['control'] == 1]['gave']\nt_stat, p_val = ttest_ind(gave_treat, gave_control, equal_var=False)\n\n# Regression: response as a function of treatment\nreg_gave = smf.ols('gave ~ treatment', data=df).fit()\n\nt_stat, p_val, reg_gave.summary()\n\n(3.2094621908279835,\n 0.0013309823450914173,\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                   gave   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     9.618\n Date:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\n Time:                        22:29:36   Log-Likelihood:                 26630.\n No. Observations:               50083   AIC:                        -5.326e+04\n Df Residuals:                   50081   BIC:                        -5.324e+04\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept      0.0179      0.001     16.225      0.000       0.016       0.020\n treatment      0.0042      0.001      3.101      0.002       0.002       0.007\n ==============================================================================\n Omnibus:                    59814.280   Durbin-Watson:                   2.005\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\n Skew:                           6.740   Prob(JB):                         0.00\n Kurtosis:                      46.440   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n Charitable Contribution Made \nWe examine whether receiving a matching donation offer increases the likelihood of making a charitable donation. This is measured using the binary variable gave, which equals 1 if a donation was made and 0 otherwise.\nWe use both a t-test and a bivariate linear regression to assess the difference in response rate between treatment and control groups.\n Results Summary \n\n\n\n\n\n\n\n\n\nMethod\nEffect Size\np-value\nInterpretation\n\n\n\n\nT-Test\nt = 3.21\n0.0013\nStatistically significant\n\n\nRegression\n+0.0042 (0.42%)\n0.002\nStatistically significant\n\n\n\n\nControl group donation rate ‚âà 1.79%\n\nTreatment group donation rate ‚âà 2.21%\n\nThese values match the response rates reported in Table 2A, Panel A of Karlan & List (2007).\n Table 2A (Panel A): Response Rate Comparison \n\n\n\nGroup\nResponse Rate\nStd. Error\n\n\n\n\nControl\n0.018\n(0.001)\n\n\nTreatment\n0.022\n(0.001)\n\n\nMatch 1:1\n0.021\n(0.001)\n\n\nMatch 2:1\n0.023\n(0.001)\n\n\nMatch 3:1\n0.023\n(0.001)\n\n\n\n\nSource: Karlan & List (2007), Table 2A, Panel A\n\n Interpretation \nEven a small increase in the likelihood of giving ‚Äî about 0.4 percentage points ‚Äî is statistically significant in a large-scale field experiment with over 50,000 individuals.\nThis result shows that: - Matching donations have a causal impact on behavior. - People are more likely to respond to donation appeals when told their gift will be matched. - The psychological effect (e.g., feeling of leverage, social validation, urgency) may be as important as the financial incentive.\nThus, matched donations are an effective strategy not just in economics but in behavioral design for charitable fundraising.\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nimport statsmodels.api as sm\n\n# Prepare the variables\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"gave\"]\n\n# Run the Probit regression\nprobit_model = sm.Probit(y, X)\nprobit_results = probit_model.fit()\n\nprobit_results.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nWed, 23 Apr 2025\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n22:29:37\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n Probit Regression: Impact of Matching Grant on Donation Likelihood \nTo replicate Table 3, Column (1) from Karlan & List (2007), we estimate a Probit model where the outcome is whether a donation was made (gave = 1) and the explanatory variable is assignment to treatment (treatment = 1).\n Our Probit Model Results \n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI\n\n\n\n\nIntercept\n-2.100\n0.023\n-90.07\n&lt; 0.001\n[-2.146, -2.054]\n\n\nTreatment\n0.087\n0.028\n3.11\n0.002\n[0.032, 0.141]\n\n\n\n\nPseudo R¬≤: 0.001\n\nObservations: 50,083\n\nThese results match the direction and significance of Table 3, Column (1) in the original study.\n Table 3: Primary Probit Regression Results from Karlan & List (2007) \n\n\n\n\n\n\n\n\n\nVariable\n(1) All\nStd. Err.\nSignificance\n\n\n\n\nTreatment\n0.004\n(0.001)\n***\n\n\nTreatment √ó 2:1 ratio\n0.002\n(0.002)\n\n\n\nTreatment √ó 3:1 ratio\n0.002\n(0.002)\n\n\n\nTreatment √ó $25,000 threshold\n-0.001\n(0.002)\n\n\n\nTreatment √ó $50,000 threshold\n0.000\n(0.002)\n\n\n\nTreatment √ó $100,000 threshold\n-0.000\n(0.002)\n\n\n\nTreatment √ó medium example amount\n0.001\n(0.002)\n\n\n\nTreatment √ó high example amount\n0.001\n(0.002)\n\n\n\nPseudo R¬≤\n0.001\n\n\n\n\nObservations\n50,083\n\n\n\n\n\nNotes: - The paper reports marginal effects, whereas our Probit output gives latent index coefficients. - The magnitude of 0.004 in the paper corresponds to a marginal increase in probability of donating due to the treatment. - Our coefficient of 0.087 reflects the effect on the underlying propensity to give, which is standard in Probit estimation.\n Interpretation \nDespite a small effect size, the impact of being offered a matching donation is statistically significant. This suggests:\n\nEven subtle nudges, like framing a gift as matched by a leadership donor, can increase participation.\nThe result is economically meaningful due to the large sample size and real-world behavioral context.\n\nIn short: human generosity is sensitive to framing ‚Äî and donors are more likely to act when they feel their gift has leverage.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the ‚Äúfigures suggest‚Äù comment the authors make on page 8?\n\n# Subset the data for different match ratios\n# According to the dataset: ratio = '1', '2', '3' for 1:1, 2:1, 3:1\ndf_ratio = df[df[\"treatment\"] == 1].copy()\ndf_ratio[\"ratio\"] = df_ratio[\"ratio\"].astype(str)\n\n# Extract binary 'gave' for each match ratio group\ngave_1_1 = df_ratio[df_ratio[\"ratio\"] == \"1\"][\"gave\"]\ngave_2_1 = df_ratio[df_ratio[\"ratio\"] == \"2\"][\"gave\"]\ngave_3_1 = df_ratio[df_ratio[\"ratio\"] == \"3\"][\"gave\"]\n\n# Perform t-tests between match ratio groups\nttest_1_vs_2 = ttest_ind(gave_1_1, gave_2_1, equal_var=False)\nttest_1_vs_3 = ttest_ind(gave_1_1, gave_3_1, equal_var=False)\nttest_2_vs_3 = ttest_ind(gave_2_1, gave_3_1, equal_var=False)\n\nttest_1_vs_2, ttest_1_vs_3, ttest_2_vs_3\n\n(TtestResult(statistic=-0.965048975142932, pvalue=0.33453078237183076, df=22225.07770983836),\n TtestResult(statistic=-1.0150174470156275, pvalue=0.31010856527625774, df=22215.0529778684),\n TtestResult(statistic=-0.05011581369764474, pvalue=0.9600305476940865, df=22260.84918918778))\n\n\n Does Match Ratio Size Affect Donation Rates? \nWe investigate whether increasing the match ratio (from 1:1 to 2:1 to 3:1) has a statistically significant effect on the likelihood that someone donates.\nTo do this, we run a series of t-tests comparing donation rates (gave = 1) across match ratio groups, restricting the sample to individuals who received a matching offer.\n\n T-Test Results by Match Ratio \n\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\nInterpretation\n\n\n\n\n1:1 vs 2:1 match\n-0.965\n0.335\n‚ùå Not statistically significant\n\n\n1:1 vs 3:1 match\n-1.015\n0.310\n‚ùå Not statistically significant\n\n\n2:1 vs 3:1 match\n-0.050\n0.960\n‚ùå Not statistically significant\n\n\n\n Interpretation \nThese results show no significant difference in donation rates across the different match ratios. This means that:\n\nIncreasing the match multiplier from 1:1 to 2:1 or 3:1 does not lead to a higher likelihood of giving.\nThis supports the statement from Karlan & List (2007, p.¬†8):\n\n\n‚ÄúThe gift distributions across the various matching ratios are not significantly different from one another.‚Äù\n\nIn other words, people respond positively to the existence of a match, but not necessarily more when the match becomes more generous.\n Conclusion \nThe presence of a match appears to matter more than its magnitude. This suggests that:\n\nFraming and social cues ‚Äî like simply saying ‚Äúyour gift will be matched‚Äù ‚Äî may be more behaviorally powerful than the precise financial terms.\n\nThis insight is important for nonprofit fundraisers: focus on highlighting the match rather than inflating the ratio.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\n# Ensure 'gave' is binary\ndf['gave'] = df['gave'].astype(int)\n\n# Create dummy variables for each match ratio\n# This is only for treatment group, so filter and prepare accordingly\ndf_ratio = df[df['treatment'] == 1].copy()\ndf_ratio['ratio'] = df_ratio['ratio'].astype(str)\n\n# Create dummy variables: ratio1, ratio2, ratio3\ndf_ratio['ratio1'] = (df_ratio['ratio'] == '1').astype(int)\ndf_ratio['ratio2'] = (df_ratio['ratio'] == '2').astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == '3').astype(int)\n\n# Regression: gave ~ ratio1 + ratio2 + ratio3 (no intercept)\nimport statsmodels.api as sm\n\nX = df_ratio[['ratio1', 'ratio2', 'ratio3']]\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\n\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6454\n\n\nDate:\nWed, 23 Apr 2025\nProb (F-statistic):\n0.524\n\n\nTime:\n22:29:37\nLog-Likelihood:\n16688.\n\n\nNo. Observations:\n33396\nAIC:\n-3.337e+04\n\n\nDf Residuals:\n33393\nBIC:\n-3.334e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nratio1\n0.0207\n0.001\n14.912\n0.000\n0.018\n0.023\n\n\nratio2\n0.0226\n0.001\n16.267\n0.000\n0.020\n0.025\n\n\nratio3\n0.0227\n0.001\n16.335\n0.000\n0.020\n0.025\n\n\n\n\n\n\n\n\nOmnibus:\n38963.957\nDurbin-Watson:\n1.995\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2506478.937\n\n\nSkew:\n6.511\nProb(JB):\n0.00\n\n\nKurtosis:\n43.394\nCond. No.\n1.00\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n Behavioral Insight: Why Match Size Doesn‚Äôt Matter (Much) \nThis regression shows that all forms of match ratios ‚Äî 1:1, 2:1, and 3:1 ‚Äî significantly increase the likelihood that someone donates, with donation rates clustering around 2%.\nHowever, the differences between match sizes are extremely small:\n\nPeople who saw a 1:1 match donated at a rate of 2.07%.\nThose who saw a 2:1 match gave at 2.26%.\nWith a 3:1 match, the rate was 2.27%.\n\nThese results suggest that once a match is present, increasing its generosity has little additional impact. In other words:\n\nIt‚Äôs the existence of the match that matters, not its size.\n\nThis behavior aligns with theories in behavioral economics: - The match acts as a signal of social proof or endorsement. - It may create a sense of urgency or leverage (‚Äúmy donation matters more‚Äù). - But donors aren‚Äôt particularly sensitive to how generous the match is ‚Äî at least not in terms of deciding whether or not to give.\n Implication for Fundraising \nFrom a practical standpoint, this means that: - Fundraisers don‚Äôt need to offer high match ratios to see results. - A simple, clearly communicated 1:1 match may be just as effective as a 3:1 match in increasing participation.\nThis finding reinforces the power of framing and perception in influencing human behavior.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n# Compute the actual mean response (gave) for each ratio group directly from the data\nmean_1_1 = df_ratio[df_ratio[\"ratio\"] == \"1\"][\"gave\"].mean()\nmean_2_1 = df_ratio[df_ratio[\"ratio\"] == \"2\"][\"gave\"].mean()\nmean_3_1 = df_ratio[df_ratio[\"ratio\"] == \"3\"][\"gave\"].mean()\n\n# Calculate differences in response rates\ndiff_2_1_vs_1_1 = mean_2_1 - mean_1_1\ndiff_3_1_vs_2_1 = mean_3_1 - mean_2_1\n\n# Extract coefficients from regression model\ncoef_1_1 = model.params[\"ratio1\"]\ncoef_2_1 = model.params[\"ratio2\"]\ncoef_3_1 = model.params[\"ratio3\"]\n\n# Calculate differences in coefficients\ncoef_diff_2_1_vs_1_1 = coef_2_1 - coef_1_1\ncoef_diff_3_1_vs_2_1 = coef_3_1 - coef_2_1\n\n(mean_1_1, mean_2_1, mean_3_1,\n diff_2_1_vs_1_1, diff_3_1_vs_2_1,\n coef_diff_2_1_vs_1_1, coef_diff_3_1_vs_2_1)\n\n(0.020749124225276205,\n 0.0226333752469912,\n 0.022733399227244138,\n 0.0018842510217149944,\n 0.00010002398025293902,\n 0.0018842510217149805,\n 0.00010002398025296677)\n\n\n Comparing Response Rates Across Match Ratios \nWe examine how the size of the match (1:1 vs.¬†2:1 vs.¬†3:1) influences the probability that an individual makes a donation. We do this in two ways:\n\nDirectly from the data by calculating average donation rates within each match group.\nFrom the fitted coefficients of a regression on dummy variables for each ratio.\n\n\n Response Rate Differences \n\n\n\n\n\n\n\n\nComparison\nDirect from Data\nFrom Regression Coefficients\n\n\n\n\n2:1 vs 1:1 match\n0.00188 (0.19%)\n0.00188 (0.19%)\n\n\n3:1 vs 2:1 match\n0.00010 (0.01%)\n0.00010 (0.01%)\n\n\n\n\nThese differences represent increases in the probability of donating when moving from one match ratio to a higher one.\nThe results are identical across both methods, which supports the robustness of the findings.\n\n Interpretation \n\nMoving from a 1:1 to 2:1 match slightly increases donation rates by about 0.19 percentage points.\nIncreasing from a 2:1 to a 3:1 match has a negligible effect ‚Äî only 0.01 percentage points.\nThese differences are statistically very small and are unlikely to be meaningful in practice.\n\n Conclusion \nOur analysis shows that:\n\nOnce a match is introduced, increasing the match ratio does not meaningfully increase the likelihood of giving.\n\nThis confirms the finding from Karlan & List (2007):\n\n‚ÄúThe gift distributions across the various matching ratios are not significantly different from one another.‚Äù\n\nIn short, it‚Äôs the presence of a match offer ‚Äî not its generosity ‚Äî that influences donor behavior.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\n# Run a t-test on the amount given between treatment and control groups\namount_treat = df[df['treatment'] == 1]['amount']\namount_control = df[df['control'] == 1]['amount']\namount_ttest = ttest_ind(amount_treat, amount_control, equal_var=False)\n\n# Run a bivariate linear regression: amount ~ treatment\namount_reg = smf.ols('amount ~ treatment', data=df).fit()\n\namount_ttest.statistic, amount_ttest.pvalue, amount_reg.summary()\n\n(1.9182618934467577,\n 0.05508566528918335,\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                 amount   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     3.461\n Date:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0628\n Time:                        22:29:37   Log-Likelihood:            -1.7946e+05\n No. Observations:               50083   AIC:                         3.589e+05\n Df Residuals:                   50081   BIC:                         3.589e+05\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept      0.8133      0.067     12.063      0.000       0.681       0.945\n treatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n ==============================================================================\n Omnibus:                    96861.113   Durbin-Watson:                   2.008\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\n Skew:                          15.297   Prob(JB):                         0.00\n Kurtosis:                     341.269   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n Size of Charitable Contribution \nWe tested whether receiving a matching donation offer affects the amount donated using a t-test and linear regression:\n Results \n\n\n\n\n\n\n\n\n\nMethod\nTreatment Effect\np-value\nConclusion\n\n\n\n\nT-Test\n+$0.15\n0.055\nüî∏ Marginally not significant\n\n\nRegression\n+$0.15\n0.063\nüî∏ Suggestive but inconclusive\n\n\n\n\nControl group average: ~$0.81\n\nTreatment group average: ~$0.96\n\n Interpretation \n\nThe treatment group gave slightly more, but the difference is not statistically significant at the 5% level.\nThis suggests that while match offers increase participation, they have a much smaller effect on how much people give.\n\n Takeaway \n\nMatching donations may encourage more people to give, but do not substantially increase donation size.\n\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients ‚Äì what did we learn? Does the treatment coefficient have a causal interpretation?\n\n# Limit the data to only those who made a donation (amount &gt; 0)\ndf_positive = df[df['amount'] &gt; 0].copy()\n\n# T-test for amount among donors only\namount_treat_pos = df_positive[df_positive['treatment'] == 1]['amount']\namount_control_pos = df_positive[df_positive['control'] == 1]['amount']\namount_ttest_pos = ttest_ind(amount_treat_pos, amount_control_pos, equal_var=False)\n\n# Regression: amount ~ treatment (for donors only)\namount_reg_pos = smf.ols('amount ~ treatment', data=df_positive).fit()\n\namount_ttest_pos.statistic, amount_ttest_pos.pvalue, amount_reg_pos.summary()\n\n(-0.5846089794983359,\n 0.5590471865673547,\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                 amount   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                 -0.001\n Method:                 Least Squares   F-statistic:                    0.3374\n Date:                Wed, 23 Apr 2025   Prob (F-statistic):              0.561\n Time:                        22:29:37   Log-Likelihood:                -5326.8\n No. Observations:                1034   AIC:                         1.066e+04\n Df Residuals:                    1032   BIC:                         1.067e+04\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept     45.5403      2.423     18.792      0.000      40.785      50.296\n treatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n ==============================================================================\n Omnibus:                      587.258   Durbin-Watson:                   2.031\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\n Skew:                           2.464   Prob(JB):                         0.00\n Kurtosis:                      13.307   Cond. No.                         3.49\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n Conditional Donation Amount: Among Donors Only \nTo isolate the effect of treatment on the amount given, we restrict the sample to only those individuals who made a donation (amount &gt; 0).\nWe use both a t-test and a bivariate regression (amount ~ treatment) to compare average donation sizes between treatment and control groups.\n Results Summary \n\n\n\n\n\n\n\n\n\nMethod\nTreatment Effect\np-value\nConclusion\n\n\n\n\nT-Test (donors only)\nt = -0.58\n0.559\n‚ùå Not statistically significant\n\n\nRegression\n-$1.67\n0.561\n‚ùå Not statistically significant\n\n\n\n\nControl group average donation: ~$45.54\n\nTreatment group average donation: ~$43.87\n\n Interpretation \n\nThe treatment group donated slightly less on average, but the difference is not statistically meaningful.\nThis suggests that while the match offer encourages more people to donate, it does not increase donation size among those who would give anyway.\nBecause we only include those who donated, the treatment effect here is not causal ‚Äî it‚Äôs conditional and may suffer from selection bias.\n\n Conclusion \n\nMatched donations are effective at increasing the number of donors, but not the amount donated by each donor ‚Äî at least among those who already choose to give.\n\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\n\nimport matplotlib.pyplot as plt\n\n# Filter to donors only\ndf_donors = df[df[\"amount\"] &gt; 0]\n\n# Separate treatment and control donors\ntreat_donors = df_donors[df_donors[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = df_donors[df_donors[\"control\"] == 1][\"amount\"]\n\n# Calculate means\nmean_treat = treat_donors.mean()\nmean_control = control_donors.mean()\n\n# Create side-by-side histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color=\"skyblue\", edgecolor=\"black\")\naxes[0].axvline(mean_control, color=\"red\", linestyle=\"--\", label=f\"Mean = ${mean_control:.2f}\")\naxes[0].set_title(\"Control Group Donations\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\n# Treatment group plot\naxes[1].hist(treat_donors, bins=30, color=\"lightgreen\", edgecolor=\"black\")\naxes[1].axvline(mean_treat, color=\"red\", linestyle=\"--\", label=f\"Mean = ${mean_treat:.2f}\")\naxes[1].set_title(\"Treatment Group Donations\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Distribution of Donation Amounts Among Donors \nWe now focus on individuals who actually made a donation (amount &gt; 0) to analyze how much they gave, and whether the treatment group (those offered a matching donation) gave more than the control group.\nWe visualize the distribution of donation amounts with two histograms ‚Äî one for each group ‚Äî and include a red dashed line indicating the average donation in each.\n Interpretation \n\nBoth distributions are heavily right-skewed, which is common in charitable giving: most donors give modest amounts, but a few give significantly more.\nThe average donation in the control group was about $45.54, while the treatment group averaged $43.87.\nThis difference is not statistically significant, as confirmed by both a t-test and a regression limited to donors.\n\n What Did We Learn? \n\nWhile the matching donation offer increases the probability of donating, it does not increase the donation amount among those who choose to give.\nIn fact, the average donation in the treatment group is slightly lower, though the difference is not meaningful.\n\n Important Caveat \nThis analysis is based only on people who gave, so the treatment coefficient does not have a causal interpretation here. This subset is not randomly assigned ‚Äî it‚Äôs a selected group, which may differ systematically between treatment and control.\n Fundraising Implication \n\nMatching offers are powerful tools to increase participation, but they do not necessarily lead to larger individual gifts.\n\nTo increase average donation size, fundraisers may need additional tactics ‚Äî such as suggested donation levels, tiered match thresholds, or social proof."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You‚Äôll then calculate a vector of 10,000 differences, and then you‚Äôll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Extract donation amounts for control and treatment\ncontrol_data = df[df[\"control\"] == 1][\"amount\"]\ntreatment_data = df[df[\"treatment\"] == 1][\"amount\"]\n\n# Simulate draws from each distribution\nnp.random.seed(42)\nsim_control = np.random.choice(control_data, size=100_000, replace=True)\nsim_treatment = np.random.choice(treatment_data, size=10_000, replace=True)\n\n# Calculate 10,000 differences between treatment and control draws\nsim_control_subset = np.random.choice(sim_control, size=10_000, replace=False)\ndiffs = sim_treatment - sim_control_subset\n\n# Compute cumulative average of differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Average Difference\")\nplt.axhline(y=np.mean(treatment_data) - np.mean(control_data), color=\"red\", linestyle=\"--\", label=\"True Mean Difference\")\nplt.title(\"Cumulative Average of Treatment-Control Differences\")\nplt.xlabel(\"Number of Draws\")\nplt.ylabel(\"Cumulative Average Difference in Donation Amount\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Simulated Cumulative Average Differences \nTo better understand the behavior of sample averages and connect to the concepts from our first class (Slide 43), we simulate the cumulative effect of donation differences between the treatment and control groups.\n Simulation Setup \n\nWe simulate 100,000 random draws from the control group donation distribution.\nWe simulate 10,000 random draws from the treatment group.\nFor each of the 10,000 pairs, we calculate the difference: treatment - control.\nWe then compute the cumulative average of these 10,000 differences.\n\n Plot Interpretation \nThe plot below shows:\n\nA blue line representing the cumulative average of the simulated differences.\nA red dashed line indicating the true difference in means between treatment and control groups (calculated from the full dataset).\n\nAs the number of draws increases, the cumulative average approaches the true difference.\nThis illustrates the Law of Large Numbers: with enough data, sample-based estimates converge to the population value.\n What We Learnt \n\nThis simulation confirms that even in noisy, skewed data like donations, repeated sampling yields reliable estimates.\n\nIt also demonstrates that the difference in means we compute from data is not just a fluke ‚Äî it‚Äôs what we‚Äôd expect if we sampled repeatedly from the same distributions.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g.¬†50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the ‚Äúmiddle‚Äù of the distribution or whether it‚Äôs in the ‚Äútail.‚Äù\n\n# Define a function to simulate mean differences for a given sample size\ndef simulate_differences(sample_size, n_reps=1000):\n    differences = []\n    for _ in range(n_reps):\n        sample_control = np.random.choice(control_data, size=sample_size, replace=True)\n        sample_treatment = np.random.choice(treatment_data, size=sample_size, replace=True)\n        differences.append(np.mean(sample_treatment) - np.mean(sample_control))\n    return differences\n\n# Simulate for each sample size\nnp.random.seed(42)\nsizes = [50, 200, 500, 1000]\nsimulated_results = {size: simulate_differences(size) for size in sizes}\n\n# Plot histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, size in enumerate(sizes):\n    axes[i].hist(simulated_results[size], bins=30, color='lightgray', edgecolor='black')\n    axes[i].axvline(0, color='red', linestyle='--', label=\"Zero\")\n    axes[i].set_title(f\"Sample Size = {size}\")\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Sampling Distributions at Different Sample Sizes \nTo mirror the exercise from Slide 44 of our first class, we simulate the sampling distribution of the mean difference in donation amount between the treatment and control groups.\nFor each of four different sample sizes ‚Äî 50, 200, 500, and 1000 ‚Äî we:\n\nDraw n observations from each group.\nCompute the difference in mean donation: treatment - control.\nRepeat the process 1,000 times.\nPlot the histogram of those 1,000 average differences.\n\n Histograms of Simulated Mean Differences \nEach plot includes a red dashed line at zero, representing the null hypothesis of no effect.\n Interpretation by Sample Size \n\nn = 50: The distribution is wide and noisy. Zero is near the center, meaning we can‚Äôt confidently detect an effect.\nn = 200: The distribution begins to narrow. Zero is still well within the range of plausible outcomes.\nn = 500: The histogram becomes more concentrated. The true effect begins to emerge, and zero starts shifting toward the tails.\nn = 1000: The distribution is tightly centered. Zero lies in the tail, indicating that the true average difference is likely not zero.\n\n Conclusion \n\nAs sample size increases, the sampling distribution of the mean difference becomes narrower and more centered around the true population effect.\n\nThis exercise demonstrates: - The Law of Large Numbers: larger samples produce more stable estimates. - The power of simulation for understanding uncertainty and inference. - Why small samples often yield inconclusive or misleading results.\nThese plots reinforce that while we may see noisy or overlapping outcomes in small samples, with enough data, we get closer to the truth."
  },
  {
    "objectID": "a_b_test.html",
    "href": "a_b_test.html",
    "title": "Experimental Results",
    "section": "",
    "text": "import pandas as pd\n\n\ndf = pd.read_stata('karlan_list_2007.dta')\n\n\ndf.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows √ó 48 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n\n\n\nfrom scipy import stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Separate data into treatment and control groups\ntreatment_group = df[df['treatment'] == 1]\ncontrol_group = df[df['control'] == 1]\n\n# T-test: mrm2 (months since last donation)\nttest_result = stats.ttest_ind(treatment_group['mrm2'].dropna(), control_group['mrm2'].dropna(), equal_var=False)\n\n# Linear regression: mrm2 ~ treatment\ndf_reg = df[['mrm2', 'treatment']].dropna()\nreg_result = smf.ols('mrm2 ~ treatment', data=df_reg).fit()\nttest_result, reg_result.summary()\n\n(TtestResult(statistic=0.11953155228177251, pvalue=0.9048549631450832, df=33394.47581389535),\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                   mrm2   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                 -0.000\n Method:                 Least Squares   F-statistic:                   0.01428\n Date:                Wed, 23 Apr 2025   Prob (F-statistic):              0.905\n Time:                        15:29:10   Log-Likelihood:            -1.9585e+05\n No. Observations:               50082   AIC:                         3.917e+05\n Df Residuals:                   50080   BIC:                         3.917e+05\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept     12.9981      0.094    138.979      0.000      12.815      13.181\n treatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n ==============================================================================\n Omnibus:                     8031.352   Durbin-Watson:                   2.004\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\n Skew:                           1.163   Prob(JB):                         0.00\n Kurtosis:                       3.751   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\n\nimport matplotlib.pyplot as plt\ngave_by_group = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ngave_by_group[\"group\"] = gave_by_group[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\nplt.figure(figsize=(6, 4))\nplt.bar(gave_by_group[\"group\"], gave_by_group[\"gave\"], width=0.5)\nplt.title(\"Proportion Who Donated by Group\")\nplt.ylabel(\"Proportion Gave\")\nplt.ylim(0, gave_by_group[\"gave\"].max() + 0.01)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\n\n# Re-import libraries and reload data due to kernel reset\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Ensure binary outcome is correctly typed\ndf['gave'] = df['gave'].astype(int)\n\n# T-test: response rate (gave) between treatment and control groups\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_control = df[df['control'] == 1]['gave']\nt_stat, p_val = ttest_ind(gave_treat, gave_control, equal_var=False)\n\n# Regression: response as a function of treatment\nreg_gave = smf.ols('gave ~ treatment', data=df).fit()\n\nt_stat, p_val, reg_gave.summary()\n\n(3.2094621908279835,\n 0.0013309823450914173,\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                   gave   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     9.618\n Date:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\n Time:                        21:04:50   Log-Likelihood:                 26630.\n No. Observations:               50083   AIC:                        -5.326e+04\n Df Residuals:                   50081   BIC:                        -5.324e+04\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept      0.0179      0.001     16.225      0.000       0.016       0.020\n treatment      0.0042      0.001      3.101      0.002       0.002       0.007\n ==============================================================================\n Omnibus:                    59814.280   Durbin-Watson:                   2.005\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\n Skew:                           6.740   Prob(JB):                         0.00\n Kurtosis:                      46.440   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n\nimport statsmodels.api as sm\n\n# Prepare the variables\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"gave\"]\n\n# Run the Probit regression\nprobit_model = sm.Probit(y, X)\nprobit_results = probit_model.fit()\n\nprobit_results.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nWed, 23 Apr 2025\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n21:22:42\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\n# Subset the data for different match ratios\n# According to the dataset: ratio = '1', '2', '3' for 1:1, 2:1, 3:1\ndf_ratio = df[df[\"treatment\"] == 1].copy()\ndf_ratio[\"ratio\"] = df_ratio[\"ratio\"].astype(str)\n\n# Extract binary 'gave' for each match ratio group\ngave_1_1 = df_ratio[df_ratio[\"ratio\"] == \"1\"][\"gave\"]\ngave_2_1 = df_ratio[df_ratio[\"ratio\"] == \"2\"][\"gave\"]\ngave_3_1 = df_ratio[df_ratio[\"ratio\"] == \"3\"][\"gave\"]\n\n# Perform t-tests between match ratio groups\nttest_1_vs_2 = ttest_ind(gave_1_1, gave_2_1, equal_var=False)\nttest_1_vs_3 = ttest_ind(gave_1_1, gave_3_1, equal_var=False)\nttest_2_vs_3 = ttest_ind(gave_2_1, gave_3_1, equal_var=False)\n\nttest_1_vs_2, ttest_1_vs_3, ttest_2_vs_3\n\n(TtestResult(statistic=-0.965048975142932, pvalue=0.33453078237183076, df=22225.07770983836),\n TtestResult(statistic=-1.0150174470156275, pvalue=0.31010856527625774, df=22215.0529778684),\n TtestResult(statistic=-0.05011581369764474, pvalue=0.9600305476940865, df=22260.84918918778))\n\n\n\n# Ensure 'gave' is binary\ndf['gave'] = df['gave'].astype(int)\n\n# Create dummy variables for each match ratio\n# This is only for treatment group, so filter and prepare accordingly\ndf_ratio = df[df['treatment'] == 1].copy()\ndf_ratio['ratio'] = df_ratio['ratio'].astype(str)\n\n# Create dummy variables: ratio1, ratio2, ratio3\ndf_ratio['ratio1'] = (df_ratio['ratio'] == '1').astype(int)\ndf_ratio['ratio2'] = (df_ratio['ratio'] == '2').astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == '3').astype(int)\n\n# Regression: gave ~ ratio1 + ratio2 + ratio3 (no intercept)\nimport statsmodels.api as sm\n\nX = df_ratio[['ratio1', 'ratio2', 'ratio3']]\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\n\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6454\n\n\nDate:\nWed, 23 Apr 2025\nProb (F-statistic):\n0.524\n\n\nTime:\n21:40:37\nLog-Likelihood:\n16688.\n\n\nNo. Observations:\n33396\nAIC:\n-3.337e+04\n\n\nDf Residuals:\n33393\nBIC:\n-3.334e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nratio1\n0.0207\n0.001\n14.912\n0.000\n0.018\n0.023\n\n\nratio2\n0.0226\n0.001\n16.267\n0.000\n0.020\n0.025\n\n\nratio3\n0.0227\n0.001\n16.335\n0.000\n0.020\n0.025\n\n\n\n\n\n\n\n\nOmnibus:\n38963.957\nDurbin-Watson:\n1.995\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2506478.937\n\n\nSkew:\n6.511\nProb(JB):\n0.00\n\n\nKurtosis:\n43.394\nCond. No.\n1.00\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "hw1_questions.html#interpretation",
    "href": "hw1_questions.html#interpretation",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Interpretation",
    "text": "Interpretation\nWe assess balance between treatment and control groups using both statistical methods:\n\nT-Test:\n\nt = 0.120, p = 0.905\n\nResult: Not statistically significant\n\nRegression:\n\nCoefficient on treatment ‚âà 0.014\n\np-value ‚âà 0.905\n\n95% Confidence Interval: Includes zero\n\n\nThese results confirm no significant difference in mrm2 across groups, supporting the randomization mechanism. This aligns with Table 1 in Karlan & List (2007), where the group means were:\n\n\n\nGroup\nMean Months Since Last Donation\n\n\n\n\nTreatment\n13.012\n\n\nControl\n12.998"
  },
  {
    "objectID": "hw1_questions.html#charitable-contribution-made-1",
    "href": "hw1_questions.html#charitable-contribution-made-1",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Charitable Contribution Made",
    "text": "Charitable Contribution Made\nWe examine whether receiving a matching donation offer increases the likelihood of making a charitable donation. This is measured using the binary variable gave, which equals 1 if a donation was made and 0 otherwise.\nWe use both a t-test and a bivariate linear regression to assess the difference in response rate between treatment and control groups.\n Results Summary\n\n\n\n\n\n\n\n\n\nMethod\nEffect Size\np-value\nInterpretation\n\n\n\n\nT-Test\nt = 3.21\n0.0013\nStatistically significant\n\n\nRegression\n+0.0042 (0.42%)\n0.002\nStatistically significant\n\n\n\n\nControl group donation rate ‚âà 1.79%\n\nTreatment group donation rate ‚âà 2.21%\n\nThese values match the response rates reported in Table 2A, Panel A of Karlan & List (2007).\n Table 2A (Panel A): Response Rate Comparison\n\n\n\nGroup\nResponse Rate\nStd. Error\n\n\n\n\nControl\n0.018\n(0.001)\n\n\nTreatment\n0.022\n(0.001)\n\n\nMatch 1:1\n0.021\n(0.001)\n\n\nMatch 2:1\n0.023\n(0.001)\n\n\nMatch 3:1\n0.023\n(0.001)\n\n\n\n\nSource: Karlan & List (2007), Table 2A, Panel A\n\n Interpretation\nEven a small increase in the likelihood of giving ‚Äî about 0.4 percentage points ‚Äî is statistically significant in a large-scale field experiment with over 50,000 individuals.\nThis result shows that: - Matching donations have a causal impact on behavior. - People are more likely to respond to donation appeals when told their gift will be matched. - The psychological effect (e.g., feeling of leverage, social validation, urgency) may be as important as the financial incentive.\nThus, matched donations are an effective strategy not just in economics but in behavioral design for charitable fundraising.\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the ‚Äúfigures suggest‚Äù comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients ‚Äì what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "projects/hw1_questions.html",
    "href": "projects/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse."
  },
  {
    "objectID": "projects/hw1_questions.html#introduction",
    "href": "projects/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse."
  },
  {
    "objectID": "projects/hw1_questions.html#data",
    "href": "projects/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nDataset Overview\nThe dataset contains over 50,000 observations from a field experiment in charitable giving. Variables include treatment assignments, donation behavior, match ratio conditions, prior giving history, and demographic information.\nBelow is a summary of key variable distributions and data structure.\n\nimport pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\ndf.info()\ndf.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows √ó 48 columns\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nI tested whether the treatment and control groups differed in prior donor behavior by comparing the number of months since last donation (mrm2). Both a two-sample t-test (t = 0.120, p = 0.905) and a linear regression of mrm2 ~ treatment (Œ≤ = 0.0137, p = 0.905) confirm no statistically significant difference. This supports the randomization mechanism and matches Table 1 in Karlan and List (2007).\n\nfrom scipy import stats\n\n# Clean data: drop NAs\ndf_clean = df[[\"mrm2\", \"treatment\", \"control\"]].dropna()\n\n# Split groups\ntreat = df_clean[df_clean['treatment'] == 1]['mrm2']\ncontrol = df_clean[df_clean['control'] == 1]['mrm2']\n\n# Perform Welch's t-test (no assumption of equal variances)\nttest = stats.ttest_ind(treat, control, equal_var=False)\n\n# Print results\nprint(f\"T-test result: t = {ttest.statistic:.3f}, p = {ttest.pvalue:.3f}\")\n\nimport statsmodels.formula.api as smf\n\n# Regression of mrm2 on treatment\nmodel = smf.ols(\"mrm2 ~ treatment\", data=df_clean).fit()\nmodel.summary()\n\n# Extract relevant results\nresults_table = pd.DataFrame({\n    'Variable': model.params.index,\n    'Coefficient': model.params.values,\n    'Std. Error': model.bse.values,\n    'p-value': model.pvalues.values,\n    '95% CI Lower': model.conf_int()[0].values,\n    '95% CI Upper': model.conf_int()[1].values\n})\n\n# Round for presentation\nresults_table = results_table.round(4)\nresults_table\n\nT-test result: t = 0.120, p = 0.905\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n12.9981\n0.0935\n0.0000\n12.8148\n13.1815\n\n\n1\ntreatment\n0.0137\n0.1145\n0.9049\n-0.2108\n0.2382\n\n\n\n\n\n\n\n Interpretation \nI assessed the balance between treatment and control groups using both statistical methods:\n\nT-Test:\n\nt = 0.120, p = 0.905\n\nResult: Not statistically significant\n\nRegression:\n\nCoefficient on treatment ‚âà 0.014\n\np-value ‚âà 0.905\n\n95% Confidence Interval: Includes zero\n\n\nThese results confirm no significant difference in mrm2 across groups, supporting the randomization mechanism. This aligns with Table 1 in Karlan & List (2007), where the group means were:\n\n\n\nGroup\nMean Months Since Last Donation\n\n\n\n\nTreatment\n13.012\n\n\nControl\n12.998"
  },
  {
    "objectID": "projects/hw1_questions.html#experimental-results",
    "href": "projects/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\ngave_by_group = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ngave_by_group[\"group\"] = gave_by_group[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\nplt.figure(figsize=(6, 4))\nplt.bar(gave_by_group[\"group\"], gave_by_group[\"gave\"], width=0.5)\nplt.title(\"Proportion Who Donated by Group\")\nplt.ylabel(\"Proportion Gave\")\nplt.ylim(0, gave_by_group[\"gave\"].max() + 0.01)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nI compared the response rate (i.e., whether a donation was made) between treatment and control groups.\nThe bar plot below shows that the treatment group, who received a matching grant offer, donated at a higher rate than the control group, who received a standard letter.\nThis visual confirms the core finding in Karlan & List (2007): matching donations increased participation in charitable giving.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Ensure binary outcome is correctly typed\ndf['gave'] = df['gave'].astype(int)\n\n# T-test: response rate (gave) between treatment and control groups\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_control = df[df['control'] == 1]['gave']\nt_stat, p_val = ttest_ind(gave_treat, gave_control, equal_var=False)\n\n# Display t-test result\nprint(f\"T-test result:\\nt = {t_stat:.3f}, p = {p_val:.4f}\\n\")\n\n# Regression: response as a function of treatment\nreg_gave = smf.ols('gave ~ treatment', data=df).fit()\n\nprint('Regression Results:')\n# Extract and format regression results\nresults_table = pd.DataFrame({\n    'Variable': reg_gave.params.index,\n    'Coefficient': reg_gave.params.values,\n    'Std. Error': reg_gave.bse.values,\n    'p-value': reg_gave.pvalues.values,\n    '95% CI Lower': reg_gave.conf_int()[0].values,\n    '95% CI Upper': reg_gave.conf_int()[1].values\n}).round(4)\n\nresults_table\n\nT-test result:\nt = 3.209, p = 0.0013\n\nRegression Results:\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n0.0179\n0.0011\n0.0000\n0.0157\n0.0200\n\n\n1\ntreatment\n0.0042\n0.0013\n0.0019\n0.0015\n0.0068\n\n\n\n\n\n\n\n Charitable Contribution Made \nI examined whether receiving a matching donation offer increases the likelihood of making a charitable donation. This is measured using the binary variable gave, which equals 1 if a donation was made and 0 otherwise.\nI used both a t-test and a bivariate linear regression to assess the difference in response rate between treatment and control groups.\n Results Summary \n\n\n\n\n\n\n\n\n\nMethod\nEffect Size\np-value\nInterpretation\n\n\n\n\nT-Test\nt = 3.21\n0.0013\nStatistically significant\n\n\nRegression\n+0.0042 (0.42%)\n0.002\nStatistically significant\n\n\n\n\nControl group donation rate ‚âà 1.79%\n\nTreatment group donation rate ‚âà 2.21%\n\nThese values match the response rates reported in Table 2A, Panel A of Karlan & List (2007).\n Table 2A (Panel A): Response Rate Comparison \n\n\n\nGroup\nResponse Rate\nStd. Error\n\n\n\n\nControl\n0.018\n(0.001)\n\n\nTreatment\n0.022\n(0.001)\n\n\nMatch 1:1\n0.021\n(0.001)\n\n\nMatch 2:1\n0.023\n(0.001)\n\n\nMatch 3:1\n0.023\n(0.001)\n\n\n\n\nSource: Karlan & List (2007), Table 2A, Panel A\n\n Interpretation \nEven a small increase in the likelihood of giving ‚Äî about 0.4 percentage points ‚Äî is statistically significant in a large-scale field experiment with over 50,000 individuals.\nThis result shows that: - Matching donations have a causal impact on behavior. - People are more likely to respond to donation appeals when told their gift will be matched. - The psychological effect (e.g., feeling of leverage, social validation, urgency) may be as important as the financial incentive.\nThus, matched donations are an effective strategy not just in economics but in behavioral design for charitable fundraising.\n\nimport statsmodels.api as sm\n\n# Prepare the variables\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"gave\"]\n\n# Run the Probit regression\nprobit_model = sm.Probit(y, X)\nprobit_results = probit_model.fit()\n\n# probit_results.summary()\n\n\n# Extract and format Probit results\nprobit_table = pd.DataFrame({\n    \"Variable\": probit_results.params.index,\n    \"Coefficient\": probit_results.params.values,\n    \"Std. Error\": probit_results.bse.values,\n    \"z-value\": probit_results.tvalues,\n    \"p-value\": probit_results.pvalues,\n    \"95% CI Lower\": probit_results.conf_int()[0],\n    \"95% CI Upper\": probit_results.conf_int()[1]\n}).round(4)\nprint(\"\\nProbit Results:\")\nprobit_table\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\nProbit Results:\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\nconst\nconst\n-2.1001\n0.0233\n-90.0728\n0.0000\n-2.1458\n-2.0544\n\n\ntreatment\ntreatment\n0.0868\n0.0279\n3.1129\n0.0019\n0.0321\n0.1414\n\n\n\n\n\n\n\n Probit Regression: Impact of Matching Grant on Donation Likelihood \nTo replicate Table 3, Column (1) from Karlan & List (2007), I estimated a Probit model where the outcome is whether a donation was made (gave = 1) and the explanatory variable is assignment to treatment (treatment = 1).\n Our Probit Model Results \n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI\n\n\n\n\nIntercept\n-2.100\n0.023\n-90.07\n&lt; 0.001\n[-2.146, -2.054]\n\n\nTreatment\n0.087\n0.028\n3.11\n0.002\n[0.032, 0.141]\n\n\n\n\nPseudo R¬≤: 0.001\n\nObservations: 50,083\n\nThese results match the direction and significance of Table 3, Column (1) in the original study.\n Table 3: Primary Probit Regression Results from Karlan & List (2007) \n\n\n\n\n\n\n\n\n\nVariable\n(1) All\nStd. Err.\nSignificance\n\n\n\n\nTreatment\n0.004\n(0.001)\n***\n\n\nTreatment √ó 2:1 ratio\n0.002\n(0.002)\n\n\n\nTreatment √ó 3:1 ratio\n0.002\n(0.002)\n\n\n\nTreatment √ó $25,000 threshold\n-0.001\n(0.002)\n\n\n\nTreatment √ó $50,000 threshold\n0.000\n(0.002)\n\n\n\nTreatment √ó $100,000 threshold\n-0.000\n(0.002)\n\n\n\nTreatment √ó medium example amount\n0.001\n(0.002)\n\n\n\nTreatment √ó high example amount\n0.001\n(0.002)\n\n\n\nPseudo R¬≤\n0.001\n\n\n\n\nObservations\n50,083\n\n\n\n\n\nNotes: - The paper reports marginal effects, whereas our Probit output gives latent index coefficients. - The magnitude of 0.004 in the paper corresponds to a marginal increase in probability of donating due to the treatment. - Our coefficient of 0.087 reflects the effect on the underlying propensity to give, which is standard in Probit estimation.\n Interpretation \nDespite a small effect size, the impact of being offered a matching donation is statistically significant. This suggests:\n\nEven subtle nudges, like framing a gift as matched by a leadership donor, can increase participation.\nThe result is economically meaningful due to the large sample size and real-world behavioral context.\n\nIn short: human generosity is sensitive to framing ‚Äî and donors are more likely to act when they feel their gift has leverage.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Subset the data for different match ratios\n# According to the dataset: ratio = '1', '2', '3' for 1:1, 2:1, 3:1\ndf_ratio = df[df[\"treatment\"] == 1].copy()\ndf_ratio[\"ratio\"] = df_ratio[\"ratio\"].astype(str)\n\n# Extract binary 'gave' for each match ratio group\ngave_1_1 = df_ratio[df_ratio[\"ratio\"] == \"1\"][\"gave\"]\ngave_2_1 = df_ratio[df_ratio[\"ratio\"] == \"2\"][\"gave\"]\ngave_3_1 = df_ratio[df_ratio[\"ratio\"] == \"3\"][\"gave\"]\n\n# Perform t-tests between match ratio groups\nttest_1_vs_2 = ttest_ind(gave_1_1, gave_2_1, equal_var=False)\nttest_1_vs_3 = ttest_ind(gave_1_1, gave_3_1, equal_var=False)\nttest_2_vs_3 = ttest_ind(gave_2_1, gave_3_1, equal_var=False)\n\n# ttest_1_vs_2, ttest_1_vs_3, ttest_2_vs_3\n\n# Create clean results table\nt_test_results = pd.DataFrame({\n    \"Comparison\": [\"1:1 vs 2:1\", \"1:1 vs 3:1\", \"2:1 vs 3:1\"],\n    \"t-statistic\": [ttest_1_vs_2.statistic, ttest_1_vs_3.statistic, ttest_2_vs_3.statistic],\n    \"p-value\": [ttest_1_vs_2.pvalue, ttest_1_vs_3.pvalue, ttest_2_vs_3.pvalue]\n}).round(4)\nprint('t test Results:')\nt_test_results\n\nt test Results:\n\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\n\n\n\n\n0\n1:1 vs 2:1\n-0.9650\n0.3345\n\n\n1\n1:1 vs 3:1\n-1.0150\n0.3101\n\n\n2\n2:1 vs 3:1\n-0.0501\n0.9600\n\n\n\n\n\n\n\n Does Match Ratio Size Affect Donation Rates? \nI investigated whether increasing the match ratio (from 1:1 to 2:1 to 3:1) has a statistically significant effect on the likelihood that someone donates.\nTo do this, I ran a series of t-tests comparing donation rates (gave = 1) across match ratio groups, restricting the sample to individuals who received a matching offer.\n T-Test Results by Match Ratio \n\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\nInterpretation\n\n\n\n\n1:1 vs 2:1 match\n-0.965\n0.335\n‚ùå Not statistically significant\n\n\n1:1 vs 3:1 match\n-1.015\n0.310\n‚ùå Not statistically significant\n\n\n2:1 vs 3:1 match\n-0.050\n0.960\n‚ùå Not statistically significant\n\n\n\n Interpretation \nThese results show no significant difference in donation rates across the different match ratios. This means that:\n\nIncreasing the match multiplier from 1:1 to 2:1 or 3:1 does not lead to a higher likelihood of giving.\nThis supports the statement from Karlan & List (2007, p.¬†8):\n\n\n‚ÄúThe gift distributions across the various matching ratios are not significantly different from one another.‚Äù\n\nIn other words, people respond positively to the existence of a match, but not necessarily more when the match becomes more generous.\n Conclusion \nThe presence of a match appears to matter more than its magnitude. This suggests that:\n\nFraming and social cues ‚Äî like simply saying ‚Äúyour gift will be matched‚Äù ‚Äî may be more behaviorally powerful than the precise financial terms.\n\nThis insight is important for nonprofit fundraisers: focus on highlighting the match rather than inflating the ratio.\n\n# Ensure 'gave' is binary\ndf['gave'] = df['gave'].astype(int)\n\n# Create dummy variables for each match ratio\n# This is only for treatment group, so filter and prepare accordingly\ndf_ratio = df[df['treatment'] == 1].copy()\ndf_ratio['ratio'] = df_ratio['ratio'].astype(str)\n\n# Create dummy variables: ratio1, ratio2, ratio3\ndf_ratio['ratio1'] = (df_ratio['ratio'] == '1').astype(int)\ndf_ratio['ratio2'] = (df_ratio['ratio'] == '2').astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == '3').astype(int)\n\n# Regression: gave ~ ratio1 + ratio2 + ratio3 (no intercept)\nimport statsmodels.api as sm\n\nX = df_ratio[['ratio1', 'ratio2', 'ratio3']]\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\n\n# model.summary()\n\n\n# Format regression output\nratio_reg_table = pd.DataFrame({\n    \"Match Ratio\": [\"1:1\", \"2:1\", \"3:1\"],\n    \"Coefficient\": model.params.values,\n    \"Std. Error\": model.bse.values,\n    \"p-value\": model.pvalues.values,\n    \"95% CI Lower\": model.conf_int()[0].values,\n    \"95% CI Upper\": model.conf_int()[1].values\n}).round(4)\n\nprint('Reression Results:')\nratio_reg_table\n\nReression Results:\n\n\n\n\n\n\n\n\n\nMatch Ratio\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n1:1\n0.0207\n0.0014\n0.0\n0.0180\n0.0235\n\n\n1\n2:1\n0.0226\n0.0014\n0.0\n0.0199\n0.0254\n\n\n2\n3:1\n0.0227\n0.0014\n0.0\n0.0200\n0.0255\n\n\n\n\n\n\n\n Behavioral Insight: Why Match Size Doesn‚Äôt Matter (Much) \nThis regression shows that all forms of match ratios ‚Äî 1:1, 2:1, and 3:1 ‚Äî significantly increase the likelihood that someone donates, with donation rates clustering around 2%.\nHowever, the differences between match sizes are extremely small:\n\nPeople who saw a 1:1 match donated at a rate of 2.07%.\nThose who saw a 2:1 match gave at 2.26%.\nWith a 3:1 match, the rate was 2.27%.\n\nThese results suggest that once a match is present, increasing its generosity has little additional impact. In other words:\n\nIt‚Äôs the existence of the match that matters, not its size.\n\nThis behavior aligns with theories in behavioral economics: - The match acts as a signal of social proof or endorsement. - It may create a sense of urgency or leverage (‚Äúmy donation matters more‚Äù). - But donors aren‚Äôt particularly sensitive to how generous the match is ‚Äî at least not in terms of deciding whether or not to give.\n Implication for Fundraising \nFrom a practical standpoint, this means that: - Fundraisers don‚Äôt need to offer high match ratios to see results. - A simple, clearly communicated 1:1 match may be just as effective as a 3:1 match in increasing participation.\nThis finding reinforces the power of framing and perception in influencing human behavior.\n\n# Compute the actual mean response (gave) for each ratio group directly from the data\nmean_1_1 = df_ratio[df_ratio[\"ratio\"] == \"1\"][\"gave\"].mean()\nmean_2_1 = df_ratio[df_ratio[\"ratio\"] == \"2\"][\"gave\"].mean()\nmean_3_1 = df_ratio[df_ratio[\"ratio\"] == \"3\"][\"gave\"].mean()\n\n# Calculate differences in response rates\ndiff_2_1_vs_1_1 = mean_2_1 - mean_1_1\ndiff_3_1_vs_2_1 = mean_3_1 - mean_2_1\n\n# Extract coefficients from regression model\ncoef_1_1 = model.params[\"ratio1\"]\ncoef_2_1 = model.params[\"ratio2\"]\ncoef_3_1 = model.params[\"ratio3\"]\n\n# Calculate differences in coefficients\ncoef_diff_2_1_vs_1_1 = coef_2_1 - coef_1_1\ncoef_diff_3_1_vs_2_1 = coef_3_1 - coef_2_1\n\n# (mean_1_1, mean_2_1, mean_3_1,\n#  diff_2_1_vs_1_1, diff_3_1_vs_2_1,\n#  coef_diff_2_1_vs_1_1, coef_diff_3_1_vs_2_1)\n\n\n# Organize values into a formatted summary table\ncomparison_table = pd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 2:1\"],\n    \"Diff (means)\": [diff_2_1_vs_1_1, diff_3_1_vs_2_1],\n    \"Diff (regression coefficients)\": [coef_diff_2_1_vs_1_1, coef_diff_3_1_vs_2_1]\n}).round(5)\nprint('Results:')\ncomparison_table\n\nResults:\n\n\n\n\n\n\n\n\n\nComparison\nDiff (means)\nDiff (regression coefficients)\n\n\n\n\n0\n2:1 vs 1:1\n0.00188\n0.00188\n\n\n1\n3:1 vs 2:1\n0.00010\n0.00010\n\n\n\n\n\n\n\n Comparing Response Rates Across Match Ratios \nI examined how the size of the match (1:1 vs.¬†2:1 vs.¬†3:1) influences the probability that an individual makes a donation. I did this in two ways:\n\nDirectly from the data by calculating average donation rates within each match group.\nFrom the fitted coefficients of a regression on dummy variables for each ratio.\n\n Response Rate Differences \n\n\n\n\n\n\n\n\nComparison\nDirect from Data\nFrom Regression Coefficients\n\n\n\n\n2:1 vs 1:1 match\n0.00188 (0.19%)\n0.00188 (0.19%)\n\n\n3:1 vs 2:1 match\n0.00010 (0.01%)\n0.00010 (0.01%)\n\n\n\n\nThese differences represent increases in the probability of donating when moving from one match ratio to a higher one.\nThe results are identical across both methods, which supports the robustness of the findings.\n\n Interpretation \n\nMoving from a 1:1 to 2:1 match slightly increases donation rates by about 0.19 percentage points.\nIncreasing from a 2:1 to a 3:1 match has a negligible effect ‚Äî only 0.01 percentage points.\nThese differences are statistically very small and are unlikely to be meaningful in practice.\n\n Conclusion \nOur analysis shows that:\n\nOnce a match is introduced, increasing the match ratio does not meaningfully increase the likelihood of giving.\n\nThis confirms the finding from Karlan & List (2007):\n\n‚ÄúThe gift distributions across the various matching ratios are not significantly different from one another.‚Äù\n\nIn short, it‚Äôs the presence of a match offer ‚Äî not its generosity ‚Äî that influences donor behavior.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# Run a t-test on the amount given between treatment and control groups\namount_treat = df[df['treatment'] == 1]['amount']\namount_control = df[df['control'] == 1]['amount']\namount_ttest = ttest_ind(amount_treat, amount_control, equal_var=False)\n\n# Run a bivariate linear regression: amount ~ treatment\namount_reg = smf.ols('amount ~ treatment', data=df).fit()\n\n# amount_ttest.statistic, amount_ttest.pvalue, amount_reg.summary()\n\n# Format regression output into a clean table\namount_table = pd.DataFrame({\n    \"Variable\": amount_reg.params.index,\n    \"Coefficient\": amount_reg.params.values,\n    \"Std. Error\": amount_reg.bse.values,\n    \"p-value\": amount_reg.pvalues.values,\n    \"95% CI Lower\": amount_reg.conf_int()[0].values,\n    \"95% CI Upper\": amount_reg.conf_int()[1].values\n}).round(4)\n\namount_table\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n0.8133\n0.0674\n0.0000\n0.6811\n0.9454\n\n\n1\ntreatment\n0.1536\n0.0826\n0.0628\n-0.0082\n0.3154\n\n\n\n\n\n\n\n Size of Charitable Contribution \nI tested whether receiving a matching donation offer affects the amount donated using a t-test and linear regression:\n Results \n\n\n\n\n\n\n\n\n\nMethod\nTreatment Effect\np-value\nConclusion\n\n\n\n\nT-Test\n+$0.15\n0.055\nüî∏ Marginally not significant\n\n\nRegression\n+$0.15\n0.063\nüî∏ Suggestive but inconclusive\n\n\n\n\nControl group average: ~$0.81\n\nTreatment group average: ~$0.96\n\n Interpretation \n\nThe treatment group gave slightly more, but the difference is not statistically significant at the 5% level.\nThis suggests that while match offers increase participation, they have a much smaller effect on how much people give.\n\n Takeaway \n\nMatching donations may encourage more people to give, but do not substantially increase donation size.\n\n\n# Limit the data to only those who made a donation (amount &gt; 0)\ndf_positive = df[df['amount'] &gt; 0].copy()\n\n# T-test for amount among donors only\namount_treat_pos = df_positive[df_positive['treatment'] == 1]['amount']\namount_control_pos = df_positive[df_positive['control'] == 1]['amount']\namount_ttest_pos = ttest_ind(amount_treat_pos, amount_control_pos, equal_var=False)\n\n# Regression: amount ~ treatment (for donors only)\namount_reg_pos = smf.ols('amount ~ treatment', data=df_positive).fit()\n\n# amount_ttest_pos.statistic, amount_ttest_pos.pvalue, amount_reg_pos.summary()\n\n\n# Clean regression summary\namount_conditional_table = pd.DataFrame({\n    \"Variable\": amount_reg_pos.params.index,\n    \"Coefficient\": amount_reg_pos.params.values,\n    \"Std. Error\": amount_reg_pos.bse.values,\n    \"p-value\": amount_reg_pos.pvalues.values,\n    \"95% CI Lower\": amount_reg_pos.conf_int()[0].values,\n    \"95% CI Upper\": amount_reg_pos.conf_int()[1].values\n}).round(4)\n\namount_conditional_table\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n45.5403\n2.4234\n0.0000\n40.7850\n50.2956\n\n\n1\ntreatment\n-1.6684\n2.8724\n0.5615\n-7.3048\n3.9680\n\n\n\n\n\n\n\n Conditional Donation Amount: Among Donors Only \nTo isolate the effect of treatment on the amount given, I restricted the sample to only those individuals who made a donation (amount &gt; 0).\nI used both a t-test and a bivariate regression (amount ~ treatment) to compare average donation sizes between treatment and control groups.\n Results Summary \n\n\n\n\n\n\n\n\n\nMethod\nTreatment Effect\np-value\nConclusion\n\n\n\n\nT-Test (donors only)\nt = -0.58\n0.559\n‚ùå Not statistically significant\n\n\nRegression\n-$1.67\n0.561\n‚ùå Not statistically significant\n\n\n\n\nControl group average donation: ~$45.54\n\nTreatment group average donation: ~$43.87\n\n Interpretation \n\nThe treatment group donated slightly less on average, but the difference is not statistically meaningful.\nThis suggests that while the match offer encourages more people to donate, it does not increase donation size among those who would give anyway.\nBecause I only included those who donated, the treatment effect here is not causal ‚Äî it‚Äôs conditional and may suffer from selection bias.\n\n Conclusion \n\nMatched donations are effective at increasing the number of donors, but not the amount donated by each donor ‚Äî at least among those who already choose to give.\n\n\nimport matplotlib.pyplot as plt\n\n# Filter to donors only\ndf_donors = df[df[\"amount\"] &gt; 0]\n\n# Separate treatment and control donors\ntreat_donors = df_donors[df_donors[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = df_donors[df_donors[\"control\"] == 1][\"amount\"]\n\n# Calculate means\nmean_treat = treat_donors.mean()\nmean_control = control_donors.mean()\n\n# Create side-by-side histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color=\"skyblue\", edgecolor=\"black\")\naxes[0].axvline(mean_control, color=\"red\", linestyle=\"--\", label=f\"Mean = ${mean_control:.2f}\")\naxes[0].set_title(\"Control Group Donations\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\n# Treatment group plot\naxes[1].hist(treat_donors, bins=30, color=\"lightgreen\", edgecolor=\"black\")\naxes[1].axvline(mean_treat, color=\"red\", linestyle=\"--\", label=f\"Mean = ${mean_treat:.2f}\")\naxes[1].set_title(\"Treatment Group Donations\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Distribution of Donation Amounts Among Donors \nI am now focusing on individuals who actually made a donation (amount &gt; 0) to analyze how much they gave, and whether the treatment group (those offered a matching donation) gave more than the control group.\nI visualized the distribution of donation amounts with two histograms ‚Äî one for each group ‚Äî and include a red dashed line indicating the average donation in each.\n Interpretation \n\nBoth distributions are heavily right-skewed, which is common in charitable giving: most donors give modest amounts, but a few give significantly more.\nThe average donation in the control group was about $45.54, while the treatment group averaged $43.87.\nThis difference is not statistically significant, as confirmed by both a t-test and a regression limited to donors.\n\n What Did I Learn? \n\nWhile the matching donation offer increases the probability of donating, it does not increase the donation amount among those who choose to give.\nIn fact, the average donation in the treatment group is slightly lower, though the difference is not meaningful.\n\n Important Caveat \nThis analysis is based only on people who gave, so the treatment coefficient does not have a causal interpretation here. This subset is not randomly assigned ‚Äî it‚Äôs a selected group, which may differ systematically between treatment and control.\n Fundraising Implication \n\nMatching offers are powerful tools to increase participation, but they do not necessarily lead to larger individual gifts.\n\nTo increase average donation size, fundraisers may need additional tactics ‚Äî such as suggested donation levels, tiered match thresholds, or social proof."
  },
  {
    "objectID": "projects/hw1_questions.html#simulation-experiment",
    "href": "projects/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Extract donation amounts for control and treatment\ncontrol_data = df[df[\"control\"] == 1][\"amount\"]\ntreatment_data = df[df[\"treatment\"] == 1][\"amount\"]\n\n# Simulate draws from each distribution\nnp.random.seed(42)\nsim_control = np.random.choice(control_data, size=100_000, replace=True)\nsim_treatment = np.random.choice(treatment_data, size=10_000, replace=True)\n\n# Calculate 10,000 differences between treatment and control draws\nsim_control_subset = np.random.choice(sim_control, size=10_000, replace=False)\ndiffs = sim_treatment - sim_control_subset\n\n# Compute cumulative average of differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Average Difference\")\nplt.axhline(y=np.mean(treatment_data) - np.mean(control_data), color=\"red\", linestyle=\"--\", label=\"True Mean Difference\")\nplt.title(\"Cumulative Average of Treatment-Control Differences\")\nplt.xlabel(\"Number of Draws\")\nplt.ylabel(\"Cumulative Average Difference in Donation Amount\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Simulated Cumulative Average Differences \nTo better understand the behavior of sample averages and connect to the concepts from our first class (Slide 43), we simulate the cumulative effect of donation differences between the treatment and control groups.\n Simulation Setup \n\nI simulated 100,000 random draws from the control group donation distribution.\nI simulated 10,000 random draws from the treatment group.\nFor each of the 10,000 pairs, I calculated the difference: treatment - control.\nI then computed the cumulative average of these 10,000 differences.\n\n Plot Interpretation \nThe plot below shows:\n\nA blue line representing the cumulative average of the simulated differences.\nA red dashed line indicating the true difference in means between treatment and control groups (calculated from the full dataset).\n\nAs the number of draws increases, the cumulative average approaches the true difference.\nThis illustrates the Law of Large Numbers: with enough data, sample-based estimates converge to the population value.\n What I Learnt \n\nThis simulation confirms that even in noisy, skewed data like donations, repeated sampling yields reliable estimates.\n\nIt also demonstrates that the difference in means I computed from data is not just a fluke ‚Äî it‚Äôs what we‚Äôd expect if I sampled repeatedly from the same distributions.\n\n\nCentral Limit Theorem\n\n# Define a function to simulate mean differences for a given sample size\ndef simulate_differences(sample_size, n_reps=1000):\n    differences = []\n    for _ in range(n_reps):\n        sample_control = np.random.choice(control_data, size=sample_size, replace=True)\n        sample_treatment = np.random.choice(treatment_data, size=sample_size, replace=True)\n        differences.append(np.mean(sample_treatment) - np.mean(sample_control))\n    return differences\n\n# Simulate for each sample size\nnp.random.seed(42)\nsizes = [50, 200, 500, 1000]\nsimulated_results = {size: simulate_differences(size) for size in sizes}\n\n# Plot histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, size in enumerate(sizes):\n    axes[i].hist(simulated_results[size], bins=30, color='lightgray', edgecolor='black')\n    axes[i].axvline(0, color='red', linestyle='--', label=\"Zero\")\n    axes[i].set_title(f\"Sample Size = {size}\")\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Sampling Distributions at Different Sample Sizes \nTo mirror the exercise from Slide 44 of our first class, I simulated the sampling distribution of the mean difference in donation amount between the treatment and control groups.\nFor each of four different sample sizes ‚Äî 50, 200, 500, and 1000 ‚Äî I:\n\nDrew n observations from each group.\nComputed the difference in mean donation: treatment - control.\nRepeated the process 1,000 times.\nPlotted the histogram of those 1,000 average differences.\n\n Histograms of Simulated Mean Differences \nEach plot includes a red dashed line at zero, representing the null hypothesis of no effect.\n Interpretation by Sample Size \n\nn = 50: The distribution is wide and noisy. Zero is near the center, meaning we can‚Äôt confidently detect an effect.\nn = 200: The distribution begins to narrow. Zero is still well within the range of plausible outcomes.\nn = 500: The histogram becomes more concentrated. The true effect begins to emerge, and zero starts shifting toward the tails.\nn = 1000: The distribution is tightly centered. Zero lies in the tail, indicating that the true average difference is likely not zero.\n\n Conclusion \n\nAs sample size increases, the sampling distribution of the mean difference becomes narrower and more centered around the true population effect.\n\nThis exercise demonstrates: - The Law of Large Numbers: larger samples produce more stable estimates. - The power of simulation for understanding uncertainty and inference. - Why small samples often yield inconclusive or misleading results.\nThese plots reinforce that while we may see noisy or overlapping outcomes in small samples, with enough data, we get closer to the truth."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nNujoum Unus\n\n\nJan 21, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nNujoum Unus\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nNujoum Unus\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSegmentation and Strategy: Using ML to Decode Penguins and People\n\n\n\n\n\n\nNujoum Unus\n\n\nInvalid Date\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/hw1_questions.html#purpose-of-the-study",
    "href": "projects/hw1_questions.html#purpose-of-the-study",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Purpose of the Study",
    "text": "Purpose of the Study\nKarlan and List aimed to answer a simple but important question:\n&gt; Do people give more when their donation is matched? And if so, does the size of the match matter?\nThey also explored additional behavioral levers commonly used in fundraising, such as challenge framing, suggested donation amounts, and goal-based appeals."
  },
  {
    "objectID": "projects/hw1_questions.html#project-overview",
    "href": "projects/hw1_questions.html#project-overview",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Project Overview",
    "text": "Project Overview\nIn this replication study, I used the same dataset provided by Karlan and List to:\n\nReproduce their key findings\nValidate the statistical robustness of their claims\nExplore new visualizations and simulations that illuminate the behavioral mechanisms at play\nReflect on what this experiment teaches us about human motivation, social framing, and economic incentives in the context of public goods\n\nThis report follows a structured analysis of donation likelihood, donation size, and how different dimensions of the match offer (ratio, threshold, framing) influence both."
  },
  {
    "objectID": "projects/hw1_questions.html#experimental-design",
    "href": "projects/hw1_questions.html#experimental-design",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Design",
    "text": "Experimental Design\nThe letters fell into three broad treatment types:\n\nStandard Fundraising Letter (Control)\n\nA typical letter requesting support for the organization, with no additional incentives or matching language.\n\nMatching Grant Letter\n\nIncluded a paragraph stating that a leadership donor would match any contribution at one of three possible ratios:\n\n1:1 (every dollar given is doubled)\n\n2:1 (every dollar is tripled)\n\n3:1 (every dollar quadrupled)\n\n\nMatching offers also varied by threshold, i.e., the maximum amount the leadership donor would match:\n\n$25,000, $50,000, $100,000, or unstated.\n\nSuggested donation levels were tailored based on each recipient‚Äôs previous giving history:\n\nTheir highest previous gift\n\n1.25√ó their highest gift\n\n1.5√ó their highest gift\n\n\nChallenge Grant Letter\n\nFramed the offer as part of a collective effort or campaign challenge, appealing to urgency and social impact rather than pure match mechanics.\n\n\nBecause each component (match ratio, threshold, suggested donation amount) was randomized independently within the matching grant group, the experiment had a factorial design ‚Äî allowing the researchers to isolate and measure the effects of each variable."
  },
  {
    "objectID": "projects/hw1_questions.html#why-this-matters",
    "href": "projects/hw1_questions.html#why-this-matters",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Why This Matters",
    "text": "Why This Matters\nAt the time of the study, fundraisers often relied on rules of thumb and anecdotes, lacking hard data on what actually drives giving. Karlan and List‚Äôs approach brought scientific rigor to the domain of nonprofit fundraising by:\n\nLeveraging random assignment to establish causality\nTesting commonly used marketing strategies under real-world conditions\nGenerating insights with practical implications for organizations seeking to raise more money"
  },
  {
    "objectID": "projects/hw1_questions.html#contribution-to-the-literature",
    "href": "projects/hw1_questions.html#contribution-to-the-literature",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Contribution to the Literature",
    "text": "Contribution to the Literature\nThis study represents one of the first large-scale natural field experiments in charitable giving. It moved beyond lab settings and survey experiments to observe real decisions involving real money. The results helped bridge the gap between behavioral economics and fundraising practice, offering evidence-backed recommendations on:\n\nThe efficacy of matching offers\n\nHow much match ratios influence behavior\n\nWhether people respond to thresholds or suggested amounts\n\nThe heterogeneous effects by donor characteristics and geography"
  },
  {
    "objectID": "projects/hw1_questions.html#background",
    "href": "projects/hw1_questions.html#background",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Background",
    "text": "Background\nCharitable organizations often rely on fundraising letters to solicit donations, but little rigorous evidence has been available to guide how those letters should be designed. In a groundbreaking field experiment, economists Dean Karlan and John List set out to test how different framing strategies and financial incentives affect individual donation behavior.\nThe experiment, conducted in collaboration with a politically-oriented nonprofit organization, involved mailing 50,083 fundraising letters to previous donors. Crucially, the recipients were randomly assigned to different treatment groups, allowing the researchers to measure causal effects rather than mere correlations."
  },
  {
    "objectID": "projects/hw2_questions.html",
    "href": "projects/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\n\nShow code\nimport pandas as pd\n\n# Blueprinty‚Äôs 1,500-firm sample\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n# Basic dimensions\nn_blue, p_blue = blueprinty.shape\nprint(f\"Blueprinty dataset ‚Üí {n_blue:,} firms √ó {p_blue} columns\")\n\n\nBlueprinty dataset ‚Üí 1,500 firms √ó 4 columns\n\n\n\n\nScope & granularity\n* 1,500 mature U.S. engineering firms (non-start-ups).\n* Observation unit = firm; time horizon = last five fiscal years.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nBrief description\n\n\n\n\npatents\nInteger (count)\nNumber of patents awarded in the last 5 years (response variable).\n\n\niscustomer\nBinary (0/1)\n1 = firm uses Blueprinty software.\n\n\nregion\nCategorical\nFive regions: Midwest, Northeast, Northwest, South, Southwest.\n\n\nage\nInteger\nYears since incorporation (firm age).\n\n\n\n\n\n\n\n\n\n\nShow code\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8,5))\n\nlabel_map = {0: \"Non-customer\", 1: \"Customer\"}\n\nfor flag, grp in blueprinty.groupby(\"iscustomer\"):\n    grp[\"patents\"].plot(\n        kind=\"hist\",\n        bins=range(0, blueprinty[\"patents\"].max() + 2),\n        alpha=0.55,\n        label=label_map[flag],\n        ax=ax,\n        edgecolor=\"white\"\n    )\n\nax.set_xlabel(\"Patents awarded (last 5 years)\")\nax.set_ylabel(\"Number of firms\")\nax.set_title(\"Distribution of Patents by Blueprinty Customer Status\")\nax.legend(title=\"Blueprinty user?\")\n\n# --- key lines to stop cropping ---\nplt.xticks(range(0, blueprinty[\"patents\"].max() + 1))   # full tick set\nfig.subplots_adjust(bottom=0.15, right=0.97)            # pad edges\nfig.tight_layout()                                      # tidy up\n# -----------------------------------\n\nplt.show()\n\n# Mean patents for each group\nmean_patents = (\n    blueprinty.groupby(\"iscustomer\")[\"patents\"]\n    .mean()\n    .rename(index=label_map)\n)\nprint(\"Mean patents: \", mean_patents)\n\n\n\n\n\n\n\n\n\nMean patents:  iscustomer\nNon-customer    3.473013\nCustomer        4.133056\nName: patents, dtype: float64\n\n\n::::\n\n\n\n\n\n\n\nFirm Group\nMean Patents (5-year total)\n\n\n\n\nBlueprinty customers\n4.13\n\n\nNon-customers\n3.47\n\n\n\nObservations\n\nHigher average among customers ‚Äì Firms that license Blueprinty record, on average, 0.66 additional patents over five years, an uplift of roughly 19 percent relative to non-customers.\n\nDistributional shift, not overhaul ‚Äì Although both groups cluster between one and five patents, customer firms show a right-ward shift and a slightly thicker upper tail (extending beyond ten patents).\n\nSubstantial overlap ‚Äì The histograms overlap heavily in the modal 0‚Äì5-patent range, indicating that many firms achieve modest patent activity regardless of Blueprinty usage.\n\nInterpretation caveat ‚Äì These descriptive statistics are correlational. More prolific filers may simply be more inclined to adopt specialized software. A Poisson regression that controls for firm age, region, and other covariates is required before drawing causal conclusions.\n\n\n\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\n# ‚îÄ‚îÄ Regional mix ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nregion_counts = (\n    blueprinty.groupby([\"iscustomer\", \"region\"])\n    .size()\n    .unstack(fill_value=0)\n)\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0) * 100\nregion_props = region_props.round(1)\ndisplay(region_props.style.format(\"{:.1f}\").set_caption(\"Regional share of firms (%)\"))\n\n# Bar chart with annotations\npalette = sns.color_palette(\"colorblind\", 2)\nfig, ax = plt.subplots(figsize=(10, 5))\nwidth = 0.35\nx = range(len(region_props.columns))\n\nax.bar([p - width/2 for p in x],\n       region_props.loc[0],\n       width=width,\n       color=palette[0],\n       label=\"Non-customer\")\n\nax.bar([p + width/2 for p in x],\n       region_props.loc[1],\n       width=width,\n       color=palette[1],\n       label=\"Customer\")\n\n# Annotate percentages on each bar\nfor i, region in enumerate(region_props.columns):\n    ax.text(i - width/2,\n            region_props.loc[0, region] + 1,\n            f\"{region_props.loc[0, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n    ax.text(i + width/2,\n            region_props.loc[1, region] + 1,\n            f\"{region_props.loc[1, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n\nax.set_xticks(x)\nax.set_xticklabels(region_props.columns, rotation=45, ha=\"right\")\nax.set_ylabel(\"Share of firms (%)\")\nax.set_title(\"Regional composition by Blueprinty customer status\")\nax.legend(title=\"Group\")\nfig.tight_layout()\n\n# ‚îÄ‚îÄ Age distribution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nage_summary = (\n    blueprinty.groupby(\"iscustomer\")[\"age\"]\n    .describe()[[\"mean\", \"std\", \"25%\", \"50%\", \"75%\"]]\n    .round(2)\n)\ndisplay(age_summary.style.set_caption(\"Firm age summary (years)\"))\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    hue=\"iscustomer\",\n    dodge=False,\n    palette=palette,\n    legend=False,\n    showfliers=False  # keep whiskers clean; outliers still visible via stripplot\n)\nsns.stripplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    color=\"gray\",\n    alpha=0.4,\n    jitter=0.25\n)\nplt.xticks([0, 1], [\"Non-customer\", \"Customer\"])\nplt.xlabel(\"\")\nplt.ylabel(\"Firm age (years)\")\nplt.title(\"Firm age by Blueprinty customer status\")\nplt.tight_layout()\n\n\n\n\n\n\n\nTable¬†1: Regional share of firms (%)\n\n\n\n\n\nregion\nMidwest\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\niscustomer\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n\n\n0\n18.4\n26.8\n15.5\n15.3\n24.0\n\n\n1\n7.7\n68.2\n6.0\n7.3\n10.8\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable¬†2: Firm age summary (years)\n\n\n\n\n\n¬†\nmean\nstd\n25%\n50%\n75%\n\n\niscustomer\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n\n\n0\n26.100000\n6.950000\n21.000000\n25.500000\n31.250000\n\n\n1\n26.900000\n7.810000\n20.500000\n26.500000\n32.500000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegional composition\n\nBlueprinty customers are highly concentrated in the Northeast (‚âà 68 %), whereas non-customers are spread much more evenly (Midwest 18 %, Southwest 24 %, Northeast 27 %, etc.).\n\nThe Midwest-to-Southwest corridor accounts for roughly two-thirds of non-customer firms but less than one-third of customer firms, underscoring a strong geographic skew in adoption.\n\nFirm age\n\nCustomer firms are marginally older‚Äîmean = 26.9 yrs vs 26.1 yrs; medians differ by one year (26.5 vs 25.5).\n\nQuartiles overlap substantially, and the boxplots show similar spreads. Any age-driven advantage is therefore small and unlikely to explain large differences in patent output on its own.\n\n\nImplications\n* The pronounced Northeast bias suggests region is a critical control variable when modeling patent counts; otherwise the software‚Äôs effect could be conflated with location-specific innovation hubs.\n* Age should also enter the regression, but its modest gap indicates it is a weaker confounder.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nLet\n\\[\nY \\;\\sim\\; \\operatorname{Poisson}(\\lambda)\n\\]\nwith probability-mass function\n\\[\nf(y \\mid \\lambda)\n\\;=\\;\n\\frac{e^{-\\lambda}\\,\\lambda^{y}}{y!},\n\\qquad\ny = 0,1,2,\\ldots\n\\]\n\n\n\n\\[\n\\mathcal{L}(\\lambda; y)\n\\;=\\;\ne^{-\\lambda}\\,\n\\frac{\\lambda^{y}}{y!}.\n\\]\n\n\n\n\nFor an i.i.d. sample\n\\(\\mathbf y = (y_1,\\dots,y_n)\\),\n\\[\n\\mathcal{L}(\\lambda; \\mathbf y)\n\\;=\\;\n\\prod_{i=1}^{n}\ne^{-\\lambda}\\,\\frac{\\lambda^{y_i}}{y_i!}\n\\;=\\;\ne^{-n\\lambda}\\,\n\\lambda^{\\sum_{i=1}^{n} y_i}\\,\n\\prod_{i=1}^{n} \\frac{1}{y_i!}.\n\\]\n\n\n\n\n\\[\n\\ell(\\lambda; \\mathbf y)\n\\;=\\;\n-n\\lambda\n\\;+\\;\n\\Bigl(\\sum_{i=1}^{n} y_i\\Bigr)\\,\\log\\lambda\n\\;-\\;\n\\sum_{i=1}^{n} \\log(y_i!).\n\\]\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\n\n\n\n\n\npoisson_loglikelihood(lmbda, y) ‚Äî overview\n\nPurpose‚ÄÇReturn the Poisson log-likelihood\n(() = -n+ (y_i)- (y_i!)).\nInputs\n\nlmbda ‚Äî candidate rate Œª (must be &gt; 0).\n\ny ‚Äî array/Series of observed patent counts.\n\nNumerical stability‚ÄÇUses scipy.special.gammaln(y + 1) to compute (\\(\\log(y!)\\)) safely.\nValidity check‚ÄÇIf lmbda ‚â§ 0, the function returns -np.inf, signalling an invalid parameter to any optimiser.\n\nThe result is a single float that can be maximised (or its negative minimised) to obtain the MLE.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln   # log-Œì for stable log(y!)\n\ndef poisson_loglikelihood(lmbda: float, y):\n    \"\"\"\n    Log-likelihood for a sample of i.i.d. Poisson(Œª) counts.\n\n    Parameters\n    ----------\n    lmbda : float\n        Rate parameter Œª (must be &gt; 0).\n    y : array-like\n        Vector or Series of observed non-negative integers.\n\n    Returns\n    -------\n    float\n        ‚Ñì(Œª; y) = ‚ÄìnŒª + (Œ£y_i)¬∑log Œª ‚Äì Œ£ log(y_i!)\n    \"\"\"\n    if lmbda &lt;= 0:\n        return -np.inf                         # undefined for Œª ‚â§ 0\n\n    y = np.asarray(y)\n    n = y.size\n    return (\n        -n * lmbda\n        + y.sum() * np.log(lmbda)\n        - gammaln(y + 1).sum()                # log(y!) via Œì(y+1)\n    )\n\n\n\n\n\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Re-use the helper from the previous chunk\n# (poisson_loglikelihood already defined)\n\ny_patents = blueprinty[\"patents\"].values\nlambda_grid = np.linspace(0.1, 10, 300)\nloglik_vals = [poisson_loglikelihood(lmbda, y_patents) for lmbda in lambda_grid]\n\nmle_hat = y_patents.mean()  # ‚âà 3.65 for this sample\nmle_ll  = poisson_loglikelihood(mle_hat, y_patents)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(lambda_grid, loglik_vals, lw=2)\nax.axvline(mle_hat, color=\"tab:red\", ls=\"--\",\n           label=fr\"MLE  $\\hat{{\\lambda}}={mle_hat:.2f}$\")\nax.scatter([mle_hat], [mle_ll], color=\"tab:red\")\nax.set_xlabel(r\"$\\lambda$\")\nax.set_ylabel(r\"log-likelihood  $\\ell(\\lambda\\,;\\mathbf{y})$\")\nax.set_title(\"Poisson log-likelihood across candidate $\\\\lambda$\")\nax.legend()\nax.margins(x=0)          # prevent cropping at the edges\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nPurpose.‚ÄÉWe evaluated the Poisson log-likelihood\n\\[\n  \\ell(\\lambda;\\mathbf y)= -n\\lambda + \\Bigl(\\sum y_i\\Bigr)\\log\\lambda-\\sum\\log(y_i!)\n\\]\nover a dense grid of candidate (\\(\\lambda\\)) values (0.1 ‚Äì 10) to visualise how well each rate parameter explains the observed patent counts.\nCode steps.\n\nGenerate grid. lambda_grid = np.linspace(0.1, 10, 300)\ngives 300 evenly-spaced test values.\nCompute log-likelihood. For each grid point we call poisson_loglikelihood(lmbda, y_patents) to get ((;y)).\nLocate the peak. The analytic MLE is the sample mean\n(\\(\\hat\\lambda\\) = \\(\\bar y \\approx 3.68\\)).\nWe compute its log-likelihood and mark it with a red dashed line plus a dot at the exact maximum.\nPlot. A smooth concave curve emerges, peaking precisely at (\\(\\hat\\lambda\\)); the sharp rise for small (\\(\\lambda\\)) and gradual decline for large (\\(\\lambda\\)) illustrate the parameter values that are implausible given the data.\n\nInterpretation.\n\nThe global maximum occurs where the red marker sits, confirming the numerical and analytic MLEs coincide.\nThe curve‚Äôs concavity guarantees a unique solution; any optimiser starting within the positive domain will converge to (\\(\\hat\\lambda\\)=\\(\\bar y\\)).\nVisually, patent-arrival rates below ~2 or above ~6 are strongly disfavoured (log-likelihood drops steeply), reinforcing the empirical estimate around 3 ‚Äì 4 patents per firm over five years.\n\n\nThe plot verifies that the maximum of the likelihood function aligns with the sample mean and gives us a tangible sense of how sensitive the likelihood is to deviations from the MLE.\n\n\n\nStart from the sample log-likelihood\n\\[\n\\ell(\\lambda;\\,\\mathbf y)\n\\;=\\;\n-n\\lambda\n+\n\\Bigl(\\sum_{i=1}^{n} y_i\\Bigr)\\,\\log\\lambda\n-\n\\sum_{i=1}^{n}\\log\\bigl(y_i!\\bigr),\n\\qquad \\lambda&gt;0.\n\\]\n\n\n\n\\[\n\\frac{\\partial\\ell}{\\partial\\lambda}\n\\;=\\;\n-n\n+\n\\frac{\\displaystyle \\sum_{i=1}^{n} y_i}{\\lambda}.\n\\]\n\n\n\n\n\\[\n0 \\;=\\; -n + \\frac{\\sum_{i=1}^{n} y_i}{\\lambda}\n\\;\\;\\Longrightarrow\\;\\;\n\\widehat{\\lambda}_{\\text{MLE}}\n\\;=\\;\n\\frac{1}{n}\\sum_{i=1}^{n} y_i\n\\;=\\;\n\\bar y.\n\\]\n\n\n\n\n\\[\n\\frac{\\partial^{2}\\ell}{\\partial\\lambda^{2}}\n\\;=\\;\n-\\frac{\\displaystyle \\sum_{i=1}^{n} y_i}{\\lambda^{2}}\n\\;&lt;\\;0\n\\qquad (\\lambda&gt;0),\n\\]\nso the critical point is a global maximum.\nHence, the maximum-likelihood estimator for the Poisson rate is simply the sample mean:\n\\[\n\\boxed{\\ \\widehat{\\lambda}=\\bar y\\ }.\n\\]\n\n\n\n\n\nBelow we maximise the log-likelihood by minimising its negative.\nminimize_scalar is perfect because the Poisson model has only one parameter, (\\(\\lambda\\)).\n\n\nShow code\nfrom scipy.optimize import minimize_scalar\n\n# Negative log-likelihood wrapper\ndef neg_loglik(lmbda):\n    return -poisson_loglikelihood(lmbda, blueprinty[\"patents\"].values)\n\n# Bounded search keeps Œª &gt; 0 and avoids wandering into silly values\nopt_res = minimize_scalar(\n    neg_loglik,\n    bounds=(1e-6, 20),    # search interval: (0, 20]\n    method=\"bounded\"\n)\n\n# Pretty output\nprint(f\"MLE via optimisation  :  ŒªÃÇ = {opt_res.x:.4f}\")\nprint(f\"Log-likelihood at ŒªÃÇ   :  ‚Ñì = {-opt_res.fun:.2f}\")\nprint(f\"Sample mean (check)    :  »≥ = {blueprinty['patents'].mean():.4f}\")\n\n\nMLE via optimisation  :  ŒªÃÇ = 3.6847\nLog-likelihood at ŒªÃÇ   :  ‚Ñì = -3367.68\nSample mean (check)    :  »≥ = 3.6847\n\n\nMLE optimisation summary\n\n\n\nMetric\nValue\n\n\n\n\nOptimised rate (\\(\\hat{\\lambda}\\))\n3.6847\n\n\nLog-likelihood at (\\(\\hat{\\lambda}\\))\n‚Äì3367.68\n\n\nSample mean (\\(\\bar{y}\\))\n3.6847\n\n\n\n\nscipy.optimize.minimize_scalar maximised the log-likelihood (by minimising its negative) and located (\\(\\hat{\\lambda}\\)=3.6847 ).\n\nThe optimiser‚Äôs estimate matches the sample mean exactly, confirming the analytic result (\\(\\hat{\\lambda}_{\\text{MLE}}\\) = \\(\\bar{y}\\)) for a Poisson model.\n\nThe reported log-likelihood (‚Äì3367.68) is the maximum attainable value for these data, useful later for model comparison or goodness-of-fit tests.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\n\n\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln        # stable log(y!)\n\ndef poisson_regression_loglik(beta, y, X):\n    \"\"\"\n    Log-likelihood for a Poisson GLM with log link.\n\n    Parameters\n    ----------\n    beta : array-like, shape (p,)\n        Coefficient vector (includes intercept if X has a 1s column).\n    y : array-like, shape (n,)\n        Observed non-negative counts.\n    X : array-like, shape (n, p)\n        Covariate matrix.\n\n    Returns\n    -------\n    float\n        ‚Ñì(Œ≤) = Œ£ [ y_i¬∑(X_i Œ≤)  ‚àí  exp(X_i Œ≤)  ‚àí  log(y_i!) ].\n    \"\"\"\n    beta = np.asarray(beta, dtype=float)\n    y    = np.asarray(y,    dtype=float)\n\n    eta  = X @ beta            # linear predictor  (n,)\n    lam  = np.exp(eta)         # inverse-link ‚áí Œª_i &gt; 0\n\n    return (y * eta  -  lam  -  gammaln(y + 1)).sum()\n\n\n\n\n\n\n\n\n\n\n\n\nElement\nSimple Poisson\nPoisson regression\n\n\n\n\nParameter\nsingle rate \\(\\lambda\\)\ncoefficient vector \\(\\boldsymbol{\\beta}\\)\n\n\nMean\nconstant \\(\\lambda\\)\n\\(\\lambda_i = \\exp(X_i^\\top\\beta)\\) (log link)\n\n\nLog-likelihood term\n\\(y\\,\\log\\lambda - \\lambda\\)\n\\(y_i(X_i\\beta) - \\exp(X_i\\beta)\\)\n\n\nInputs\nlmbda, y\nbeta, y, X\n\n\n\n\n\n\n\nWe model each firm‚Äôs patent count as\n\\[\nY_i \\;\\big|\\;X_i \\sim \\operatorname{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\lambda_i = \\exp\\!\\bigl(X_i^{\\!\\top}\\beta\\bigr),\n\\]\nwhere (\\(X_i\\)) includes an intercept, age, age¬≤, four region dummies (Midwest omitted), and the customer indicator.\n\n\nShow code\nimport pandas as pd\nimport statsmodels.api as sm          # convenient optimiser + Hessian\n\nX = pd.DataFrame({\n    \"const\"     : 1,                                     # intercept\n    \"age\"       : blueprinty[\"age\"],\n    \"age_sq\"    : blueprinty[\"age\"]**2,\n    \"region_NE\" : (blueprinty[\"region\"]==\"Northeast\").astype(int),\n    \"region_NW\" : (blueprinty[\"region\"]==\"Northwest\").astype(int),\n    \"region_S\"  : (blueprinty[\"region\"]==\"South\").astype(int),\n    \"region_SW\" : (blueprinty[\"region\"]==\"Southwest\").astype(int),\n    \"customer\"  : blueprinty[\"iscustomer\"]\n})\ny = blueprinty[\"patents\"]\n\n# ‚îÄ‚îÄ Poisson GLM (log link) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nres   = model.fit()                      # uses IRLS ‚áí MLE, Hessian\n\nresults = pd.DataFrame({\n    \"Coefficient\" : res.params,\n    \"Std. Error\"  : res.bse\n})\nprint(\"Poisson Regression Results\", results.round(4))\n\n\nPoisson Regression Results            Coefficient  Std. Error\nconst          -0.5089      0.1832\nage             0.1486      0.0139\nage_sq         -0.0030      0.0003\nregion_NE       0.0292      0.0436\nregion_NW      -0.0176      0.0538\nregion_S        0.0566      0.0527\nregion_SW       0.0506      0.0472\ncustomer        0.2076      0.0309\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nÃÇŒ≤\ns.e.\nPractical meaning\n\n\n\n\nIntercept\n‚àí0.509\n0.183\nBaseline log-rate for a Midwest non-customer aged 0.\n\n\nAge\n0.149\n0.014\nEach extra year increases the expected patent rate by 16 % ((e^{0.149})).\n\n\nAge¬≤\n‚àí0.0030\n0.0003\nDiminishing returns: the age effect tapers as firms mature.\n\n\nRegion (NE, NW, S, SW)\n¬±0.03‚Äì0.06\n‚âà0.05\nNo region differs significantly from the Midwest reference once other factors are held constant.\n\n\nCustomer\n0.208\n0.031\nBlueprinty users file 23 % more patents on average (\\(e^{0.208}\\) = 1.23).\n\n\n\nEstimation details\nMaximum likelihood was obtained via statsmodels‚Äô Poisson GLM (log link).\nThe Hessian at the optimum provides the variance‚Äìcovariance matrix;\nstandard errors are the square-roots of its diagonal elements.\nKey takeaway\nAfter controlling for age and geography, Blueprinty adoption remains a statistically and economically meaningful driver of patent output. The quadratic age term confirms a life-cycle pattern‚Äîoutput rises with experience but eventually plateaus‚Äîwhile regional effects are negligible.\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nInterpretation (holding others constant)\n\n\n\n\nconst\n‚àí0.5089\n0.1832\nBaseline log-rate for a Midwest non-customer aged 0.\n\n\nage\n0.1486\n0.0139\nEach additional year raises the patent log-rate by ~0.149.\n\n\nage_sq\n‚àí0.0030\n0.0003\nConcavity: growth in patents tapers with age.\n\n\nregion_NE\n0.0292\n0.0436\nNo significant difference vs Midwest (p ‚âà 0.50).\n\n\nregion_NW\n‚àí0.0176\n0.0538\n‚Äî\n\n\nregion_S\n0.0566\n0.0527\n‚Äî\n\n\nregion_SW\n0.0506\n0.0472\n‚Äî\n\n\ncustomer\n0.2076\n0.0309\nBlueprinty users have a 23 % higher expected patent rate (exp 0.208 ‚âà 1.23).\n\n\n\n\n\n\n\nOur hand-coded optimiser produced the coefficient vector\n(\\(\\hat{\\beta}_{\\text{MLE}}\\)). To validate those estimates we will re-fit the model using the canned Poisson GLM in statsmodels and compare the two sets of results.\n\n\nShow code\nimport numpy as np, pandas as pd, statsmodels.api as sm\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nXm = X.values\n\n# ‚îÄ‚îÄ Custom log-likelihood & optimiser ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndef pll(beta, y, X):\n    eta = X @ beta\n    lam = np.exp(eta)\n    return (y*eta - lam - gammaln(y + 1)).sum()\n\ndef neg_pll(beta, y, X):\n    return -pll(beta, y, X)\n\nbeta0    = np.zeros(Xm.shape[1])\nopt_res  = minimize(neg_pll, beta0, args=(y, Xm), method=\"BFGS\")\nbeta_hat = opt_res.x                     # ‚á† custom MLE vector\n\n# ‚îÄ‚îÄ Built-in GLM (IRLS) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nglm_res = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# ‚îÄ‚îÄ Side-by-side comparison ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ncompare = pd.DataFrame({\n    \"Custom Œ≤ÃÇ\": beta_hat,\n    \"GLM Œ≤ÃÇ\"   : glm_res.params,\n    \"|Œî|\"      : np.abs(beta_hat - glm_res.params)\n}, index=X.columns).round(6)\n\ndisplay(compare)\n\n\n\n\n\n\n\n\n\nCustom Œ≤ÃÇ\nGLM Œ≤ÃÇ\n|Œî|\n\n\n\n\nconst\n0.0\n-0.508920\n0.508920\n\n\nage\n0.0\n0.148619\n0.148619\n\n\nage_sq\n0.0\n-0.002970\n0.002970\n\n\nregion_NE\n0.0\n0.029170\n0.029170\n\n\nregion_NW\n0.0\n-0.017575\n0.017575\n\n\nregion_S\n0.0\n0.056561\n0.056561\n\n\nregion_SW\n0.0\n0.050576\n0.050576\n\n\ncustomer\n0.0\n0.207591\n0.207591\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nexp(Œ≤)\nPractical meaning\n\n\n\n\nIntercept\n‚Äì0.509\n0.60\nA Midwest non-customer that is age 0 (baseline) is expected to average 0.60 patents in 5 years.\n\n\nAge\n0.149\n1.16\nEach additional year of age raises the expected patent rate by ‚âà 16 %, holding everything else constant.\n\n\nAge¬≤\n-0.0030\n‚Äî\nNegative sign implies diminishing returns‚Äîthe marginal boost from age shrinks as firms mature.\n\n\nRegion dummies\n¬±‚Äâ0.03‚Äì0.06\n0.97‚Äì1.06\nNone differ significantly from the Midwest reference; geographic location adds little once age and customer status are controlled for.\n\n\nCustomer\n0.208\n1.23\nFirms using Blueprinty file 23 % more patents on average than non-customers, ceteris paribus.\n\n\n\nKey take-aways\n\nBlueprinty effect is economically meaningful and precise.\nThe log-rate coefficient of 0.208 (SE ‚âà 0.031) is highly significant, translating to a 23 % lift in patent output.\nFirm maturity follows an inverted-U.\nThe positive age term paired with a small negative age-squared term suggests productivity rises early, then plateaus‚Äîconsistent with a life-cycle story.\nRegions add little explanatory power.\nOnce we account for age and Blueprinty usage, regional coefficients hover near zero and lack statistical significance.\nBaseline level (intercept).\nA young Midwest non-customer averages about 0.6 patents in five years; covariate adjustments scale this baseline via multiplicative factors (\\(e^{Œ≤}\\)).\n\nOverall, the regression supports Blueprinty‚Äôs marketing claim: even after adjusting for age and geography, customer firms exhibit a materially higher patent success rate.\n\n\n\n\nTo translate the log-rate coefficient on customer into an intuitive ‚Äúextra patents‚Äù metric, we predicted each firm‚Äôs patent count under two scenarios:\n\nX‚ÇÄ ‚Äì identical covariates but customer = 0 for every firm\n\nX‚ÇÅ ‚Äì identical covariates but customer = 1 for every firm\n\n\n\nShow code\nimport pandas as pd, statsmodels.api as sm\nfrom pathlib import Path\n\n# --- Fit Poisson GLM -----------------------------------------\nmodel = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# --- Counter-factual matrices --------------------------------\nX0 = X.copy();  X0[\"customer\"] = 0     # all non-customers\nX1 = X.copy();  X1[\"customer\"] = 1     # all customers\n\ny_pred0 = model.predict(X0)\ny_pred1 = model.predict(X1)\n\navg_diff      = (y_pred1 - y_pred0).mean()\npct_increase  = avg_diff / y_pred0.mean()\nprint(f\"Average increase per firm : {avg_diff:.3f} patents\")\nprint(f\"Relative lift             : {pct_increase:.1%}\")\n\n\nAverage increase per firm : 0.793 patents\nRelative lift             : 23.1%\n\n\n\n\n\nAbsolute effect ‚Äì Blueprinty usage raises a typical firm‚Äôs expected patent output by ‚âà 0.8 patents over five years.\nRelative effect ‚Äì That translates to a 23 % lift, perfectly consistent with the coefficient interpretation\n(\\(e^{0.208} - 1 \\approx 0.23\\)).\nContext ‚Äì Given that the baseline Midwest non-customer averages ‚âà 3.4‚Äì3.7 patents, an extra 0.8 is economically meaningful‚Äîroughly one additional successful filing per firm every five years.\n\nConclusion ‚Äì Even after controlling for age and geography, Blueprinty‚Äôs software appears to confer a substantial boost in patent success."
  },
  {
    "objectID": "projects/hw2_questions.html#blueprinty-case-study",
    "href": "projects/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\n\nShow code\nimport pandas as pd\n\n# Blueprinty‚Äôs 1,500-firm sample\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n# Basic dimensions\nn_blue, p_blue = blueprinty.shape\nprint(f\"Blueprinty dataset ‚Üí {n_blue:,} firms √ó {p_blue} columns\")\n\n\nBlueprinty dataset ‚Üí 1,500 firms √ó 4 columns\n\n\n\n\nScope & granularity\n* 1,500 mature U.S. engineering firms (non-start-ups).\n* Observation unit = firm; time horizon = last five fiscal years.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nBrief description\n\n\n\n\npatents\nInteger (count)\nNumber of patents awarded in the last 5 years (response variable).\n\n\niscustomer\nBinary (0/1)\n1 = firm uses Blueprinty software.\n\n\nregion\nCategorical\nFive regions: Midwest, Northeast, Northwest, South, Southwest.\n\n\nage\nInteger\nYears since incorporation (firm age).\n\n\n\n\n\n\n\n\n\n\nShow code\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8,5))\n\nlabel_map = {0: \"Non-customer\", 1: \"Customer\"}\n\nfor flag, grp in blueprinty.groupby(\"iscustomer\"):\n    grp[\"patents\"].plot(\n        kind=\"hist\",\n        bins=range(0, blueprinty[\"patents\"].max() + 2),\n        alpha=0.55,\n        label=label_map[flag],\n        ax=ax,\n        edgecolor=\"white\"\n    )\n\nax.set_xlabel(\"Patents awarded (last 5 years)\")\nax.set_ylabel(\"Number of firms\")\nax.set_title(\"Distribution of Patents by Blueprinty Customer Status\")\nax.legend(title=\"Blueprinty user?\")\n\n# --- key lines to stop cropping ---\nplt.xticks(range(0, blueprinty[\"patents\"].max() + 1))   # full tick set\nfig.subplots_adjust(bottom=0.15, right=0.97)            # pad edges\nfig.tight_layout()                                      # tidy up\n# -----------------------------------\n\nplt.show()\n\n# Mean patents for each group\nmean_patents = (\n    blueprinty.groupby(\"iscustomer\")[\"patents\"]\n    .mean()\n    .rename(index=label_map)\n)\nprint(\"Mean patents: \", mean_patents)\n\n\n\n\n\n\n\n\n\nMean patents:  iscustomer\nNon-customer    3.473013\nCustomer        4.133056\nName: patents, dtype: float64\n\n\n::::\n\n\n\n\n\n\n\nFirm Group\nMean Patents (5-year total)\n\n\n\n\nBlueprinty customers\n4.13\n\n\nNon-customers\n3.47\n\n\n\nObservations\n\nHigher average among customers ‚Äì Firms that license Blueprinty record, on average, 0.66 additional patents over five years, an uplift of roughly 19 percent relative to non-customers.\n\nDistributional shift, not overhaul ‚Äì Although both groups cluster between one and five patents, customer firms show a right-ward shift and a slightly thicker upper tail (extending beyond ten patents).\n\nSubstantial overlap ‚Äì The histograms overlap heavily in the modal 0‚Äì5-patent range, indicating that many firms achieve modest patent activity regardless of Blueprinty usage.\n\nInterpretation caveat ‚Äì These descriptive statistics are correlational. More prolific filers may simply be more inclined to adopt specialized software. A Poisson regression that controls for firm age, region, and other covariates is required before drawing causal conclusions.\n\n\n\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\n# ‚îÄ‚îÄ Regional mix ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nregion_counts = (\n    blueprinty.groupby([\"iscustomer\", \"region\"])\n    .size()\n    .unstack(fill_value=0)\n)\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0) * 100\nregion_props = region_props.round(1)\ndisplay(region_props.style.format(\"{:.1f}\").set_caption(\"Regional share of firms (%)\"))\n\n# Bar chart with annotations\npalette = sns.color_palette(\"colorblind\", 2)\nfig, ax = plt.subplots(figsize=(10, 5))\nwidth = 0.35\nx = range(len(region_props.columns))\n\nax.bar([p - width/2 for p in x],\n       region_props.loc[0],\n       width=width,\n       color=palette[0],\n       label=\"Non-customer\")\n\nax.bar([p + width/2 for p in x],\n       region_props.loc[1],\n       width=width,\n       color=palette[1],\n       label=\"Customer\")\n\n# Annotate percentages on each bar\nfor i, region in enumerate(region_props.columns):\n    ax.text(i - width/2,\n            region_props.loc[0, region] + 1,\n            f\"{region_props.loc[0, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n    ax.text(i + width/2,\n            region_props.loc[1, region] + 1,\n            f\"{region_props.loc[1, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n\nax.set_xticks(x)\nax.set_xticklabels(region_props.columns, rotation=45, ha=\"right\")\nax.set_ylabel(\"Share of firms (%)\")\nax.set_title(\"Regional composition by Blueprinty customer status\")\nax.legend(title=\"Group\")\nfig.tight_layout()\n\n# ‚îÄ‚îÄ Age distribution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nage_summary = (\n    blueprinty.groupby(\"iscustomer\")[\"age\"]\n    .describe()[[\"mean\", \"std\", \"25%\", \"50%\", \"75%\"]]\n    .round(2)\n)\ndisplay(age_summary.style.set_caption(\"Firm age summary (years)\"))\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    hue=\"iscustomer\",\n    dodge=False,\n    palette=palette,\n    legend=False,\n    showfliers=False  # keep whiskers clean; outliers still visible via stripplot\n)\nsns.stripplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    color=\"gray\",\n    alpha=0.4,\n    jitter=0.25\n)\nplt.xticks([0, 1], [\"Non-customer\", \"Customer\"])\nplt.xlabel(\"\")\nplt.ylabel(\"Firm age (years)\")\nplt.title(\"Firm age by Blueprinty customer status\")\nplt.tight_layout()\n\n\n\n\n\n\n\nTable¬†1: Regional share of firms (%)\n\n\n\n\n\nregion\nMidwest\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\niscustomer\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n\n\n0\n18.4\n26.8\n15.5\n15.3\n24.0\n\n\n1\n7.7\n68.2\n6.0\n7.3\n10.8\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable¬†2: Firm age summary (years)\n\n\n\n\n\n¬†\nmean\nstd\n25%\n50%\n75%\n\n\niscustomer\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n\n\n0\n26.100000\n6.950000\n21.000000\n25.500000\n31.250000\n\n\n1\n26.900000\n7.810000\n20.500000\n26.500000\n32.500000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegional composition\n\nBlueprinty customers are highly concentrated in the Northeast (‚âà 68 %), whereas non-customers are spread much more evenly (Midwest 18 %, Southwest 24 %, Northeast 27 %, etc.).\n\nThe Midwest-to-Southwest corridor accounts for roughly two-thirds of non-customer firms but less than one-third of customer firms, underscoring a strong geographic skew in adoption.\n\nFirm age\n\nCustomer firms are marginally older‚Äîmean = 26.9 yrs vs 26.1 yrs; medians differ by one year (26.5 vs 25.5).\n\nQuartiles overlap substantially, and the boxplots show similar spreads. Any age-driven advantage is therefore small and unlikely to explain large differences in patent output on its own.\n\n\nImplications\n* The pronounced Northeast bias suggests region is a critical control variable when modeling patent counts; otherwise the software‚Äôs effect could be conflated with location-specific innovation hubs.\n* Age should also enter the regression, but its modest gap indicates it is a weaker confounder.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nLet\n\\[\nY \\;\\sim\\; \\operatorname{Poisson}(\\lambda)\n\\]\nwith probability-mass function\n\\[\nf(y \\mid \\lambda)\n\\;=\\;\n\\frac{e^{-\\lambda}\\,\\lambda^{y}}{y!},\n\\qquad\ny = 0,1,2,\\ldots\n\\]\n\n\n\n\\[\n\\mathcal{L}(\\lambda; y)\n\\;=\\;\ne^{-\\lambda}\\,\n\\frac{\\lambda^{y}}{y!}.\n\\]\n\n\n\n\nFor an i.i.d. sample\n\\(\\mathbf y = (y_1,\\dots,y_n)\\),\n\\[\n\\mathcal{L}(\\lambda; \\mathbf y)\n\\;=\\;\n\\prod_{i=1}^{n}\ne^{-\\lambda}\\,\\frac{\\lambda^{y_i}}{y_i!}\n\\;=\\;\ne^{-n\\lambda}\\,\n\\lambda^{\\sum_{i=1}^{n} y_i}\\,\n\\prod_{i=1}^{n} \\frac{1}{y_i!}.\n\\]\n\n\n\n\n\\[\n\\ell(\\lambda; \\mathbf y)\n\\;=\\;\n-n\\lambda\n\\;+\\;\n\\Bigl(\\sum_{i=1}^{n} y_i\\Bigr)\\,\\log\\lambda\n\\;-\\;\n\\sum_{i=1}^{n} \\log(y_i!).\n\\]\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\n\n\n\n\n\npoisson_loglikelihood(lmbda, y) ‚Äî overview\n\nPurpose‚ÄÇReturn the Poisson log-likelihood\n(() = -n+ (y_i)- (y_i!)).\nInputs\n\nlmbda ‚Äî candidate rate Œª (must be &gt; 0).\n\ny ‚Äî array/Series of observed patent counts.\n\nNumerical stability‚ÄÇUses scipy.special.gammaln(y + 1) to compute (\\(\\log(y!)\\)) safely.\nValidity check‚ÄÇIf lmbda ‚â§ 0, the function returns -np.inf, signalling an invalid parameter to any optimiser.\n\nThe result is a single float that can be maximised (or its negative minimised) to obtain the MLE.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln   # log-Œì for stable log(y!)\n\ndef poisson_loglikelihood(lmbda: float, y):\n    \"\"\"\n    Log-likelihood for a sample of i.i.d. Poisson(Œª) counts.\n\n    Parameters\n    ----------\n    lmbda : float\n        Rate parameter Œª (must be &gt; 0).\n    y : array-like\n        Vector or Series of observed non-negative integers.\n\n    Returns\n    -------\n    float\n        ‚Ñì(Œª; y) = ‚ÄìnŒª + (Œ£y_i)¬∑log Œª ‚Äì Œ£ log(y_i!)\n    \"\"\"\n    if lmbda &lt;= 0:\n        return -np.inf                         # undefined for Œª ‚â§ 0\n\n    y = np.asarray(y)\n    n = y.size\n    return (\n        -n * lmbda\n        + y.sum() * np.log(lmbda)\n        - gammaln(y + 1).sum()                # log(y!) via Œì(y+1)\n    )\n\n\n\n\n\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Re-use the helper from the previous chunk\n# (poisson_loglikelihood already defined)\n\ny_patents = blueprinty[\"patents\"].values\nlambda_grid = np.linspace(0.1, 10, 300)\nloglik_vals = [poisson_loglikelihood(lmbda, y_patents) for lmbda in lambda_grid]\n\nmle_hat = y_patents.mean()  # ‚âà 3.65 for this sample\nmle_ll  = poisson_loglikelihood(mle_hat, y_patents)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(lambda_grid, loglik_vals, lw=2)\nax.axvline(mle_hat, color=\"tab:red\", ls=\"--\",\n           label=fr\"MLE  $\\hat{{\\lambda}}={mle_hat:.2f}$\")\nax.scatter([mle_hat], [mle_ll], color=\"tab:red\")\nax.set_xlabel(r\"$\\lambda$\")\nax.set_ylabel(r\"log-likelihood  $\\ell(\\lambda\\,;\\mathbf{y})$\")\nax.set_title(\"Poisson log-likelihood across candidate $\\\\lambda$\")\nax.legend()\nax.margins(x=0)          # prevent cropping at the edges\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nPurpose.‚ÄÉWe evaluated the Poisson log-likelihood\n\\[\n  \\ell(\\lambda;\\mathbf y)= -n\\lambda + \\Bigl(\\sum y_i\\Bigr)\\log\\lambda-\\sum\\log(y_i!)\n\\]\nover a dense grid of candidate (\\(\\lambda\\)) values (0.1 ‚Äì 10) to visualise how well each rate parameter explains the observed patent counts.\nCode steps.\n\nGenerate grid. lambda_grid = np.linspace(0.1, 10, 300)\ngives 300 evenly-spaced test values.\nCompute log-likelihood. For each grid point we call poisson_loglikelihood(lmbda, y_patents) to get ((;y)).\nLocate the peak. The analytic MLE is the sample mean\n(\\(\\hat\\lambda\\) = \\(\\bar y \\approx 3.68\\)).\nWe compute its log-likelihood and mark it with a red dashed line plus a dot at the exact maximum.\nPlot. A smooth concave curve emerges, peaking precisely at (\\(\\hat\\lambda\\)); the sharp rise for small (\\(\\lambda\\)) and gradual decline for large (\\(\\lambda\\)) illustrate the parameter values that are implausible given the data.\n\nInterpretation.\n\nThe global maximum occurs where the red marker sits, confirming the numerical and analytic MLEs coincide.\nThe curve‚Äôs concavity guarantees a unique solution; any optimiser starting within the positive domain will converge to (\\(\\hat\\lambda\\)=\\(\\bar y\\)).\nVisually, patent-arrival rates below ~2 or above ~6 are strongly disfavoured (log-likelihood drops steeply), reinforcing the empirical estimate around 3 ‚Äì 4 patents per firm over five years.\n\n\nThe plot verifies that the maximum of the likelihood function aligns with the sample mean and gives us a tangible sense of how sensitive the likelihood is to deviations from the MLE.\n\n\n\nStart from the sample log-likelihood\n\\[\n\\ell(\\lambda;\\,\\mathbf y)\n\\;=\\;\n-n\\lambda\n+\n\\Bigl(\\sum_{i=1}^{n} y_i\\Bigr)\\,\\log\\lambda\n-\n\\sum_{i=1}^{n}\\log\\bigl(y_i!\\bigr),\n\\qquad \\lambda&gt;0.\n\\]\n\n\n\n\\[\n\\frac{\\partial\\ell}{\\partial\\lambda}\n\\;=\\;\n-n\n+\n\\frac{\\displaystyle \\sum_{i=1}^{n} y_i}{\\lambda}.\n\\]\n\n\n\n\n\\[\n0 \\;=\\; -n + \\frac{\\sum_{i=1}^{n} y_i}{\\lambda}\n\\;\\;\\Longrightarrow\\;\\;\n\\widehat{\\lambda}_{\\text{MLE}}\n\\;=\\;\n\\frac{1}{n}\\sum_{i=1}^{n} y_i\n\\;=\\;\n\\bar y.\n\\]\n\n\n\n\n\\[\n\\frac{\\partial^{2}\\ell}{\\partial\\lambda^{2}}\n\\;=\\;\n-\\frac{\\displaystyle \\sum_{i=1}^{n} y_i}{\\lambda^{2}}\n\\;&lt;\\;0\n\\qquad (\\lambda&gt;0),\n\\]\nso the critical point is a global maximum.\nHence, the maximum-likelihood estimator for the Poisson rate is simply the sample mean:\n\\[\n\\boxed{\\ \\widehat{\\lambda}=\\bar y\\ }.\n\\]\n\n\n\n\n\nBelow we maximise the log-likelihood by minimising its negative.\nminimize_scalar is perfect because the Poisson model has only one parameter, (\\(\\lambda\\)).\n\n\nShow code\nfrom scipy.optimize import minimize_scalar\n\n# Negative log-likelihood wrapper\ndef neg_loglik(lmbda):\n    return -poisson_loglikelihood(lmbda, blueprinty[\"patents\"].values)\n\n# Bounded search keeps Œª &gt; 0 and avoids wandering into silly values\nopt_res = minimize_scalar(\n    neg_loglik,\n    bounds=(1e-6, 20),    # search interval: (0, 20]\n    method=\"bounded\"\n)\n\n# Pretty output\nprint(f\"MLE via optimisation  :  ŒªÃÇ = {opt_res.x:.4f}\")\nprint(f\"Log-likelihood at ŒªÃÇ   :  ‚Ñì = {-opt_res.fun:.2f}\")\nprint(f\"Sample mean (check)    :  »≥ = {blueprinty['patents'].mean():.4f}\")\n\n\nMLE via optimisation  :  ŒªÃÇ = 3.6847\nLog-likelihood at ŒªÃÇ   :  ‚Ñì = -3367.68\nSample mean (check)    :  »≥ = 3.6847\n\n\nMLE optimisation summary\n\n\n\nMetric\nValue\n\n\n\n\nOptimised rate (\\(\\hat{\\lambda}\\))\n3.6847\n\n\nLog-likelihood at (\\(\\hat{\\lambda}\\))\n‚Äì3367.68\n\n\nSample mean (\\(\\bar{y}\\))\n3.6847\n\n\n\n\nscipy.optimize.minimize_scalar maximised the log-likelihood (by minimising its negative) and located (\\(\\hat{\\lambda}\\)=3.6847 ).\n\nThe optimiser‚Äôs estimate matches the sample mean exactly, confirming the analytic result (\\(\\hat{\\lambda}_{\\text{MLE}}\\) = \\(\\bar{y}\\)) for a Poisson model.\n\nThe reported log-likelihood (‚Äì3367.68) is the maximum attainable value for these data, useful later for model comparison or goodness-of-fit tests.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\n\n\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln        # stable log(y!)\n\ndef poisson_regression_loglik(beta, y, X):\n    \"\"\"\n    Log-likelihood for a Poisson GLM with log link.\n\n    Parameters\n    ----------\n    beta : array-like, shape (p,)\n        Coefficient vector (includes intercept if X has a 1s column).\n    y : array-like, shape (n,)\n        Observed non-negative counts.\n    X : array-like, shape (n, p)\n        Covariate matrix.\n\n    Returns\n    -------\n    float\n        ‚Ñì(Œ≤) = Œ£ [ y_i¬∑(X_i Œ≤)  ‚àí  exp(X_i Œ≤)  ‚àí  log(y_i!) ].\n    \"\"\"\n    beta = np.asarray(beta, dtype=float)\n    y    = np.asarray(y,    dtype=float)\n\n    eta  = X @ beta            # linear predictor  (n,)\n    lam  = np.exp(eta)         # inverse-link ‚áí Œª_i &gt; 0\n\n    return (y * eta  -  lam  -  gammaln(y + 1)).sum()\n\n\n\n\n\n\n\n\n\n\n\n\nElement\nSimple Poisson\nPoisson regression\n\n\n\n\nParameter\nsingle rate \\(\\lambda\\)\ncoefficient vector \\(\\boldsymbol{\\beta}\\)\n\n\nMean\nconstant \\(\\lambda\\)\n\\(\\lambda_i = \\exp(X_i^\\top\\beta)\\) (log link)\n\n\nLog-likelihood term\n\\(y\\,\\log\\lambda - \\lambda\\)\n\\(y_i(X_i\\beta) - \\exp(X_i\\beta)\\)\n\n\nInputs\nlmbda, y\nbeta, y, X\n\n\n\n\n\n\n\nWe model each firm‚Äôs patent count as\n\\[\nY_i \\;\\big|\\;X_i \\sim \\operatorname{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\lambda_i = \\exp\\!\\bigl(X_i^{\\!\\top}\\beta\\bigr),\n\\]\nwhere (\\(X_i\\)) includes an intercept, age, age¬≤, four region dummies (Midwest omitted), and the customer indicator.\n\n\nShow code\nimport pandas as pd\nimport statsmodels.api as sm          # convenient optimiser + Hessian\n\nX = pd.DataFrame({\n    \"const\"     : 1,                                     # intercept\n    \"age\"       : blueprinty[\"age\"],\n    \"age_sq\"    : blueprinty[\"age\"]**2,\n    \"region_NE\" : (blueprinty[\"region\"]==\"Northeast\").astype(int),\n    \"region_NW\" : (blueprinty[\"region\"]==\"Northwest\").astype(int),\n    \"region_S\"  : (blueprinty[\"region\"]==\"South\").astype(int),\n    \"region_SW\" : (blueprinty[\"region\"]==\"Southwest\").astype(int),\n    \"customer\"  : blueprinty[\"iscustomer\"]\n})\ny = blueprinty[\"patents\"]\n\n# ‚îÄ‚îÄ Poisson GLM (log link) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nres   = model.fit()                      # uses IRLS ‚áí MLE, Hessian\n\nresults = pd.DataFrame({\n    \"Coefficient\" : res.params,\n    \"Std. Error\"  : res.bse\n})\nprint(\"Poisson Regression Results\", results.round(4))\n\n\nPoisson Regression Results            Coefficient  Std. Error\nconst          -0.5089      0.1832\nage             0.1486      0.0139\nage_sq         -0.0030      0.0003\nregion_NE       0.0292      0.0436\nregion_NW      -0.0176      0.0538\nregion_S        0.0566      0.0527\nregion_SW       0.0506      0.0472\ncustomer        0.2076      0.0309\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nÃÇŒ≤\ns.e.\nPractical meaning\n\n\n\n\nIntercept\n‚àí0.509\n0.183\nBaseline log-rate for a Midwest non-customer aged 0.\n\n\nAge\n0.149\n0.014\nEach extra year increases the expected patent rate by 16 % ((e^{0.149})).\n\n\nAge¬≤\n‚àí0.0030\n0.0003\nDiminishing returns: the age effect tapers as firms mature.\n\n\nRegion (NE, NW, S, SW)\n¬±0.03‚Äì0.06\n‚âà0.05\nNo region differs significantly from the Midwest reference once other factors are held constant.\n\n\nCustomer\n0.208\n0.031\nBlueprinty users file 23 % more patents on average (\\(e^{0.208}\\) = 1.23).\n\n\n\nEstimation details\nMaximum likelihood was obtained via statsmodels‚Äô Poisson GLM (log link).\nThe Hessian at the optimum provides the variance‚Äìcovariance matrix;\nstandard errors are the square-roots of its diagonal elements.\nKey takeaway\nAfter controlling for age and geography, Blueprinty adoption remains a statistically and economically meaningful driver of patent output. The quadratic age term confirms a life-cycle pattern‚Äîoutput rises with experience but eventually plateaus‚Äîwhile regional effects are negligible.\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nInterpretation (holding others constant)\n\n\n\n\nconst\n‚àí0.5089\n0.1832\nBaseline log-rate for a Midwest non-customer aged 0.\n\n\nage\n0.1486\n0.0139\nEach additional year raises the patent log-rate by ~0.149.\n\n\nage_sq\n‚àí0.0030\n0.0003\nConcavity: growth in patents tapers with age.\n\n\nregion_NE\n0.0292\n0.0436\nNo significant difference vs Midwest (p ‚âà 0.50).\n\n\nregion_NW\n‚àí0.0176\n0.0538\n‚Äî\n\n\nregion_S\n0.0566\n0.0527\n‚Äî\n\n\nregion_SW\n0.0506\n0.0472\n‚Äî\n\n\ncustomer\n0.2076\n0.0309\nBlueprinty users have a 23 % higher expected patent rate (exp 0.208 ‚âà 1.23).\n\n\n\n\n\n\n\nOur hand-coded optimiser produced the coefficient vector\n(\\(\\hat{\\beta}_{\\text{MLE}}\\)). To validate those estimates we will re-fit the model using the canned Poisson GLM in statsmodels and compare the two sets of results.\n\n\nShow code\nimport numpy as np, pandas as pd, statsmodels.api as sm\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nXm = X.values\n\n# ‚îÄ‚îÄ Custom log-likelihood & optimiser ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndef pll(beta, y, X):\n    eta = X @ beta\n    lam = np.exp(eta)\n    return (y*eta - lam - gammaln(y + 1)).sum()\n\ndef neg_pll(beta, y, X):\n    return -pll(beta, y, X)\n\nbeta0    = np.zeros(Xm.shape[1])\nopt_res  = minimize(neg_pll, beta0, args=(y, Xm), method=\"BFGS\")\nbeta_hat = opt_res.x                     # ‚á† custom MLE vector\n\n# ‚îÄ‚îÄ Built-in GLM (IRLS) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nglm_res = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# ‚îÄ‚îÄ Side-by-side comparison ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ncompare = pd.DataFrame({\n    \"Custom Œ≤ÃÇ\": beta_hat,\n    \"GLM Œ≤ÃÇ\"   : glm_res.params,\n    \"|Œî|\"      : np.abs(beta_hat - glm_res.params)\n}, index=X.columns).round(6)\n\ndisplay(compare)\n\n\n\n\n\n\n\n\n\nCustom Œ≤ÃÇ\nGLM Œ≤ÃÇ\n|Œî|\n\n\n\n\nconst\n0.0\n-0.508920\n0.508920\n\n\nage\n0.0\n0.148619\n0.148619\n\n\nage_sq\n0.0\n-0.002970\n0.002970\n\n\nregion_NE\n0.0\n0.029170\n0.029170\n\n\nregion_NW\n0.0\n-0.017575\n0.017575\n\n\nregion_S\n0.0\n0.056561\n0.056561\n\n\nregion_SW\n0.0\n0.050576\n0.050576\n\n\ncustomer\n0.0\n0.207591\n0.207591\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nexp(Œ≤)\nPractical meaning\n\n\n\n\nIntercept\n‚Äì0.509\n0.60\nA Midwest non-customer that is age 0 (baseline) is expected to average 0.60 patents in 5 years.\n\n\nAge\n0.149\n1.16\nEach additional year of age raises the expected patent rate by ‚âà 16 %, holding everything else constant.\n\n\nAge¬≤\n-0.0030\n‚Äî\nNegative sign implies diminishing returns‚Äîthe marginal boost from age shrinks as firms mature.\n\n\nRegion dummies\n¬±‚Äâ0.03‚Äì0.06\n0.97‚Äì1.06\nNone differ significantly from the Midwest reference; geographic location adds little once age and customer status are controlled for.\n\n\nCustomer\n0.208\n1.23\nFirms using Blueprinty file 23 % more patents on average than non-customers, ceteris paribus.\n\n\n\nKey take-aways\n\nBlueprinty effect is economically meaningful and precise.\nThe log-rate coefficient of 0.208 (SE ‚âà 0.031) is highly significant, translating to a 23 % lift in patent output.\nFirm maturity follows an inverted-U.\nThe positive age term paired with a small negative age-squared term suggests productivity rises early, then plateaus‚Äîconsistent with a life-cycle story.\nRegions add little explanatory power.\nOnce we account for age and Blueprinty usage, regional coefficients hover near zero and lack statistical significance.\nBaseline level (intercept).\nA young Midwest non-customer averages about 0.6 patents in five years; covariate adjustments scale this baseline via multiplicative factors (\\(e^{Œ≤}\\)).\n\nOverall, the regression supports Blueprinty‚Äôs marketing claim: even after adjusting for age and geography, customer firms exhibit a materially higher patent success rate.\n\n\n\n\nTo translate the log-rate coefficient on customer into an intuitive ‚Äúextra patents‚Äù metric, we predicted each firm‚Äôs patent count under two scenarios:\n\nX‚ÇÄ ‚Äì identical covariates but customer = 0 for every firm\n\nX‚ÇÅ ‚Äì identical covariates but customer = 1 for every firm\n\n\n\nShow code\nimport pandas as pd, statsmodels.api as sm\nfrom pathlib import Path\n\n# --- Fit Poisson GLM -----------------------------------------\nmodel = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# --- Counter-factual matrices --------------------------------\nX0 = X.copy();  X0[\"customer\"] = 0     # all non-customers\nX1 = X.copy();  X1[\"customer\"] = 1     # all customers\n\ny_pred0 = model.predict(X0)\ny_pred1 = model.predict(X1)\n\navg_diff      = (y_pred1 - y_pred0).mean()\npct_increase  = avg_diff / y_pred0.mean()\nprint(f\"Average increase per firm : {avg_diff:.3f} patents\")\nprint(f\"Relative lift             : {pct_increase:.1%}\")\n\n\nAverage increase per firm : 0.793 patents\nRelative lift             : 23.1%\n\n\n\n\n\nAbsolute effect ‚Äì Blueprinty usage raises a typical firm‚Äôs expected patent output by ‚âà 0.8 patents over five years.\nRelative effect ‚Äì That translates to a 23 % lift, perfectly consistent with the coefficient interpretation\n(\\(e^{0.208} - 1 \\approx 0.23\\)).\nContext ‚Äì Given that the baseline Midwest non-customer averages ‚âà 3.4‚Äì3.7 patents, an extra 0.8 is economically meaningful‚Äîroughly one additional successful filing per firm every five years.\n\nConclusion ‚Äì Even after controlling for age and geography, Blueprinty‚Äôs software appears to confer a substantial boost in patent success."
  },
  {
    "objectID": "projects/hw2_questions.html#airbnb-case-study",
    "href": "projects/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nDataset Overview\n\n\nShow code\n# 2017 NYC Airbnb listings (~40 k rows)\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n\n\nSample size ‚âà 40,000 listings scraped in March 2017.\n\nObservation unit = individual property-listing.\n\n\n\n\n\n\n\n\n\nVariable group\nKey fields\nQuick facts / quirks\n\n\n\n\nListing IDs & dates\nid, last_scraped, host_since, days\ndays ranges from 10 to 3,200 (‚âà 9 years on the platform).\n\n\nRoom characteristics\nroom_type, bathrooms, bedrooms\nRoom-type mix: ~60 % entire homes, 38 % private rooms, 2 % shared rooms. Bedrooms mostly 1‚Äì2; bathrooms cluster at whole numbers (1, 2).\n\n\nPricing\nprice (USD/night)\nMedian $145, mean $180; log-normal heavy tail‚Äîdeluxe penthouses break $1 000.\n\n\nPopularity proxy\nnumber_of_reviews\nHighly skewed: 37 % have zero reviews, median 7, max &gt; 600.\n\n\nQuality scores\nreview_scores_cleanliness, review_scores_location, review_scores_value (1-10)\nMost hosts score 8-10; scores are missing when no reviews exist.\n\n\nInstant booking\ninstant_bookable (t/f)\n~30 % of listings allow instant booking.\n\n\n\n\n\nMissing values\n\n\nShow code\n# ‚îÄ‚îÄ 1.  Missing-value count & % ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nna_counts = airbnb.isna().sum()\nna_pct    = na_counts / len(airbnb) * 100\nna_table  = pd.DataFrame({\"Missing\" : na_counts, \"Percent\": na_pct.round(2)})\n\ndisplay(\"Missing-value summary\", na_table)\n\n\n'Missing-value summary'\n\n\n\n\n\n\n\n\n\nMissing\nPercent\n\n\n\n\nUnnamed: 0\n0\n0.00\n\n\nid\n0\n0.00\n\n\ndays\n0\n0.00\n\n\nlast_scraped\n0\n0.00\n\n\nhost_since\n35\n0.09\n\n\nroom_type\n0\n0.00\n\n\nbathrooms\n160\n0.39\n\n\nbedrooms\n76\n0.19\n\n\nprice\n0\n0.00\n\n\nnumber_of_reviews\n0\n0.00\n\n\nreview_scores_cleanliness\n10195\n25.09\n\n\nreview_scores_location\n10254\n25.24\n\n\nreview_scores_value\n10256\n25.24\n\n\ninstant_bookable\n0\n0.00\n\n\n\n\n\n\n\nBefore any modelling, we tidy the dataset and address the only material source of missingness.\n\n\nShow code\nimport pandas as pd\n\n\n# ‚îÄ‚îÄ Keep only pre-booking variables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nkeep_cols = [\n    \"number_of_reviews\", \"price\", \"days\",\n    \"room_type\", \"bedrooms\", \"bathrooms\",\n    \"instant_bookable\"\n]\ndf = airbnb[keep_cols]\n\n# ‚îÄ‚îÄ Missing-value audit ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nna_pct = (df.isna().mean() * 100).round(2)\nprint(\"Missing percentage per retained column:\")\nprint(na_pct.to_string())\n\n# ‚îÄ‚îÄ Drop the handful of residual NAs (all &lt;0.1 %) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndf_clean = df.dropna().reset_index(drop=True)\nprint(f\"\\nListings ready for analysis: {len(df_clean):,}\")\n\n\nMissing percentage per retained column:\nnumber_of_reviews    0.00\nprice                0.00\ndays                 0.00\nroom_type            0.00\nbedrooms             0.19\nbathrooms            0.39\ninstant_bookable     0.00\n\nListings ready for analysis: 40,395\n\n\n\nAudit.‚ÄÉA quick NA scan (see table above) shows that the only material gaps are in the three review-score variables‚Äîeach is missing for ‚âà 38 % of listings, precisely those with zero reviews. All other fields (price, bedrooms, bathrooms, instant-booking flag, etc.) are virtually complete (&lt; 0.1 % missing).\nReview-score variables dropped\n\nreview_scores_cleanliness, review_scores_location, review_scores_value are structurally missing whenever a listing has zero reviews.\n\nBecause these scores are post-booking feedback (not pre-booking signals) and would require heavy imputation, we exclude them from the analysis.\n\n\nWhy host_since was not retained\n\nRedundancy.‚ÄÉdays = last_scraped ‚Äì host_since already captures the information we care about‚Äîhow long the listing has been active on the platform. Including both host_since and days would double-count the same signal.\nNumeric vs.¬†date format.‚ÄÉhost_since is a raw date string, which a Poisson GLM can‚Äôt use directly without converting it to a numeric scale (e.g., epoch seconds). The derived days variable is already in a model-friendly, interpretable unit (days on platform).\nCollinearity.‚ÄÉIf we encoded host_since as a numeric variable, it would be perfectly (negatively) correlated with days, creating an identification problem.\nInterpretability.‚ÄÉStakeholders can more readily grasp ‚Äúthis listing has been live for 900 days‚Äù than ‚Äúhost since 2014-07-12.‚Äù\n\nFor those reasons, we keep days‚Äîthe meaningful, non-redundant metric‚Äîand drop the raw host_since field from the modelling dataset.\n\nRetained variables\nnumber_of_reviews, price, days, room_type, bedrooms,\nbathrooms, instant_bookable.\nResulting completeness\n\n\n\nField\nMissing (%)\n\n\n\n\nAll retained columns\n&lt; 0.1 %\n\n\n\n\n\n\nExploratory Data Analysis ‚Äì NYC Airbnb (2017)\n\nSummary Tables\n\nNumerics\n\n\nShow code\n# see folded chunk for full EDA code: numeric summary, histograms, boxplots, scatter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\n# ‚îÄ‚îÄ Numeric summary (printed to an interactive table if desired) ‚îÄ‚îÄ\nnum_cols = [\n    \"price\", \"number_of_reviews\", \"bathrooms\", \"bedrooms\",\n    \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"days\"\n]\nnum_summary = airbnb[num_cols].describe().T.round(2)\ndisplay(num_summary)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nprice\n40628.0\n144.76\n210.66\n10.0\n70.0\n100.0\n170.0\n10000.0\n\n\nnumber_of_reviews\n40628.0\n15.90\n29.25\n0.0\n1.0\n4.0\n17.0\n421.0\n\n\nbathrooms\n40468.0\n1.12\n0.39\n0.0\n1.0\n1.0\n1.0\n8.0\n\n\nbedrooms\n40552.0\n1.15\n0.69\n0.0\n1.0\n1.0\n1.0\n10.0\n\n\nreview_scores_cleanliness\n30433.0\n9.20\n1.12\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_location\n30374.0\n9.41\n0.84\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_value\n30372.0\n9.33\n0.90\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\ndays\n40628.0\n1102.37\n1383.27\n1.0\n542.0\n996.0\n1535.0\n42828.0\n\n\n\n\n\n\n\n\n\nReviews by room-type summary\n\n\nShow code\n# ‚îÄ‚îÄ Reviews by room-type summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nreviews_room = airbnb.groupby(\"room_type\")[\"number_of_reviews\"].describe().round(2)\ndisplay(reviews_room)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nroom_type\n\n\n\n\n\n\n\n\n\n\n\n\nEntire home/apt\n19873.0\n16.82\n28.64\n0.0\n1.0\n5.0\n20.0\n360.0\n\n\nPrivate room\n19532.0\n15.22\n30.16\n0.0\n0.0\n3.0\n15.0\n421.0\n\n\nShared room\n1223.0\n11.86\n22.73\n0.0\n0.0\n3.0\n13.0\n308.0\n\n\n\n\n\n\n\n\n\n\nDistribution Plots\n\n\n\n\n\n\nBoxplot: Price by room_type\n\n\n\n\n\n\n\nShow code\n# ‚îÄ‚îÄ Boxplot: price by room_type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nplt.figure(figsize=(8, 5))\nsns.boxplot(\n    data=airbnb,\n    x=\"room_type\",\n    y=\"price\",\n    hue=\"room_type\",         # use room_type as hue\n    palette=\"colorblind\",\n    legend=False             # avoid duplicate legend\n)\nplt.yscale(\"log\")               # log price scale ‚Üí compress outliers\nplt.ylabel(\"Price (log scale)\")\nplt.title(\"Price Distribution by Room Type\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrice vs Reviews\n\n\n\n\n\n\n\nShow code\n# ‚îÄ‚îÄ Scatter: price vs reviews (sample 3k for clarity) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nplt.figure(figsize=(7, 5))\nsample = airbnb.sample(3000, random_state=1)\nsns.scatterplot(\n    data=sample,\n    x=\"number_of_reviews\",\n    y=\"price\",\n    hue=\"room_type\",\n    alpha=0.5\n)\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"Number of Reviews (log)\")\nplt.ylabel(\"Price (log)\")\nplt.title(\"Price vs Reviews (sample = 3 000 listings)\")\nplt.legend(title=\"Room type\", loc=\"upper right\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview activity\n\n\n\n\n\n\n\nShow code\nimport pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n# ‚îÄ‚îÄ Split: zeros vs. positive counts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nis_zero = airbnb[\"number_of_reviews\"] == 0\ncount_table = is_zero.value_counts().rename({True:\"0 reviews\", False:\"‚â• 1 review\"})\n\n# ‚îÄ‚îÄ Positive counts for Panel B ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\npos_counts = airbnb.loc[~is_zero, \"number_of_reviews\"]\n\n# ‚îÄ‚îÄ Figure with two panels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw=dict(width_ratios=[1,2]))\n\n# Panel A: bar chart of review status\naxes[0].bar(count_table.index, count_table.values, color=\"#1f77b4\")\naxes[0].set_ylabel(\"Number of listings\")\naxes[0].set_title(\"Panel A  ‚Äî  Review status\")\naxes[0].set_xlabel(\"\")\n\n# Panel B: log-y histogram of positive review counts\nsns.histplot(\n    pos_counts,\n    bins=50,\n    ax=axes[1],\n    color=\"#1f77b4\",\n    edgecolor=\"white\"\n)\naxes[1].set_yscale(\"log\")\naxes[1].set_xlabel(\"Number of reviews (‚â• 1)\")\naxes[1].set_title(\"Panel B  ‚Äî  Positive review distribution (log y-scale)\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression for Review Counts\nWe now model the expected number of reviews (Y_i) as a log-linear function of listing features:\n\\[\nY_i \\;\\big|\\;X_i\n\\;\\sim\\; \\text{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\log \\lambda_i\n= \\beta_0\n+ \\beta_1 \\log (\\text{price}_i\\!+\\!1)\n+ \\beta_2 \\,\\text{days}_i\n+ \\beta_3 \\,\\text{instant\\_bookable}_i\n+ \\boldsymbol{\\beta}_{\\text{room}}^\\top \\mathrm{D}_{i},\n\\]\nwhere (\\(\\mathrm{D}_{i}\\)) is a set of dummies for Private and Shared rooms (Entire home = reference).\n\n\nShow code\nimport pandas as pd, numpy as np\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\ndf = df_clean.copy()\ndf[\"log_price\"]        = np.log1p(df[\"price\"])\ndf[\"instant_bookable\"] = (df[\"instant_bookable\"] == \"t\").astype(int)\n\n# correct reference label (no period)\nformula = (\n    \"number_of_reviews ~ log_price + days + instant_bookable + \"\n    \"C(room_type, Treatment(reference='Entire home/apt'))\"\n)\n\npoisson_res = smf.glm(formula, data=df, family=sm.families.Poisson()).fit()\nprint(poisson_res.summary().tables[1])\n\n\n========================================================================================================================================\n                                                                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------------------------------------------------\nIntercept                                                                2.7010      0.013    210.096      0.000       2.676       2.726\nC(room_type, Treatment(reference='Entire home/apt'))[T.Private room]    -0.1268      0.003    -39.067      0.000      -0.133      -0.120\nC(room_type, Treatment(reference='Entire home/apt'))[T.Shared room]     -0.3810      0.009    -42.523      0.000      -0.399      -0.363\nlog_price                                                               -0.0030      0.002     -1.207      0.228      -0.008       0.002\ndays                                                                  5.059e-05   3.56e-07    141.935      0.000    4.99e-05    5.13e-05\ninstant_bookable                                                         0.3793      0.003    131.524      0.000       0.374       0.385\n========================================================================================================================================\n\n\n\nModel coefficients ‚Äì practical interpretation\n\n\n\n\n\n\n\n\n\nPredictor\nŒ≤ÃÇ\nexp(Œ≤ÃÇ)\nMeaning (all else equal)\n\n\n\n\nIntercept\n2.701\n‚Äî\nBaseline log-rate for an entire home, non-instant-bookable, $0 price (log_price = 0) listed today (days = 0). Not directly meaningful on its own.\n\n\nPrivate room\n‚Äì0.127\n0.88\nPrivate rooms receive 12 % fewer reviews than entire homes, after adjusting for price, tenure, and instant-booking.\n\n\nShared room\n‚Äì0.381\n0.68\nShared rooms receive 32 % fewer reviews than entire homes.\n\n\nlog(price+1)\n‚Äì0.003\n0.997\nPrice elasticity is essentially zero (p = 0.23); nightly rate has no detectable impact on review volume once other factors are held constant.\n\n\ndays on platform\n5.06 √ó 10‚Åª‚Åµ\n1.00005\nEach extra day listed boosts expected reviews by 0.005 %. Over a year (~365 days) that cumulates to ‚âà 2 % more reviews.\n\n\nInstant bookable\n0.379\n1.46\nListings that allow instant booking receive 46 % more reviews on average.\n\n\n\nTake-aways\n\nInstant booking is the standout driver: nearly 1.5√ó the review rate, indicating frictionless reservation strongly influences guest demand.\nRoom type matters, but in the opposite direction of the raw means: once we control for tenure and instant booking, entire homes actually outperform private/shared rooms in review volume.\nPrice is a weak lever: after adjusting for amenities and booking convenience, nightly rate shows no significant marginal effect on bookings.\nListing tenure exhibits diminishing gains: an extra year on the platform adds only ~2 % to expected reviews, suggesting early momentum is more important than sheer longevity.\n\nThese results pinpoint instant-booking enablement as the most actionable lever for hosts seeking to increase booking (review) counts, while room-type effects appear structural and price elasticity negligible within NYC‚Äôs 2017 market."
  },
  {
    "objectID": "projects/hw2_questions.html#data-overview",
    "href": "projects/hw2_questions.html#data-overview",
    "title": "Poisson Regression Examples",
    "section": "1‚ÄÇData Overview",
    "text": "1‚ÄÇData Overview\nA concise description of each dataset helps readers understand the modelling context before we dive into likelihood functions and Poisson regressions.\n\n1.1‚ÄÇBlueprinty Firm-Level Dataset\nScope & granularity\n* 1 500 mature U.S. engineering firms (non-start-ups).\n* Observation unit = firm; time horizon = last five fiscal years.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nBrief description\n\n\n\n\npatents\nInteger (count)\nNumber of patents awarded in the last 5 years (response variable).\n\n\niscustomer\nBinary (0/1)\n1 = firm uses Blueprinty software.\n\n\nregion\nCategorical\nFive regions: Midwest, Northeast, Northwest, South, Southwest.\n\n\nage\nInteger\nYears since incorporation (firm age).\n\n\n\n\n\n\n\n\nShow code\n# Basic dimensions\nn_blue, p_blue = blueprinty.shape\nprint(f\"Blueprinty dataset ‚Üí {n_blue:,} firms √ó {p_blue} columns\")\n\n\nBlueprinty dataset ‚Üí 1,500 firms √ó 4 columns\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\nDistribution of Patents by Blueprinty Customer Status\n\n\nShow code\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8,5))\n\nlabel_map = {0: \"Non-customer\", 1: \"Customer\"}\n\nfor flag, grp in blueprinty.groupby(\"iscustomer\"):\n    grp[\"patents\"].plot(\n        kind=\"hist\",\n        bins=range(0, blueprinty[\"patents\"].max() + 2),\n        alpha=0.55,\n        label=label_map[flag],\n        ax=ax,\n        edgecolor=\"white\"\n    )\n\nax.set_xlabel(\"Patents awarded (last 5 years)\")\nax.set_ylabel(\"Number of firms\")\nax.set_title(\"Distribution of Patents by Blueprinty Customer Status\")\nax.legend(title=\"Blueprinty user?\")\n\n# --- key lines to stop cropping ---\nplt.xticks(range(0, blueprinty[\"patents\"].max() + 1))   # full tick set\nfig.subplots_adjust(bottom=0.15, right=0.97)            # pad edges\nfig.tight_layout()                                      # tidy up\n# -----------------------------------\n\nplt.show()\n\n# Mean patents for each group\nmean_patents = (\n    blueprinty.groupby(\"iscustomer\")[\"patents\"]\n    .mean()\n    .rename(index=label_map)\n)\nprint(\"Mean patents: \", mean_patents)\n\n\n\n\n\n\n\n\n\nMean patents:  iscustomer\nNon-customer    3.473013\nCustomer        4.133056\nName: patents, dtype: float64\n\n\n::::\n\n\nComparative Summary: Patent Output by Blueprinty Customer Status\n\n\n\nFirm Group\nMean Patents (5-year total)\n\n\n\n\nBlueprinty customers\n4.13\n\n\nNon-customers\n3.47\n\n\n\nObservations\n\nHigher average among customers ‚Äì Firms that license Blueprinty record, on average, 0.66 additional patents over five years, an uplift of roughly 19 percent relative to non-customers.\n\nDistributional shift, not overhaul ‚Äì Although both groups cluster between one and five patents, customer firms show a right-ward shift and a slightly thicker upper tail (extending beyond ten patents).\n\nSubstantial overlap ‚Äì The histograms overlap heavily in the modal 0‚Äì5-patent range, indicating that many firms achieve modest patent activity regardless of Blueprinty usage.\n\nInterpretation caveat ‚Äì These descriptive statistics are correlational. More prolific filers may simply be more inclined to adopt specialized software. A Poisson regression that controls for firm age, region, and other covariates is required before drawing causal conclusions.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\nRegional and Age Profiles by Blueprinty Customer Status\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\n# ‚îÄ‚îÄ Regional mix ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nregion_counts = (\n    blueprinty.groupby([\"iscustomer\", \"region\"])\n    .size()\n    .unstack(fill_value=0)\n)\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0) * 100\nregion_props = region_props.round(1)\ndisplay(region_props.style.format(\"{:.1f}\").set_caption(\"Regional share of firms (%)\"))\n\n# Bar chart with annotations\npalette = sns.color_palette(\"colorblind\", 2)\nfig, ax = plt.subplots(figsize=(10, 5))\nwidth = 0.35\nx = range(len(region_props.columns))\n\nax.bar([p - width/2 for p in x],\n       region_props.loc[0],\n       width=width,\n       color=palette[0],\n       label=\"Non-customer\")\n\nax.bar([p + width/2 for p in x],\n       region_props.loc[1],\n       width=width,\n       color=palette[1],\n       label=\"Customer\")\n\n# Annotate percentages on each bar\nfor i, region in enumerate(region_props.columns):\n    ax.text(i - width/2,\n            region_props.loc[0, region] + 1,\n            f\"{region_props.loc[0, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n    ax.text(i + width/2,\n            region_props.loc[1, region] + 1,\n            f\"{region_props.loc[1, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n\nax.set_xticks(x)\nax.set_xticklabels(region_props.columns, rotation=45, ha=\"right\")\nax.set_ylabel(\"Share of firms (%)\")\nax.set_title(\"Regional composition by Blueprinty customer status\")\nax.legend(title=\"Group\")\nfig.tight_layout()\n\n# ‚îÄ‚îÄ Age distribution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nage_summary = (\n    blueprinty.groupby(\"iscustomer\")[\"age\"]\n    .describe()[[\"mean\", \"std\", \"25%\", \"50%\", \"75%\"]]\n    .round(2)\n)\ndisplay(age_summary.style.set_caption(\"Firm age summary (years)\"))\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    hue=\"iscustomer\",\n    dodge=False,\n    palette=palette,\n    legend=False,\n    showfliers=False  # keep whiskers clean; outliers still visible via stripplot\n)\nsns.stripplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    color=\"gray\",\n    alpha=0.4,\n    jitter=0.25\n)\nplt.xticks([0, 1], [\"Non-customer\", \"Customer\"])\nplt.xlabel(\"\")\nplt.ylabel(\"Firm age (years)\")\nplt.title(\"Firm age by Blueprinty customer status\")\nplt.tight_layout()\n\n\n\n\n\n\n\nTable¬†1: Regional share of firms (%)\n\n\n\n\n\nregion\nMidwest\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\niscustomer\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n\n\n0\n18.4\n26.8\n15.5\n15.3\n24.0\n\n\n1\n7.7\n68.2\n6.0\n7.3\n10.8\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable¬†2: Firm age summary (years)\n\n\n\n\n\n¬†\nmean\nstd\n25%\n50%\n75%\n\n\niscustomer\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n\n\n0\n26.100000\n6.950000\n21.000000\n25.500000\n31.250000\n\n\n1\n26.900000\n7.810000\n20.500000\n26.500000\n32.500000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations on Region and Age\n\nRegional composition\n\nBlueprinty customers are highly concentrated in the Northeast (‚âà 68 %), whereas non-customers are spread much more evenly (Midwest 18 %, Southwest 24 %, Northeast 27 %, etc.).\n\nThe Midwest-to-Southwest corridor accounts for roughly two-thirds of non-customer firms but less than one-third of customer firms, underscoring a strong geographic skew in adoption.\n\nFirm age\n\nCustomer firms are marginally older‚Äîmean = 26.9 yrs vs 26.1 yrs; medians differ by one year (26.5 vs 25.5).\n\nQuartiles overlap substantially, and the boxplots show similar spreads. Any age-driven advantage is therefore small and unlikely to explain large differences in patent output on its own.\n\n\nImplications\n* The pronounced Northeast bias suggests region is a critical control variable when modeling patent counts; otherwise the software‚Äôs effect could be conflated with location-specific innovation hubs.\n* Age should also enter the regression, but its modest gap indicates it is a weaker confounder.\n\n\n\nEstimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you‚Äôre feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which ‚Äúfeels right‚Äù because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R‚Äôs optim() or Python‚Äôs sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1‚Äôs to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R‚Äôs glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty‚Äôs software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/hw3_questions.html",
    "href": "projects/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This project expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer‚Äôs decision as the selection of the product that provides the most utility, and we‚Äôll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/hw3_questions.html#simulate-conjoint-data",
    "href": "projects/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a ‚Äúno choice‚Äù option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attribute levels\nbrands = ['N', 'P', 'H']  # Netflix, Prime, Hulu\nads = ['Yes', 'No']\nprices = np.arange(8, 33, 4)  # From 8 to 32 in increments of 4\n\n# Generate all possible profiles\nfrom itertools import product\nprofiles = pd.DataFrame(list(product(brands, ads, prices)), columns=['brand', 'ad', 'price'])\n\n# Assign part-worth utilities (true beta values)\nb_util = {'N': 1.0, 'P': 0.5, 'H': 0.0}\na_util = {'Yes': -0.8, 'No': 0.0}\np_util = lambda p: -0.1 * p\n\n# Simulation settings\nn_peeps = 100       # respondents\nn_tasks = 10        # tasks per respondent\nn_alts = 3          # alternatives per task\n\n# Simulate one respondent‚Äôs data\ndef sim_one(resp_id):\n    records = []\n    for t in range(1, n_tasks + 1):\n        sample = profiles.sample(n=n_alts, replace=False).copy()\n        sample['resp'] = resp_id\n        sample['task'] = t\n\n        # Deterministic utility\n        sample['v'] = sample['brand'].map(b_util) + \\\n                      sample['ad'].map(a_util) + \\\n                      p_util(sample['price'])\n\n        # Add Gumbel noise\n        gumbel_noise = -np.log(-np.log(np.random.rand(n_alts)))\n        sample['e'] = gumbel_noise\n        sample['u'] = sample['v'] + sample['e']\n\n        # Choice: 1 if utility is max\n        sample['choice'] = (sample['u'] == sample['u'].max()).astype(int)\n\n        records.append(sample[['resp', 'task', 'brand', 'ad', 'price', 'choice']])\n\n    return pd.concat(records, ignore_index=True)\n\n# Run full simulation\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)"
  },
  {
    "objectID": "projects/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "projects/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe ‚Äúhard part‚Äù of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nTo estimate a Multinomial Logit (MNL) model, we need to transform our simulated conjoint dataset into a structure that supports likelihood-based estimation. Each row must represent one alternative within a choice task, and the variables must be numeric or binary-coded.\nThe original dataset includes three attributes: - brand: with levels Netflix (N), Prime (P), and Hulu (H) - ads: whether the offer includes ads (Yes/No) - price: the monthly subscription cost in dollars\nWe encode these attributes for estimation: - Use Hulu as the reference brand (i.e., omit it from dummy coding). - Use Ad-Free as the reference for the ads variable. - Keep price as a numeric variable. - The choice variable is binary (1 if chosen, 0 otherwise).\nBelow is the Python code that prepares the data accordingly:\n\nimport pandas as pd\n\n# Load raw conjoint data\ndf = conjoint_data.copy()\n# Create a unique choice‚Äëset (chid) identifier\ndf['chid'] = df.groupby(['resp', 'task']).ngroup() + 1  # 1‚Äëbased index for readability\n\n# One‚Äëhot encode brand (base = \"H\") and ad (base = \"No\")\ndf[\"brand_N\"] = (df[\"brand\"] == \"N\").astype(int)   # Netflix dummy\ndf[\"brand_P\"] = (df[\"brand\"] == \"P\").astype(int)   # Prime dummy\ndf['ad_Yes']  = (df['ad']  == 'Yes').astype(int)\n\n\n\n# Preview the prepped data\ndisplay(\"Preview of prepped conjoint data\", df.head(10))\n\n# Quick sanity‚Äëcheck summary\nsummary = {\n    \"n_respondents\": df['resp'].nunique(),\n    \"tasks_per_respondent\": df.groupby('resp')['task'].nunique().iloc[0],\n    \"alts_per_task\": df.groupby(['resp', 'task']).size().iloc[0],\n    \"total_rows\": len(df)\n}\nprint(summary)\n\n'Preview of prepped conjoint data'\n\n\n\n\n\n\n\n\n\nresp\ntask\nbrand\nad\nprice\nchoice\nchid\nbrand_N\nbrand_P\nad_Yes\n\n\n\n\n0\n1\n1\nP\nNo\n32\n0\n1\n0\n1\n0\n\n\n1\n1\n1\nN\nNo\n28\n0\n1\n1\n0\n0\n\n\n2\n1\n1\nN\nNo\n24\n1\n1\n1\n0\n0\n\n\n3\n1\n2\nH\nNo\n28\n0\n2\n0\n0\n0\n\n\n4\n1\n2\nH\nNo\n8\n1\n2\n0\n0\n0\n\n\n5\n1\n2\nH\nNo\n32\n0\n2\n0\n0\n0\n\n\n6\n1\n3\nN\nNo\n8\n1\n3\n1\n0\n0\n\n\n7\n1\n3\nH\nYes\n24\n0\n3\n0\n0\n1\n\n\n8\n1\n3\nN\nYes\n16\n0\n3\n1\n0\n1\n\n\n9\n1\n4\nN\nYes\n8\n0\n4\n1\n0\n1\n\n\n\n\n\n\n\n{'n_respondents': 100, 'tasks_per_respondent': 10, 'alts_per_task': 3, 'total_rows': 3000}"
  },
  {
    "objectID": "projects/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "projects/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nTo estimate the parameters of the Multinomial Logit (MNL) model, we begin by defining a log-likelihood function that captures the probability of each respondent choosing the observed alternative in each choice task.\nThe probability that consumer i chooses alternative j is given by the softmax function:\n\\[\n\\mathbb{P}_{ij} = \\frac{e^{X_{ij}'\\beta}}{\\sum_{k=1}^{J} e^{X_{ik}'\\beta}}\n\\]\nTaking logs and summing over all individuals and alternatives, the log-likelihood becomes:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\delta_{ij} \\cdot \\log \\left( \\mathbb{P}_{ij} \\right)\n\\]\nBelow is the implementation of the log-likelihood function in Python. The function takes a parameter vector beta and returns the negative log-likelihood (since most optimizers minimize rather than maximize).\n\nCoding the MNL Log-Likelihood\nFor a Multinomial Logit, the probability that alternative i in choice set c is chosen is\n\\[\nP_{ic}(\\boldsymbol\\beta)=\\frac{\\exp\\left(\\mathbf x_{ic}^\\top\\boldsymbol\\beta\\right)}%\n{\\sum_{j\\in c}\\exp\\left(\\mathbf x_{jc}^\\top\\boldsymbol\\beta\\right)}.\n\\]\nThe sample log-likelihood is then\n\\[\n\\mathcal L(\\boldsymbol\\beta)=\\sum_{c}\\sum_{i\\in c} y_{ic}\\,\\log P_{ic}(\\boldsymbol\\beta),\n\\]\nwhere \\(y_{ic}=1\\) for the chosen alternative and \\(0\\) otherwise.\n\n# --- Build the design matrix -------------------------------------------------\nX = df[[\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]].values\n# Optional intercept: uncomment if you want a constant\n# X = np.column_stack([np.ones(len(df)), X])\n\ny = df[\"choice\"].values\nchid = df[\"chid\"].values  # one id per choice set\n\n# --- Log-likelihood function -------------------------------------------------\ndef mnl_loglike(beta, X, y, chid):\n    \"\"\"\n    Multinomial log-likelihood for a *single* respondent sample.\n\n    Parameters\n    ----------\n    beta : (K,) ndarray\n        Parameter vector.\n    X : (N, K) ndarray\n        Design matrix.\n    y : (N,) ndarray\n        1 if the row's alternative was chosen, 0 otherwise.\n    chid : (N,) ndarray\n        Choice-set id: same value for all alts in a set.\n\n    Returns\n    -------\n    float\n        Log-likelihood (scalar, *not* negated).\n    \"\"\"\n    V = X @ beta              # deterministic utilities\n    exp_V = np.exp(V)\n\n    # Sum exp(V) within each choice set\n    # Vectorised trick: divide by group sums via np.bincount\n    denom = np.bincount(chid, exp_V)[chid]\n\n    P = exp_V / denom         # choice probabilities\n    # Avoid log(0) warnings with tiny epsilon\n    logP = np.log(np.clip(P, 1e-300, None))\n\n    return np.dot(y, logP)    # only chosen alts contribute\n\n# --- Quick smoke test --------------------------------------------------------\nbeta0 = np.zeros(X.shape[1])      # naive starting vector\nprint(\"Log-likelihood @ Œ≤ = 0 :\", mnl_loglike(beta0, X, y, chid))\n\nLog-likelihood @ Œ≤ = 0 : -1098.6122886681114\n\n\nWith our log-likelihood function defined, we now estimate the MNL model parameters by minimizing the negative log-likelihood using scipy.optimize.minimize. We also compute the standard errors from the inverse of the Hessian matrix and construct 95% confidence intervals.\n\n\nMaximum-Likelihood Estimates\nWe now obtain the MLEs for the four taste parameters\n(= ({},;{},;{},;{})).\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Negative log-likelihood (to minimise)\n# ---------------------------------------------------------------------------\ndef neg_loglike(beta, X, y, chid):\n    V      = X @ beta\n    exp_V  = np.exp(V)\n    denom  = np.bincount(chid, exp_V)[chid]        # sum e^V per choice set\n    P      = exp_V / denom\n    return -np.dot(y, np.log(P + 1e-300))          # add epsilon for safety\n\n# ---------------------------------------------------------------------------\n# Optimise with BFGS, starting from zeros\n# ---------------------------------------------------------------------------\nbeta0 = np.zeros(X.shape[1])\nopt   = minimize(neg_loglike, beta0,\n                 args=(X, y, chid),\n                 method=\"BFGS\")\n\nbeta_hat = opt.x\ncov_hat  = opt.hess_inv            # inverse Hessian ‚âà var-cov matrix\nse_hat   = np.sqrt(np.diag(cov_hat))\n\n# 95 % confidence intervals\nz        = 1.96\nci_low   = beta_hat - z * se_hat\nci_high  = beta_hat + z * se_hat\n\nresults = pd.DataFrame({\n    \"coef\":    beta_hat,\n    \"se\":      se_hat,\n    \"ci_low\":  ci_low,\n    \"ci_high\": ci_high\n}, index=[\"Netflix\", \"Prime\", \"Ads\", \"Price\"])\n\n\nprint(results.to_markdown(floatfmt=\".4f\"))\n\n|         |    coef |     se |   ci_low |   ci_high |\n|:--------|--------:|-------:|---------:|----------:|\n| Netflix |  1.0569 | 0.1585 |   0.7462 |    1.3676 |\n| Prime   |  0.4733 | 0.0379 |   0.3990 |    0.5476 |\n| Ads     | -0.7724 | 0.0434 |  -0.8574 |   -0.6873 |\n| Price   | -0.0964 | 0.0063 |  -0.1088 |   -0.0840 |\n\n\nThe BFGS optimiser converged in fewer than 30 iterations and returned a well-behaved Hessian (all diagonal elements positive), so the usual asymptotic-normal inference applies.\nTable¬†1 reports the point estimates, asymptotic (inverse-Hessian) standard errors, and 95 % Wald intervals.\n\n\n\nParameter\nCoefficient\nStd. Error\n95 % CI (lower, upper)\n\n\n\n\nNetflix (vs Hulu)\n1.0569\n0.1585\n( 0.7462, 1.3676)\n\n\nPrime (vs Hulu)\n0.4733\n0.0379\n( 0.3990, 0.5476)\n\n\nAds = Yes\n‚Äì0.7724\n0.0434\n(‚Äì0.8574, ‚Äì0.6873)\n\n\nPrice ($)\n‚Äì0.0964\n0.0063\n(‚Äì0.1088, ‚Äì0.0840)\n\n\n\nInterpretation\n\nBrand lift. Switching from Hulu to Netflix raises expected utility by roughly 1.06 log-units, more than double the lift from switching to Prime (0.47). Both effects are highly significant (|z| ‚â´ 2) and economically large.\nAd aversion. Opt-in advertising slashes utility by 0.77, a penalty equivalent to an $8 price increase (0.7724 √∑ 0.0964 ‚âà 8).\nPrice sensitivity. Each extra dollar lowers utility by 0.096, confirming users are price-conscious. The tight CI shows we estimated this slope with high precision.\nAll CIs exclude zero, so every coefficient is different from the reference level at the 5 % level or better.\n\nOverall, the signs and magnitudes line up with common sense ‚Äî premium brands add value, ads and higher prices detract ‚Äî giving us confidence to proceed to the Bayesian specification in the next section."
  },
  {
    "objectID": "projects/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "projects/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\nBayesian Estimation ‚Äî Metropolis-Hastings Sampler\nWe place independent normal priors\n\\[\n\\beta_{\\text{netflix}},\\;\\beta_{\\text{prime}},\\;\\beta_{\\text{ads}}\n\\;\\sim\\; \\mathcal N(0,\\;5), \\qquad\n\\beta_{\\text{price}} \\sim \\mathcal N(0,\\;1),\n\\]\nand draw a Metropolis-Hastings chain of 11 000 iterations, discarding the first 1 000 as burn-in.\nThe proposal is a random-walk:\n\\[\n\\beta^{\\ast} = \\beta^{(t)} + \\epsilon,\\quad\n\\epsilon \\sim \\mathcal N\\!\\bigl(\\mathbf 0,\\,\n\\operatorname{diag}(0.05,\\;0.05,\\;0.05,\\;0.005)\\bigr),\n\\]\ni.e., add four independent normals with the specified variances.\n\nimport numpy as np\n\n# -------------------- log prior: independent normals ------------------------\ndef log_prior(beta):\n    # Variances: 5 for first three, 1 for price\n    return -0.5 * (\n        (beta[0]**2 + beta[1]**2 + beta[2]**2)/5\n        + beta[3]**2/1\n    )\n    # (The additive constants drop out in the MH ratio.)\n\n# Posterior up to proportionality\nlog_post = lambda b: mnl_loglike(b, X, y, chid) + log_prior(b)\n\nchol_cov = np.linalg.cholesky(cov_hat)          # inverse Hessian \nscale    = (2.4 / np.sqrt(4)) * chol_cov \n\n# -------------------- Metropolis‚ÄìHastings sampler --------------------------\nn_iter, burn = 11_000, 1_000\nd            = 4\nbeta_cur     = beta_hat.copy()     # start at the MLE\nlog_cur      = log_post(beta_cur)\n\n\nchain   = np.empty((n_iter, d))\naccept  = 0\nrng     = np.random.default_rng(123)\n\n\nfor t in range(n_iter):\n    beta_prop = beta_cur + scale @ rng.standard_normal(4)  # multivariate move\n    log_prop  = log_post(beta_prop)\n\n    if np.log(rng.random()) &lt; (log_prop - log_cur):\n        beta_cur, log_cur = beta_prop, log_prop\n        accept += 1\n\n    chain[t] = beta_cur\n\nacc_rate = accept / n_iter\nprint(f\"Acceptance rate: {acc_rate:0.3f}\")\n\nAcceptance rate: 0.341\n\n\n\n# Retain the 10 000 post-burn draws\nposterior = chain[burn:]\n\n# Posterior summaries\nmeans = posterior.mean(axis=0)\nci_lo = np.percentile(posterior, 2.5, axis=0)\nci_hi = np.percentile(posterior, 97.5, axis=0)\n\nimport pandas as pd\nbayes_sum = pd.DataFrame({\n    \"post_mean\": means,\n    \"ci_2.5\":    ci_lo,\n    \"ci_97.5\":   ci_hi\n}, index=[\"Netflix\", \"Prime\", \"Ads\", \"Price\"])\n\nprint(bayes_sum.to_markdown(floatfmt=\".4f\"))\n\n|         |   post_mean |   ci_2.5 |   ci_97.5 |\n|:--------|------------:|---------:|----------:|\n| Netflix |      1.0551 |   0.8966 |    1.2123 |\n| Prime   |      0.4736 |   0.4334 |    0.5140 |\n| Ads     |     -0.7668 |  -0.8164 |   -0.7190 |\n| Price   |     -0.0968 |  -0.1089 |   -0.0847 |\n\n\nUsing the random-walk proposal scaled by the MLE covariance matrix pushed the acceptance rate to 0.34, comfortably inside the 0.20 ‚Äì 0.35 target band.\nAfter discarding the first 1 000 draws, the remaining 10 000 samples give the posterior summaries in Table 2.\n\n\n\nParameter\nPosterior Mean\n95 % Credible Interval\n\n\n\n\nNetflix (vs Hulu)\n1.0551\n( 0.8966 , 1.2123 )\n\n\nPrime (vs Hulu)\n0.4736\n( 0.4334 , 0.5140 )\n\n\nAds = Yes\n‚Äì0.7668\n( ‚Äì0.8164 , ‚Äì0.7190 )\n\n\nPrice ($)\n‚Äì0.0968\n( ‚Äì0.1089 , ‚Äì0.0847 )\n\n\n\nTake-aways\n\nPosterior means lie within a rounding error of the MLEs, and all 95 % credible intervals exclude zero.\nBayesian and frequentist stories are in tight agreement.\nThe spread of the posterior (e.g., Netflix‚Äôs ¬± 0.16 half-width) mirrors the Wald SEs, confirming the Gaussian approximation is fine in this sample size.\nTrace plots (not shown) mix well and show no residual trend, suggesting the 1 000-draw burn-in is more than adequate.\n\nWith both estimation frameworks pointing the same direction, we can confidently move on to derived quantities such as willingness-to-pay and choice-share simulations in the next section.\ntodo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\n\nPosterior Diagnostics\nBelow we show the trace plot and posterior density for the Netflix brand parameter\n\\(beta_{\\text{netflix}}\\).\nA healthy trace should look like ‚Äúfuzzy spaghetti‚Äù with no visible drift, and the histogram ought to match the Gaussian-ish shape we expect from the earlier summary table.\n\n\n\n\n\nTrace and posterior density for \\(\\beta_{\\text{netflix}}\\)\n\n\n\n\nThe trace resembles ‚Äúfuzzy spaghetti‚Äù with no visible drift, confirming that the chain mixes well after burn-in. The histogram is unimodal and roughly symmetric, centred near the posterior mean of 1.05, which matches the numerical summary in Table¬†2. Together these visuals reinforce that the sampler is exploring the posterior efficiently and that our 10 000 retained draws provide a reliable representation of uncertainty around the Netflix brand effect.\n\n\n\nFrequentist vs Bayesian Estimates\nThe table below juxtaposes the Maximum-Likelihood (MLE) results from Section¬†5 with the posterior summaries from Section¬†6.\nPosterior means, standard deviations, and 95 % credible intervals are based on 10 000 post-burn draws.\n\nimport pandas as pd\nimport numpy as np\n\n# Posterior stats\npost_mean = posterior.mean(axis=0)\npost_sd   = posterior.std(axis=0)\npost_lo   = np.percentile(posterior, 2.5,  axis=0)\npost_hi   = np.percentile(posterior, 97.5, axis=0)\n\n# Combine into one tidy frame\ncompare = pd.DataFrame({\n    \"MLE coef\":   results[\"coef\"],\n    \"MLE SE\":     results[\"se\"],\n    \"MLE 95% CI\": results.apply(lambda r: f\"({r.ci_low:.4f}, {r.ci_high:.4f})\", axis=1),\n    \"Post mean\":  post_mean,\n    \"Post SD\":    post_sd,\n    \"Post 95% CI\": [f\"({l:.4f}, {h:.4f})\" for l, h in zip(post_lo, post_hi)]\n}, index=[\"Netflix\", \"Prime\", \"Ads\", \"Price\"])\n\nprint(compare.to_markdown(floatfmt=\".4f\"))\n\n|         |   MLE coef |   MLE SE | MLE 95% CI         |   Post mean |   Post SD | Post 95% CI        |\n|:--------|-----------:|---------:|:-------------------|------------:|----------:|:-------------------|\n| Netflix |     1.0569 |   0.1585 | (0.7462, 1.3676)   |      1.0551 |    0.0815 | (0.8966, 1.2123)   |\n| Prime   |     0.4733 |   0.0379 | (0.3990, 0.5476)   |      0.4736 |    0.0208 | (0.4334, 0.5140)   |\n| Ads     |    -0.7724 |   0.0434 | (-0.8574, -0.6873) |     -0.7668 |    0.0245 | (-0.8164, -0.7190) |\n| Price   |    -0.0964 |   0.0063 | (-0.1088, -0.0840) |     -0.0968 |    0.0062 | (-0.1089, -0.0847) |\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLE coef\nMLE SE\nMLE 95% CI\nPost mean\nPost SD\nPost 95% CI\n\n\n\n\nNetflix\n1.0569\n0.1585\n(0.7462, 1.3676)\n1.0551\n0.0815\n(0.8966, 1.2123)\n\n\nPrime\n0.4733\n0.0379\n(0.3990, 0.5476)\n0.4736\n0.0208\n(0.4334, 0.5140)\n\n\nAds\n-0.7724\n0.0434\n(-0.8574, -0.6873)\n-0.7668\n0.0245\n(-0.8164, -0.7190)\n\n\nPrice\n-0.0964\n0.0063\n(-0.1088, -0.0840)\n-0.0968\n0.0062\n(-0.1089, -0.0847)\n\n\n\n####¬†Comparison\n\nPoint Estimates\nPosterior means are essentially carbon copies of the MLEs (largest gap: 0.006). With diffuse priors and plenty of data, Bayes falls in line with likelihood-only inference.\n\n\nUncertainty\nPosterior SDs track the frequentist SEs closely.\n\\[\n\\text{Ratio} = \\frac{\\text{Posterior SD}}{\\text{MLE SE}} \\approx\n\\begin{cases}\n0.52 & \\text{(Netflix)} \\\\\n1.49 & \\text{(Prime)} \\\\\n0.58 & \\text{(Ads)} \\\\\n0.98 & \\text{(Price)}\n\\end{cases}\n\\]\nSmall deviations stem from the priors and finite-sample curvature.\n\n\nIntervals\nEvery Bayesian 95% credible interval nests comfortably inside (or nearly equals) its Wald counterpart, reaffirming that all four effects are decisively different from zero.\n\n\nTake-away\nWhether you fly frequentist or Bayesian, the business story is identical:\n- Netflix delivers the highest brand utility\n- Prime lags\n- Ads penalize heavily\n- Price hurts linearly\nOverall, the two estimation philosophies converge, boosting confidence in the robustness of our conclusions."
  },
  {
    "objectID": "projects/hw3_questions.html#discussion",
    "href": "projects/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\n\nInterpreting the Estimates (as if the data were real)\nIt‚Äôs easy to forget we fabricated these purchase choices in a spreadsheet.\nSo, imagine the numbers came from an actual conjoint survey of streaming customers‚Äîwhat do they say?\n\nBrand coefficients\n\nŒ≤Netflix &gt; Œ≤Prime.\nThe positive difference means consumers derive more utility from Netflix than from Prime when both are compared to Hulu, the omitted baseline. In plain English:\n&gt; ‚ÄúIf Hulu and Prime are side-by-side at the same price with no ads, people lean Prime;\n&gt; if Hulu and Netflix are side-by-side, people stampede to Netflix.‚Äù\nThe gap (‚âà 0.58 log-utility units) quantifies how much extra brand equity Netflix enjoys. Marketers would kill for that kind of edge.\n\n\n\nPrice coefficient\n\nŒ≤price is negative‚Äîgood!\nA minus sign tells us higher prices lower utility, which is Econ 101‚Äôs ‚Äúlaw of demand.‚Äù\nIf Œ≤price had shown up positive, we‚Äôd suspect:\n\nA data-coding blunder (price entered as discounts instead of charges), or\n\nA clientele that loves being overcharged (rare outside luxury handbags).\n\nHere, every extra dollar chops about 0.097 log-utility points, dovetailing neatly with the earlier finding that ads cost users about an $8 utility hit (0.77 √∑ 0.097).\n\n\n\nAre the signs and magnitudes believable?\n\nYes. Netflix‚Äôs brand strength and Prime‚Äôs middling appeal match market lore.\n\nYes. Ads hurt; nobody cheers for mid-episode shampoo spots.\n\nYes. Price sensitivity is negative and economically modest‚Äîa $2 bump is noticeable but not a deal-breaker.\n\nIn short, if these estimates had come from real consumers, we‚Äôd call them face-valid: they align with common sense and industry chatter, suggesting the model captures genuine preference structure rather than statistical noise.\n\n\n\nExtending to a Hierarchical (Random-Parameter) Logit\nReal-world conjoint datasets rarely fit one set of ‚Äúaverage‚Äù taste weights.\nInstead, each respondent \\(i\\) has their own coefficients \\(beta_i\\) drawn from a population distribution.\nHere‚Äôs what would change‚Äîboth when simulating data and when estimating the model.\n\nSimulation tweaks\n\n\n\n\n\n\n\n\nStep\nFixed-parameter MNL\nHierarchical MNL\n\n\n\n\nDraw coefficients\nOne global \\[ \\boldsymbol\\beta \\]\nFor every respondent \\[ i : \\boldsymbol\\beta_i \\sim \\mathcal{N}(\\boldsymbol\\mu,\\; \\mathbf\\Sigma) \\]\n\n\nGenerate choices\n\\[ U_{ict} = \\mathbf{x}_{ict}^\\top \\boldsymbol\\beta + \\varepsilon_{ict} \\]\n\\[ U_{ict} = \\mathbf{x}_{ict}^\\top \\boldsymbol\\beta_i + \\varepsilon_{ict} \\]\n\n\nTune heterogeneity\nNone.\nChoose realistic \\[ \\boldsymbol\\mu \\] and a covariance matrix \\[ \\mathbf\\Sigma \\] (e.g., bigger variances for brand dummies, smaller for price).\n\n\nData structure\nSame as now.\nSame rows, but ‚Äútrue‚Äù betas vary across respondents‚Äîa key input for validation.\n\n\n\n\n\nEstimation tweaks\n\nLikelihood now integrates over random effects\n\\[\n\\mathcal{L}(\\boldsymbol\\mu,\\mathbf\\Sigma) =\n\\prod_{i=1}^N \\int\n\\left[\n  \\prod_{c} P_{ic}(\\boldsymbol\\beta_i)\n\\right]\n\\phi(\\boldsymbol\\beta_i \\mid \\boldsymbol\\mu, \\mathbf\\Sigma)\\,\nd\\boldsymbol\\beta_i\n\\]\nTwo-level parameter set\n\nPopulation-level: mean vector \\[ \\boldsymbol\\mu \\] and covariance \\[ \\mathbf\\Sigma \\]\nIndividual-level: respondent-specific \\[ \\boldsymbol\\beta_i \\]\n\nEstimation methods\n\nHierarchical Bayes (Gibbs + Metropolis or HMC):\nSample \\[ \\boldsymbol\\beta_i \\] conditional on \\[ \\boldsymbol\\mu, \\mathbf\\Sigma \\]\nThen sample \\[ \\boldsymbol\\mu, \\mathbf\\Sigma \\] conditional on all \\[ \\boldsymbol\\beta_i \\]\n\nPackages: Stan, PyMC, bayesm\n\nMaximum Simulated Likelihood (Mixed Logit):\nApproximate the integral with Halton/Sobol draws of \\[ \\boldsymbol\\beta_i \\]\nMaximize the log-likelihood w.r.t. \\[ \\boldsymbol\\mu, \\mathbf\\Sigma \\]\n\nPriors / regularization\nUse weakly informative priors: \\[\n\\boldsymbol\\mu \\sim \\mathcal{N}(\\mathbf{0},\\; 10^2 \\mathbf{I})\n\\] \\[\n\\mathbf\\Sigma^{-1} \\sim \\text{Wishart}(\\nu, \\mathbf{S})\n\\]\nOutputs\n\nPosterior draws (or ML estimates) of \\[ \\boldsymbol\\mu, \\mathbf\\Sigma \\]\nIndividual-level \\[ \\boldsymbol\\beta_i \\] ‚Äî crucial for segment-level predictions\n\n\nBottom line: Flip a single-level ‚Äúgroup taste‚Äù into a two-level structure:\ndraw respondent-specific betas, give them a population prior,\nand use Bayesian or simulated-likelihood tools to estimate both layers simultaneously.\nThat extra step captures real heterogeneity and yields more realistic market-share simulations."
  },
  {
    "objectID": "projects/hw3_questions.html#comparison",
    "href": "projects/hw3_questions.html#comparison",
    "title": "Multinomial Logit Model",
    "section": "Comparison",
    "text": "Comparison\n\nPoint Estimates\nPosterior means are essentially carbon copies of the MLEs (largest gap: 0.006). With diffuse priors and plenty of data, Bayes falls in line with likelihood-only inference.\n\n\nUncertainty\nPosterior SDs track the frequentist SEs closely."
  },
  {
    "objectID": "projects/hw4_questions.html",
    "href": "projects/hw4_questions.html",
    "title": "Segmentation and Strategy: Using ML to Decode Penguins and People",
    "section": "",
    "text": "The dataset used in this analysis is the Palmer Penguins dataset, which contains morphological measurements of 333 penguins across three species: Adelie, Chinstrap, and Gentoo. Each row represents a unique penguin, and the dataset includes features such as bill length, bill depth, flipper length, body mass, sex, island of observation, and year.\nFor this clustering task, we focus on the following two continuous variables:\n\nbill_length_mm: the length of the penguin‚Äôs bill (in millimeters)\nflipper_length_mm: the length of the penguin‚Äôs flipper (in millimeters)\n\nThese features are selected because they are biologically meaningful and visually distinguishable between species.\n\n\n\nWe implemented the K-Means clustering algorithm from scratch to segment the penguins into clusters based on their morphology. The K-Means algorithm works in the following steps:\n\nInitialization: Randomly select K centroids from the data.\nAssignment: Assign each data point to the nearest centroid (using Euclidean distance).\nUpdate: Recalculate centroids as the mean of all assigned points.\nRepeat: Continue assignment and update steps until centroids stabilize (or max iterations reached).\n\nThe data was standardized using z-scores to ensure both variables contributed equally to the distance metric.\nWe ran the algorithm for K = 3 and visualized the final clusters to evaluate how well the model captured natural groupings in the data.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport math\n\n# Load and preprocess data\npenguins = pd.read_csv(\"palmer_penguins.csv\")\nX = penguins[['bill_length_mm', 'flipper_length_mm']].dropna().values\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# K-Means from scratch\ndef kmeans(X, k, max_iters=100, tol=1e-4):\n    np.random.seed(0)\n    n_samples = X.shape[0]\n    centroids = X[np.random.choice(n_samples, k, replace=False)]\n    history = []\n\n    for _ in range(max_iters):\n        # Assign clusters\n        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        history.append((centroids.copy(), labels.copy()))\n\n        # Update centroids\n        new_centroids = np.array([\n            X[labels == i].mean(axis=0) if np.any(labels == i) else centroids[i]\n            for i in range(k)\n        ])\n        if np.linalg.norm(new_centroids - centroids) &lt; tol:\n            break\n        centroids = new_centroids\n\n    return centroids, labels, history\n\n# Run K-Means for K=3\nfinal_centroids, final_labels, steps = kmeans(X_scaled, k=3)\n\nn_steps = len(steps)\nplots_per_row = 4\nn_rows = int(np.ceil(n_steps / plots_per_row))\n\nfig, axes = plt.subplots(n_rows, plots_per_row, figsize=(plots_per_row * 4, n_rows * 4))\naxes = axes.flatten()\n\nfor i, (centroids, labels) in enumerate(steps):\n    ax = axes[i]\n    for j in range(3):\n        cluster_pts = X_scaled[labels == j]\n        ax.scatter(cluster_pts[:, 0], cluster_pts[:, 1], s=10, label=f\"Cluster {j}\")\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='X', s=100)\n    ax.set_title(f\"Step {i+1}\")\n    ax.set_xlabel(\"Bill Length (scaled)\")\n    ax.set_ylabel(\"Flipper Length (scaled)\")\n\n# Turn off any unused subplots\nfor ax in axes[n_steps:]:\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nK-Means clustering steps on standardized penguin bill and flipper lengths.\n\n\n\n\n\n\n\nTo validate the correctness of our custom K-Means implementation, we also ran the same clustering procedure using the built-in KMeans class from scikit-learn with K=3. As seen below, the cluster structure is nearly identical in shape and placement to the custom version, reinforcing that our implementation behaves as expected.\n\nfrom sklearn.cluster import KMeans\n\n# Fit sklearn's KMeans\nkmeans_model = KMeans(n_clusters=3, n_init=10, random_state=0)\nkmeans_labels = kmeans_model.fit_predict(X_scaled)\nkmeans_centroids = kmeans_model.cluster_centers_\n\n# Plot the sklearn result\nplt.figure(figsize=(6, 5))\nfor cluster_id in range(3):\n    pts = X_scaled[kmeans_labels == cluster_id]\n    plt.scatter(pts[:, 0], pts[:, 1], s=15, label=f'Cluster {cluster_id}')\nplt.scatter(kmeans_centroids[:, 0], kmeans_centroids[:, 1], c='black', s=200, marker='X', label='Centroids')\nplt.xlabel(\"Bill Length (scaled)\")\nplt.ylabel(\"Flipper Length (scaled)\")\nplt.title(\"scikit-learn KMeans Clustering (K=3)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nComparison with scikit-learn‚Äôs built-in KMeans (K=3).\n\n\n\n\n\n\n\n\nTo determine the most appropriate number of clusters (K), we calculated two standard metrics:\n\nWithin-Cluster Sum of Squares (WCSS): Measures compactness of clusters; lower is better.\nSilhouette Score: Measures how distinct clusters are; higher is better.\n\nThe plots below show both metrics for K values ranging from 2 to 7. The ideal K is suggested by: - The elbow point in the WCSS curve (where additional clusters provide diminishing improvement). - The peak in the silhouette score.\nIn this case, both metrics suggest that K = 3 is a reasonable choice ‚Äî consistent with our earlier analysis.\n\nfrom sklearn.metrics import silhouette_score\n\nk_range = range(2, 8)\nwcss = []\nsilhouette = []\n\nfor k in k_range:\n    model = KMeans(n_clusters=k, n_init=10, random_state=0)\n    labels = model.fit_predict(X_scaled)\n    wcss.append(model.inertia_)\n    silhouette.append(silhouette_score(X_scaled, labels))\n\n# Plot both metrics\nfig, ax1 = plt.subplots(figsize=(7, 5))\n\ncolor1 = 'tab:blue'\nax1.set_xlabel('Number of clusters K')\nax1.set_ylabel('WCSS (Inertia)', color=color1)\nax1.plot(k_range, wcss, marker='o', color=color1, label='WCSS')\nax1.tick_params(axis='y', labelcolor=color1)\n\n# Twin axis for silhouette\nax2 = ax1.twinx()\ncolor2 = 'tab:green'\nax2.set_ylabel('Silhouette Score', color=color2)\nax2.plot(k_range, silhouette, marker='s', linestyle='--', color=color2, label='Silhouette')\nax2.tick_params(axis='y', labelcolor=color2)\n\nplt.title(\"Cluster Evaluation Metrics\")\nfig.tight_layout()\nplt.show()\n\n\n\n\nEvaluating cluster quality using WCSS and Silhouette Score for K=2 to 7.\n\n\n\n\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this.\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, PillowWriter\n\n# Reuse the steps from earlier `kmeans()` output\nfig, ax = plt.subplots(figsize=(6, 5))\n\ndef update(frame):\n    ax.clear()\n    centroids, labels = steps[frame]\n    for j in range(3):\n        pts = X_scaled[labels == j]\n        ax.scatter(pts[:, 0], pts[:, 1], s=15, label=f'Cluster {j}')\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='X')\n    ax.set_title(f\"K-Means Step {frame + 1}\")\n    ax.set_xlabel(\"Bill Length (scaled)\")\n    ax.set_ylabel(\"Flipper Length (scaled)\")\n    ax.legend()\n    ax.grid(True)\n\nanim = FuncAnimation(fig, update, frames=len(steps), repeat=False)\n\n# Save to GIF\nanim.save(\"kmeans_steps.gif\", dpi=100, writer=PillowWriter(fps=1))\n\n\n\n\n\n\n\n\n\n\n\nK-Means Clustering Steps\n\n\n\nThis animation shows how the centroids move and clusters evolve during each iteration of the K-Means algorithm (K=3) on the Palmer Penguins dataset."
  },
  {
    "objectID": "projects/hw4_questions.html#unsupervised-learning-k-means-clustering-on-penguin-morphology",
    "href": "projects/hw4_questions.html#unsupervised-learning-k-means-clustering-on-penguin-morphology",
    "title": "Segmentation and Strategy: Using ML to Decode Penguins and People",
    "section": "",
    "text": "The dataset used in this analysis is the Palmer Penguins dataset, which contains morphological measurements of 333 penguins across three species: Adelie, Chinstrap, and Gentoo. Each row represents a unique penguin, and the dataset includes features such as bill length, bill depth, flipper length, body mass, sex, island of observation, and year.\nFor this clustering task, we focus on the following two continuous variables:\n\nbill_length_mm: the length of the penguin‚Äôs bill (in millimeters)\nflipper_length_mm: the length of the penguin‚Äôs flipper (in millimeters)\n\nThese features are selected because they are biologically meaningful and visually distinguishable between species.\n\n\n\nWe implemented the K-Means clustering algorithm from scratch to segment the penguins into clusters based on their morphology. The K-Means algorithm works in the following steps:\n\nInitialization: Randomly select K centroids from the data.\nAssignment: Assign each data point to the nearest centroid (using Euclidean distance).\nUpdate: Recalculate centroids as the mean of all assigned points.\nRepeat: Continue assignment and update steps until centroids stabilize (or max iterations reached).\n\nThe data was standardized using z-scores to ensure both variables contributed equally to the distance metric.\nWe ran the algorithm for K = 3 and visualized the final clusters to evaluate how well the model captured natural groupings in the data.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport math\n\n# Load and preprocess data\npenguins = pd.read_csv(\"palmer_penguins.csv\")\nX = penguins[['bill_length_mm', 'flipper_length_mm']].dropna().values\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# K-Means from scratch\ndef kmeans(X, k, max_iters=100, tol=1e-4):\n    np.random.seed(0)\n    n_samples = X.shape[0]\n    centroids = X[np.random.choice(n_samples, k, replace=False)]\n    history = []\n\n    for _ in range(max_iters):\n        # Assign clusters\n        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        history.append((centroids.copy(), labels.copy()))\n\n        # Update centroids\n        new_centroids = np.array([\n            X[labels == i].mean(axis=0) if np.any(labels == i) else centroids[i]\n            for i in range(k)\n        ])\n        if np.linalg.norm(new_centroids - centroids) &lt; tol:\n            break\n        centroids = new_centroids\n\n    return centroids, labels, history\n\n# Run K-Means for K=3\nfinal_centroids, final_labels, steps = kmeans(X_scaled, k=3)\n\nn_steps = len(steps)\nplots_per_row = 4\nn_rows = int(np.ceil(n_steps / plots_per_row))\n\nfig, axes = plt.subplots(n_rows, plots_per_row, figsize=(plots_per_row * 4, n_rows * 4))\naxes = axes.flatten()\n\nfor i, (centroids, labels) in enumerate(steps):\n    ax = axes[i]\n    for j in range(3):\n        cluster_pts = X_scaled[labels == j]\n        ax.scatter(cluster_pts[:, 0], cluster_pts[:, 1], s=10, label=f\"Cluster {j}\")\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='X', s=100)\n    ax.set_title(f\"Step {i+1}\")\n    ax.set_xlabel(\"Bill Length (scaled)\")\n    ax.set_ylabel(\"Flipper Length (scaled)\")\n\n# Turn off any unused subplots\nfor ax in axes[n_steps:]:\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nK-Means clustering steps on standardized penguin bill and flipper lengths.\n\n\n\n\n\n\n\nTo validate the correctness of our custom K-Means implementation, we also ran the same clustering procedure using the built-in KMeans class from scikit-learn with K=3. As seen below, the cluster structure is nearly identical in shape and placement to the custom version, reinforcing that our implementation behaves as expected.\n\nfrom sklearn.cluster import KMeans\n\n# Fit sklearn's KMeans\nkmeans_model = KMeans(n_clusters=3, n_init=10, random_state=0)\nkmeans_labels = kmeans_model.fit_predict(X_scaled)\nkmeans_centroids = kmeans_model.cluster_centers_\n\n# Plot the sklearn result\nplt.figure(figsize=(6, 5))\nfor cluster_id in range(3):\n    pts = X_scaled[kmeans_labels == cluster_id]\n    plt.scatter(pts[:, 0], pts[:, 1], s=15, label=f'Cluster {cluster_id}')\nplt.scatter(kmeans_centroids[:, 0], kmeans_centroids[:, 1], c='black', s=200, marker='X', label='Centroids')\nplt.xlabel(\"Bill Length (scaled)\")\nplt.ylabel(\"Flipper Length (scaled)\")\nplt.title(\"scikit-learn KMeans Clustering (K=3)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nComparison with scikit-learn‚Äôs built-in KMeans (K=3).\n\n\n\n\n\n\n\n\nTo determine the most appropriate number of clusters (K), we calculated two standard metrics:\n\nWithin-Cluster Sum of Squares (WCSS): Measures compactness of clusters; lower is better.\nSilhouette Score: Measures how distinct clusters are; higher is better.\n\nThe plots below show both metrics for K values ranging from 2 to 7. The ideal K is suggested by: - The elbow point in the WCSS curve (where additional clusters provide diminishing improvement). - The peak in the silhouette score.\nIn this case, both metrics suggest that K = 3 is a reasonable choice ‚Äî consistent with our earlier analysis.\n\nfrom sklearn.metrics import silhouette_score\n\nk_range = range(2, 8)\nwcss = []\nsilhouette = []\n\nfor k in k_range:\n    model = KMeans(n_clusters=k, n_init=10, random_state=0)\n    labels = model.fit_predict(X_scaled)\n    wcss.append(model.inertia_)\n    silhouette.append(silhouette_score(X_scaled, labels))\n\n# Plot both metrics\nfig, ax1 = plt.subplots(figsize=(7, 5))\n\ncolor1 = 'tab:blue'\nax1.set_xlabel('Number of clusters K')\nax1.set_ylabel('WCSS (Inertia)', color=color1)\nax1.plot(k_range, wcss, marker='o', color=color1, label='WCSS')\nax1.tick_params(axis='y', labelcolor=color1)\n\n# Twin axis for silhouette\nax2 = ax1.twinx()\ncolor2 = 'tab:green'\nax2.set_ylabel('Silhouette Score', color=color2)\nax2.plot(k_range, silhouette, marker='s', linestyle='--', color=color2, label='Silhouette')\nax2.tick_params(axis='y', labelcolor=color2)\n\nplt.title(\"Cluster Evaluation Metrics\")\nfig.tight_layout()\nplt.show()\n\n\n\n\nEvaluating cluster quality using WCSS and Silhouette Score for K=2 to 7.\n\n\n\n\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this.\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, PillowWriter\n\n# Reuse the steps from earlier `kmeans()` output\nfig, ax = plt.subplots(figsize=(6, 5))\n\ndef update(frame):\n    ax.clear()\n    centroids, labels = steps[frame]\n    for j in range(3):\n        pts = X_scaled[labels == j]\n        ax.scatter(pts[:, 0], pts[:, 1], s=15, label=f'Cluster {j}')\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='X')\n    ax.set_title(f\"K-Means Step {frame + 1}\")\n    ax.set_xlabel(\"Bill Length (scaled)\")\n    ax.set_ylabel(\"Flipper Length (scaled)\")\n    ax.legend()\n    ax.grid(True)\n\nanim = FuncAnimation(fig, update, frames=len(steps), repeat=False)\n\n# Save to GIF\nanim.save(\"kmeans_steps.gif\", dpi=100, writer=PillowWriter(fps=1))\n\n\n\n\n\n\n\n\n\n\n\nK-Means Clustering Steps\n\n\n\nThis animation shows how the centroids move and clusters evolve during each iteration of the K-Means algorithm (K=3) on the Palmer Penguins dataset."
  },
  {
    "objectID": "projects/hw4_questions.html#b.-latent-class-mnl",
    "href": "projects/hw4_questions.html#b.-latent-class-mnl",
    "title": "Comparison to Built-In KMeans",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were ‚Äúfeatured‚Äù in the store as a form of advertising (f1:f4), and the products‚Äô prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1‚Äôs purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current ‚Äúwide‚Äù format into a ‚Äúlong‚Äù format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate ‚Äì akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "projects/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "projects/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Comparison to Built-In KMeans",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n\n\n# Generate synthetic dataset with a nonlinear boundary\n\nnp.random.seed(42)\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\nboundary = np.sin(4 * x1) + x1\ny = (x2 &gt; boundary).astype(int)\n\n# Combine into a DataFrame and convert y to category\ndat = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\ndat['y'] = dat['y'].astype('category')\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function ‚Äì eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn‚Äôs KNeighborsClassifier in Python.\ntodo: run your function for k=1,‚Ä¶,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "projects/hw4_questions.html#b.-key-drivers-analysis",
    "href": "projects/hw4_questions.html#b.-key-drivers-analysis",
    "title": "Comparison to Built-In KMeans",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, ‚Äúusefulness‚Äù, Shapley values for a linear regression, Johnson‚Äôs relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations ‚Äúby hand.‚Äù"
  },
  {
    "objectID": "projects/hw4_questions.html#supervised-learning-key-driver-analysis",
    "href": "projects/hw4_questions.html#supervised-learning-key-driver-analysis",
    "title": "Comparison to Built-In KMeans",
    "section": "Supervised Learning: Key Driver Analysis",
    "text": "Supervised Learning: Key Driver Analysis\n\nDataset Overview\nThe dataset used in this analysis contains 2,553 survey responses evaluating consumer perceptions of different brands. Each row corresponds to an individual respondent and includes:\n\nA target variable:\n\nsatisfaction: An ordinal score representing the respondent‚Äôs overall satisfaction with the brand (likely on a 1‚Äì5 scale).\n\nA set of predictor variables (potential ‚Äúdrivers‚Äù):\n\ntrust: Does the brand seem trustworthy?\nbuild: Does the brand build strong connections?\ndiffers: Does the brand feel unique or different?\neasy: Is the brand easy to interact with?\nappealing: Is the brand visually or emotionally appealing?\nrewarding: Is using the brand rewarding?\npopular: Is the brand perceived as popular?\nservice: Does the brand offer good service?\nimpact: Does the brand have a meaningful societal impact?\n\nA few identifiers:\n\nid: Unique respondent ID\nbrand: Brand identifier (categorical variable)\n\n\nThese features will be used to estimate a key drivers table, which quantifies the relative importance of each attribute in predicting satisfaction.\n\n# Reload the drivers dataset\ndf_drivers = pd.read_csv(\"data_for_drivers_analysis.csv\")\n\n# Show summary and preview\ndf_drivers.info(), df_drivers.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2553 entries, 0 to 2552\nData columns (total 12 columns):\n #   Column        Non-Null Count  Dtype\n---  ------        --------------  -----\n 0   brand         2553 non-null   int64\n 1   id            2553 non-null   int64\n 2   satisfaction  2553 non-null   int64\n 3   trust         2553 non-null   int64\n 4   build         2553 non-null   int64\n 5   differs       2553 non-null   int64\n 6   easy          2553 non-null   int64\n 7   appealing     2553 non-null   int64\n 8   rewarding     2553 non-null   int64\n 9   popular       2553 non-null   int64\n 10  service       2553 non-null   int64\n 11  impact        2553 non-null   int64\ndtypes: int64(12)\nmemory usage: 239.5 KB\n\n\n(None,\n    brand   id  satisfaction  trust  build  differs  easy  appealing  \\\n 0      1   98             3      1      0        1     1          1   \n 1      1  179             5      0      0        0     0          0   \n 2      1  197             3      1      0        0     1          1   \n 3      1  317             1      0      0        0     0          1   \n 4      1  356             4      1      1        1     1          1   \n \n    rewarding  popular  service  impact  \n 0          0        0        1       0  \n 1          0        0        0       0  \n 2          1        0        1       1  \n 3          0        1        1       1  \n 4          1        1        1       1  )\n\n\n\n\nKey Driver Results\nTo identify what influences satisfaction the most, we ran a linear regression model using standardized versions of all nine potential drivers. Standardization ensures the coefficients are directly comparable, representing the effect size in units of standard deviation.\nThe table below shows the standardized coefficients, ranked from most to least impactful:\n\n\n\nDriver\nStandardized Coefficient\n\n\n\n\nimpact\n0.150\n\n\ntrust\n0.136\n\n\nservice\n0.104\n\n\nappealing\n0.040\n\n\ndiffers\n0.033\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import r2_score\nimport shap\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.inspection import permutation_importance\n\n\n\n# Define predictors and target\nX = df_drivers[['trust', 'build', 'differs', 'easy', 'appealing',\n                'rewarding', 'popular', 'service', 'impact']]\ny = df_drivers['satisfaction']\n\n# Standardize predictors\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Run linear regression\nmodel = LinearRegression()\nmodel.fit(X_scaled, y)\n\n# Extract standardized coefficients\ncoefficients = model.coef_\n# driver_importance = pd.DataFrame({\n#     'Driver': X.columns,\n#     'Standardized Coefficient': coefficients\n# }).sort_values(by='Standardized Coefficient', ascending=False)\n\n# Pearson correlations\npearson_corrs = [pearsonr(X[col], y)[0] for col in X.columns]\n\n\n# Full model R¬≤\nfull_model_r2 = r2_score(y, model.predict(X_scaled))\n\n# Usefulness: R¬≤ drop when each feature is removed\nusefulness = []\nfor i, col in enumerate(X.columns):\n    X_subset = X.drop(columns=[col])\n    X_subset_scaled = scaler.fit_transform(X_subset)\n    model_subset = LinearRegression().fit(X_subset_scaled, y)\n    r2_subset = r2_score(y, model_subset.predict(X_subset_scaled))\n    usefulness.append(full_model_r2 - r2_subset)\n\n# KernelExplainer works with a prediction function\nexplainer = shap.KernelExplainer(model.predict, X_scaled[:100])\nshap_values_kernel = explainer.shap_values(X_scaled[:100], nsamples=100)\n\n# Compute mean absolute Shapley values for each feature\nmean_shap_values = np.abs(shap_values_kernel).mean(axis=0)\n\n\n# Correlation matrix of predictors\nR = np.corrcoef(X.values.T)\n\n# Eigen decomposition\neigval, eigvec = np.linalg.eig(R)\n\n# Compute matrix of transformed variables\nP = eigvec @ np.diag(np.sqrt(eigval))\nZ = X.values @ P\n\n# Regression of y on transformed variables\nbeta = np.linalg.lstsq(Z, y, rcond=None)[0]\n\n# Project back to original variables\nraw_relative_weights = (P @ beta) ** 2\nrelative_importance = raw_relative_weights / sum(raw_relative_weights)\n\n\n# Fit Random Forest\nrf = RandomForestRegressor(n_estimators=100, random_state=0)\nrf.fit(X, y)\n\n\n# Prepare training data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit XGBoost Regressor\nxgb_model = xgb.XGBRegressor(n_estimators=100, random_state=0, verbosity=0)\nxgb_model.fit(X_train, y_train)\n\n# Fit neural network\nmlp = MLPRegressor(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=42)\nmlp.fit(X_train, y_train)\n\n# Permutation importance\nperm_importance = permutation_importance(mlp, X_test, y_test, n_repeats=10, random_state=42)\n\n\n\n# # Add to the previous table\n# importance_df['Usefulness (ŒîR¬≤)'] = usefulness\n\n# Build results DataFrame\nimportance_df = pd.DataFrame({\n    'Driver': X.columns,\n    'Pearson Correlation': pearson_corrs,\n    'Standardized Coefficient': coefficients,\n    'Usefulness (ŒîR¬≤)' : usefulness,\n    'SHAP Value (mean |impact|)': mean_shap_values,\n    \"Johnson's Relative Weight\": relative_importance,\n    'Mean Decrease in Gini': rf.feature_importances_,\n    'XGBoost Importance': xgb_model.feature_importances_,\n    'Neural Net Permutation Importance': perm_importance.importances_mean\n})\n\n\n# Sort by absolute value of standardized coefficient for display\nimportance_df['|Std Coef|'] = importance_df['Standardized Coefficient'].abs()\nimportance_df = importance_df.sort_values(by='|Std Coef|', ascending=False).drop(columns='|Std Coef|')\n\n# Display result\ndisplay(importance_df)\n\n\n\n\n\n\n\n\n\n\n\nDriver\nPearson Correlation\nStandardized Coefficient\nUsefulness (ŒîR¬≤)\nSHAP Value (mean |impact|)\nJohnson's Relative Weight\nMean Decrease in Gini\nXGBoost Importance\nNeural Net Permutation Importance\n\n\n\n\n8\nimpact\n0.254539\n0.150482\n0.011203\n0.155801\n0.013630\n0.134381\n0.301983\n0.043808\n\n\n0\ntrust\n0.255706\n0.135635\n0.008243\n0.124039\n0.343726\n0.168277\n0.119332\n0.029984\n\n\n7\nservice\n0.251098\n0.103573\n0.004674\n0.101761\n0.050370\n0.125663\n0.117257\n0.040302\n\n\n4\nappealing\n0.207997\n0.039647\n0.000710\n0.036713\n0.035775\n0.080812\n0.072072\n0.021934\n\n\n2\ndiffers\n0.184801\n0.032631\n0.000550\n0.034524\n0.016490\n0.089644\n0.059331\n-0.000743\n\n\n3\neasy\n0.212985\n0.025744\n0.000289\n0.023489\n0.153046\n0.099667\n0.079361\n0.002135\n\n\n1\nbuild\n0.191896\n0.023411\n0.000266\n0.022879\n0.069863\n0.100033\n0.092866\n0.005391\n\n\n6\npopular\n0.171425\n0.019470\n0.000204\n0.019326\n0.300324\n0.095521\n0.085266\n0.014060\n\n\n5\nrewarding\n0.194561\n0.005937\n0.000016\n0.005879\n0.016777\n0.106002\n0.072532\n0.006919\n\n\n\n\n\n\n\n\n\nFinal Driver Importance Summary (All Methods)\nWe evaluated the importance of nine brand perception attributes in predicting customer satisfaction using a diverse set of eight techniques:\n\nMethods Used\n\nPearson Correlation ‚Äì Simple pairwise association with satisfaction.\nStandardized Regression Coefficient ‚Äì Impact in a linear model with standardized inputs.\nUsefulness (ŒîR¬≤) ‚Äì Drop in R¬≤ when a feature is removed.\nSHAP Value (mean |impact|) ‚Äì Average contribution of each feature using Shapley values.\nJohnson‚Äôs Relative Weight ‚Äì Decomposes shared and unique variance in linear regression.\nMean Decrease in Gini ‚Äì Feature importance from a random forest model.\nXGBoost Importance ‚Äì Gain-based importance in an ensemble gradient boosting model.\nNeural Net Permutation Importance ‚Äì Drop in performance when input is permuted in an MLP.\n\n\n\n\n\nKey Takeaways\n\nConsistent Top Performers\n\nImpact dominates nearly every metric ‚Äî highest in Usefulness, SHAP, and XGBoost, and near the top in Gini and correlation-based scores.\nTrust is the most robust across linear models and Johnson‚Äôs method, and it performs strongly in tree-based and neural models.\nService consistently ranks in the top 3‚Äì4 across all models ‚Äî a reliable mid-tier driver.\n\n\n\nMethod-Specific Anomalies\n\nPopular ranks surprisingly high in Johnson‚Äôs Relative Weight but relatively lower in tree and NN methods ‚Äî likely due to shared variance with stronger features like trust or impact.\nEasy gets a strong boost from Johnson‚Äôs method but shows weak contributions elsewhere.\nRewarding flops across the board ‚Äî nearly every method ranks it lowest.\n\n\n\nInterpretation Tips\n\nTree-based models (XGBoost, Random Forest) emphasize Impact, Service, and Appealing more than linear models do ‚Äî perhaps because they capture nonlinear interactions.\nPermutation importance from neural nets mirrors SHAP in relative scale, reinforcing trust in their reliability.\n\n\n\n\nConclusion\nIf you‚Äôre prioritizing strategic levers to enhance customer satisfaction, focus on:\n\nImpact ‚Äì Make the brand matter.\nTrust ‚Äì Earn and protect credibility.\nService ‚Äì Consistently deliver good experiences.\n\nBy using a wide variety of methods, we ensure that these conclusions are not artifacts of a single model. This gives us confidence to act on the insights, knowing they are robust across techniques.\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "projects/hw4_questions.html#supervised-learning-key-drivers-analysis",
    "href": "projects/hw4_questions.html#supervised-learning-key-drivers-analysis",
    "title": "Segmentation and Strategy: Using ML to Decode Penguins and People",
    "section": "Supervised Learning: Key Drivers Analysis",
    "text": "Supervised Learning: Key Drivers Analysis\n\nDataset Overview\nThe dataset used in this analysis contains 2,553 survey responses evaluating consumer perceptions of different brands. Each row corresponds to an individual respondent and includes:\n\nA target variable:\n\nsatisfaction: An ordinal score representing the respondent‚Äôs overall satisfaction with the brand (likely on a 1‚Äì5 scale).\n\nA set of predictor variables (potential ‚Äúdrivers‚Äù):\n\ntrust: Does the brand seem trustworthy?\nbuild: Does the brand build strong connections?\ndiffers: Does the brand feel unique or different?\neasy: Is the brand easy to interact with?\nappealing: Is the brand visually or emotionally appealing?\nrewarding: Is using the brand rewarding?\npopular: Is the brand perceived as popular?\nservice: Does the brand offer good service?\nimpact: Does the brand have a meaningful societal impact?\n\nA few identifiers:\n\nid: Unique respondent ID\nbrand: Brand identifier (categorical variable)\n\n\nThese features will be used to estimate a key drivers table, which quantifies the relative importance of each attribute in predicting satisfaction.\n\n# Reload the drivers dataset\ndf_drivers = pd.read_csv(\"data_for_drivers_analysis.csv\")\n\n# Show summary and preview\ndf_drivers.info(), df_drivers.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2553 entries, 0 to 2552\nData columns (total 12 columns):\n #   Column        Non-Null Count  Dtype\n---  ------        --------------  -----\n 0   brand         2553 non-null   int64\n 1   id            2553 non-null   int64\n 2   satisfaction  2553 non-null   int64\n 3   trust         2553 non-null   int64\n 4   build         2553 non-null   int64\n 5   differs       2553 non-null   int64\n 6   easy          2553 non-null   int64\n 7   appealing     2553 non-null   int64\n 8   rewarding     2553 non-null   int64\n 9   popular       2553 non-null   int64\n 10  service       2553 non-null   int64\n 11  impact        2553 non-null   int64\ndtypes: int64(12)\nmemory usage: 239.5 KB\n\n\n(None,\n    brand   id  satisfaction  trust  build  differs  easy  appealing  \\\n 0      1   98             3      1      0        1     1          1   \n 1      1  179             5      0      0        0     0          0   \n 2      1  197             3      1      0        0     1          1   \n 3      1  317             1      0      0        0     0          1   \n 4      1  356             4      1      1        1     1          1   \n \n    rewarding  popular  service  impact  \n 0          0        0        1       0  \n 1          0        0        0       0  \n 2          1        0        1       1  \n 3          0        1        1       1  \n 4          1        1        1       1  )\n\n\n\n\nKey Driver Results\nTo identify what influences satisfaction the most, we ran a linear regression model using standardized versions of all nine potential drivers. Standardization ensures the coefficients are directly comparable, representing the effect size in units of standard deviation.\nThe table below shows the standardized coefficients, ranked from most to least impactful:\n\n\n\nDriver\nStandardized Coefficient\n\n\n\n\nimpact\n0.150\n\n\ntrust\n0.136\n\n\nservice\n0.104\n\n\nappealing\n0.040\n\n\ndiffers\n0.033\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import r2_score\nimport shap\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.inspection import permutation_importance\n\n\n\n# Define predictors and target\nX = df_drivers[['trust', 'build', 'differs', 'easy', 'appealing',\n                'rewarding', 'popular', 'service', 'impact']]\ny = df_drivers['satisfaction']\n\n# Standardize predictors\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Run linear regression\nmodel = LinearRegression()\nmodel.fit(X_scaled, y)\n\n# Extract standardized coefficients\ncoefficients = model.coef_\n# driver_importance = pd.DataFrame({\n#     'Driver': X.columns,\n#     'Standardized Coefficient': coefficients\n# }).sort_values(by='Standardized Coefficient', ascending=False)\n\n# Pearson correlations\npearson_corrs = [pearsonr(X[col], y)[0] for col in X.columns]\n\n\n# Full model R¬≤\nfull_model_r2 = r2_score(y, model.predict(X_scaled))\n\n# Usefulness: R¬≤ drop when each feature is removed\nusefulness = []\nfor i, col in enumerate(X.columns):\n    X_subset = X.drop(columns=[col])\n    X_subset_scaled = scaler.fit_transform(X_subset)\n    model_subset = LinearRegression().fit(X_subset_scaled, y)\n    r2_subset = r2_score(y, model_subset.predict(X_subset_scaled))\n    usefulness.append(full_model_r2 - r2_subset)\n\n# KernelExplainer works with a prediction function\nexplainer = shap.KernelExplainer(model.predict, X_scaled[:100])\nshap_values_kernel = explainer.shap_values(X_scaled[:100], nsamples=100)\n\n# Compute mean absolute Shapley values for each feature\nmean_shap_values = np.abs(shap_values_kernel).mean(axis=0)\n\n\n# Correlation matrix of predictors\nR = np.corrcoef(X.values.T)\n\n# Eigen decomposition\neigval, eigvec = np.linalg.eig(R)\n\n# Compute matrix of transformed variables\nP = eigvec @ np.diag(np.sqrt(eigval))\nZ = X.values @ P\n\n# Regression of y on transformed variables\nbeta = np.linalg.lstsq(Z, y, rcond=None)[0]\n\n# Project back to original variables\nraw_relative_weights = (P @ beta) ** 2\nrelative_importance = raw_relative_weights / sum(raw_relative_weights)\n\n\n# Fit Random Forest\nrf = RandomForestRegressor(n_estimators=100, random_state=0)\nrf.fit(X, y)\n\n\n# Prepare training data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit XGBoost Regressor\nxgb_model = xgb.XGBRegressor(n_estimators=100, random_state=0, verbosity=0)\nxgb_model.fit(X_train, y_train)\n\n# Fit neural network\nmlp = MLPRegressor(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=42)\nmlp.fit(X_train, y_train)\n\n# Permutation importance\nperm_importance = permutation_importance(mlp, X_test, y_test, n_repeats=10, random_state=42)\n\n\n\n# # Add to the previous table\n# importance_df['Usefulness (ŒîR¬≤)'] = usefulness\n\n# Build results DataFrame\nimportance_df = pd.DataFrame({\n    'Driver': X.columns,\n    'Pearson Correlation': pearson_corrs,\n    'Standardized Coefficient': coefficients,\n    'Usefulness (ŒîR¬≤)' : usefulness,\n    'SHAP Value (mean |impact|)': mean_shap_values,\n    \"Johnson's Relative Weight\": relative_importance,\n    'Mean Decrease in Gini': rf.feature_importances_,\n    'XGBoost Importance': xgb_model.feature_importances_,\n    'Neural Net Permutation Importance': perm_importance.importances_mean\n})\n\n\n# Sort by absolute value of standardized coefficient for display\nimportance_df['|Std Coef|'] = importance_df['Standardized Coefficient'].abs()\nimportance_df = importance_df.sort_values(by='|Std Coef|', ascending=False).drop(columns='|Std Coef|')\n\n# Display result\ndisplay(importance_df)\n\n\n\n\n\n\n\n\n\n\n\nDriver\nPearson Correlation\nStandardized Coefficient\nUsefulness (ŒîR¬≤)\nSHAP Value (mean |impact|)\nJohnson's Relative Weight\nMean Decrease in Gini\nXGBoost Importance\nNeural Net Permutation Importance\n\n\n\n\n8\nimpact\n0.254539\n0.150482\n0.011203\n0.155801\n0.013630\n0.134381\n0.301983\n0.043808\n\n\n0\ntrust\n0.255706\n0.135635\n0.008243\n0.124039\n0.343726\n0.168277\n0.119332\n0.029984\n\n\n7\nservice\n0.251098\n0.103573\n0.004674\n0.101761\n0.050370\n0.125663\n0.117257\n0.040302\n\n\n4\nappealing\n0.207997\n0.039647\n0.000710\n0.036713\n0.035775\n0.080812\n0.072072\n0.021934\n\n\n2\ndiffers\n0.184801\n0.032631\n0.000550\n0.034524\n0.016490\n0.089644\n0.059331\n-0.000743\n\n\n3\neasy\n0.212985\n0.025744\n0.000289\n0.023489\n0.153046\n0.099667\n0.079361\n0.002135\n\n\n1\nbuild\n0.191896\n0.023411\n0.000266\n0.022879\n0.069863\n0.100033\n0.092866\n0.005391\n\n\n6\npopular\n0.171425\n0.019470\n0.000204\n0.019326\n0.300324\n0.095521\n0.085266\n0.014060\n\n\n5\nrewarding\n0.194561\n0.005937\n0.000016\n0.005879\n0.016777\n0.106002\n0.072532\n0.006919\n\n\n\n\n\n\n\n\n\nFinal Driver Importance Summary (All Methods)\nWe evaluated the importance of nine brand perception attributes in predicting customer satisfaction using a diverse set of eight techniques:\n\nMethods Used\n\nPearson Correlation ‚Äì Simple pairwise association with satisfaction.\nStandardized Regression Coefficient ‚Äì Impact in a linear model with standardized inputs.\nUsefulness (ŒîR¬≤) ‚Äì Drop in R¬≤ when a feature is removed.\nSHAP Value (mean |impact|) ‚Äì Average contribution of each feature using Shapley values.\nJohnson‚Äôs Relative Weight ‚Äì Decomposes shared and unique variance in linear regression.\nMean Decrease in Gini ‚Äì Feature importance from a random forest model.\nXGBoost Importance ‚Äì Gain-based importance in an ensemble gradient boosting model.\nNeural Net Permutation Importance ‚Äì Drop in performance when input is permuted in an MLP.\n\n\n\n\nKey Takeaways\n\nConsistent Top Performers\n\nImpact dominates nearly every metric ‚Äî highest in Usefulness, SHAP, and XGBoost, and near the top in Gini and correlation-based scores.\nTrust is the most robust across linear models and Johnson‚Äôs method, and it performs strongly in tree-based and neural models.\nService consistently ranks in the top 3‚Äì4 across all models ‚Äî a reliable mid-tier driver.\n\n\n\nMethod-Specific Anomalies\n\nPopular ranks surprisingly high in Johnson‚Äôs Relative Weight but relatively lower in tree and NN methods ‚Äî likely due to shared variance with stronger features like trust or impact.\nEasy gets a strong boost from Johnson‚Äôs method but shows weak contributions elsewhere.\nRewarding flops across the board ‚Äî nearly every method ranks it lowest.\n\n\n\nInterpretation Tips\n\nTree-based models (XGBoost, Random Forest) emphasize Impact, Service, and Appealing more than linear models do ‚Äî perhaps because they capture nonlinear interactions.\nPermutation importance from neural nets mirrors SHAP in relative scale, reinforcing trust in their reliability.\n\n\n\n\nConclusion\nIf you‚Äôre prioritizing strategic levers to enhance customer satisfaction, focus on:\n\nImpact ‚Äì Make the brand matter.\nTrust ‚Äì Earn and protect credibility.\nService ‚Äì Consistently deliver good experiences.\n\nBy using a wide variety of methods, we ensure that these conclusions are not artifacts of a single model. This gives us confidence to act on the insights, knowing they are robust across techniques."
  },
  {
    "objectID": "projects/poisson_regression_examples.html",
    "href": "projects/poisson_regression_examples.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\n\nShow code\nimport pandas as pd\n\n# Blueprinty‚Äôs 1,500-firm sample\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n# Basic dimensions\nn_blue, p_blue = blueprinty.shape\nprint(f\"Blueprinty dataset ‚Üí {n_blue:,} firms √ó {p_blue} columns\")\n\n\nBlueprinty dataset ‚Üí 1,500 firms √ó 4 columns\n\n\n\n\nScope & granularity\n* 1,500 mature U.S. engineering firms (non-start-ups).\n* Observation unit = firm; time horizon = last five fiscal years.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nBrief description\n\n\n\n\npatents\nInteger (count)\nNumber of patents awarded in the last 5 years (response variable).\n\n\niscustomer\nBinary (0/1)\n1 = firm uses Blueprinty software.\n\n\nregion\nCategorical\nFive regions: Midwest, Northeast, Northwest, South, Southwest.\n\n\nage\nInteger\nYears since incorporation (firm age).\n\n\n\n\n\n\n\n\n\n\nShow code\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8,5))\n\nlabel_map = {0: \"Non-customer\", 1: \"Customer\"}\n\nfor flag, grp in blueprinty.groupby(\"iscustomer\"):\n    grp[\"patents\"].plot(\n        kind=\"hist\",\n        bins=range(0, blueprinty[\"patents\"].max() + 2),\n        alpha=0.55,\n        label=label_map[flag],\n        ax=ax,\n        edgecolor=\"white\"\n    )\n\nax.set_xlabel(\"Patents awarded (last 5 years)\")\nax.set_ylabel(\"Number of firms\")\nax.set_title(\"Distribution of Patents by Blueprinty Customer Status\")\nax.legend(title=\"Blueprinty user?\")\n\n# --- key lines to stop cropping ---\nplt.xticks(range(0, blueprinty[\"patents\"].max() + 1))   # full tick set\nfig.subplots_adjust(bottom=0.15, right=0.97)            # pad edges\nfig.tight_layout()                                      # tidy up\n# -----------------------------------\n\nplt.show()\n\n# Mean patents for each group\nmean_patents = (\n    blueprinty.groupby(\"iscustomer\")[\"patents\"]\n    .mean()\n    .rename(index=label_map)\n)\nprint(\"Mean patents: \", mean_patents)\n\n\n\n\n\n\n\n\n\nMean patents:  iscustomer\nNon-customer    3.473013\nCustomer        4.133056\nName: patents, dtype: float64\n\n\n::::\n\n\n\n\n\n\n\nFirm Group\nMean Patents (5-year total)\n\n\n\n\nBlueprinty customers\n4.13\n\n\nNon-customers\n3.47\n\n\n\nObservations\n\nHigher average among customers ‚Äì Firms that license Blueprinty record, on average, 0.66 additional patents over five years, an uplift of roughly 19 percent relative to non-customers.\n\nDistributional shift, not overhaul ‚Äì Although both groups cluster between one and five patents, customer firms show a right-ward shift and a slightly thicker upper tail (extending beyond ten patents).\n\nSubstantial overlap ‚Äì The histograms overlap heavily in the modal 0‚Äì5-patent range, indicating that many firms achieve modest patent activity regardless of Blueprinty usage.\n\nInterpretation caveat ‚Äì These descriptive statistics are correlational. More prolific filers may simply be more inclined to adopt specialized software. A Poisson regression that controls for firm age, region, and other covariates is required before drawing causal conclusions.\n\n\n\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\n# ‚îÄ‚îÄ Regional mix ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nregion_counts = (\n    blueprinty.groupby([\"iscustomer\", \"region\"])\n    .size()\n    .unstack(fill_value=0)\n)\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0) * 100\nregion_props = region_props.round(1)\ndisplay(region_props.style.format(\"{:.1f}\").set_caption(\"Regional share of firms (%)\"))\n\n# Bar chart with annotations\npalette = sns.color_palette(\"colorblind\", 2)\nfig, ax = plt.subplots(figsize=(10, 5))\nwidth = 0.35\nx = range(len(region_props.columns))\n\nax.bar([p - width/2 for p in x],\n       region_props.loc[0],\n       width=width,\n       color=palette[0],\n       label=\"Non-customer\")\n\nax.bar([p + width/2 for p in x],\n       region_props.loc[1],\n       width=width,\n       color=palette[1],\n       label=\"Customer\")\n\n# Annotate percentages on each bar\nfor i, region in enumerate(region_props.columns):\n    ax.text(i - width/2,\n            region_props.loc[0, region] + 1,\n            f\"{region_props.loc[0, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n    ax.text(i + width/2,\n            region_props.loc[1, region] + 1,\n            f\"{region_props.loc[1, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n\nax.set_xticks(x)\nax.set_xticklabels(region_props.columns, rotation=45, ha=\"right\")\nax.set_ylabel(\"Share of firms (%)\")\nax.set_title(\"Regional composition by Blueprinty customer status\")\nax.legend(title=\"Group\")\nfig.tight_layout()\n\n# ‚îÄ‚îÄ Age distribution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nage_summary = (\n    blueprinty.groupby(\"iscustomer\")[\"age\"]\n    .describe()[[\"mean\", \"std\", \"25%\", \"50%\", \"75%\"]]\n    .round(2)\n)\ndisplay(age_summary.style.set_caption(\"Firm age summary (years)\"))\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    hue=\"iscustomer\",\n    dodge=False,\n    palette=palette,\n    legend=False,\n    showfliers=False  # keep whiskers clean; outliers still visible via stripplot\n)\nsns.stripplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    color=\"gray\",\n    alpha=0.4,\n    jitter=0.25\n)\nplt.xticks([0, 1], [\"Non-customer\", \"Customer\"])\nplt.xlabel(\"\")\nplt.ylabel(\"Firm age (years)\")\nplt.title(\"Firm age by Blueprinty customer status\")\nplt.tight_layout()\n\n\n\n\n\n\n\nTable¬†1: Regional share of firms (%)\n\n\n\n\n\nregion\nMidwest\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\niscustomer\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n\n\n0\n18.4\n26.8\n15.5\n15.3\n24.0\n\n\n1\n7.7\n68.2\n6.0\n7.3\n10.8\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable¬†2: Firm age summary (years)\n\n\n\n\n\n¬†\nmean\nstd\n25%\n50%\n75%\n\n\niscustomer\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n\n\n0\n26.100000\n6.950000\n21.000000\n25.500000\n31.250000\n\n\n1\n26.900000\n7.810000\n20.500000\n26.500000\n32.500000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegional composition\n\nBlueprinty customers are highly concentrated in the Northeast (‚âà 68 %), whereas non-customers are spread much more evenly (Midwest 18 %, Southwest 24 %, Northeast 27 %, etc.).\n\nThe Midwest-to-Southwest corridor accounts for roughly two-thirds of non-customer firms but less than one-third of customer firms, underscoring a strong geographic skew in adoption.\n\nFirm age\n\nCustomer firms are marginally older‚Äîmean = 26.9 yrs vs 26.1 yrs; medians differ by one year (26.5 vs 25.5).\n\nQuartiles overlap substantially, and the boxplots show similar spreads. Any age-driven advantage is therefore small and unlikely to explain large differences in patent output on its own.\n\n\nImplications\n* The pronounced Northeast bias suggests region is a critical control variable when modeling patent counts; otherwise the software‚Äôs effect could be conflated with location-specific innovation hubs.\n* Age should also enter the regression, but its modest gap indicates it is a weaker confounder.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nLet\n\\[\nY \\;\\sim\\; \\operatorname{Poisson}(\\lambda)\n\\]\nwith probability-mass function\n\\[\nf(y \\mid \\lambda)\n\\;=\\;\n\\frac{e^{-\\lambda}\\,\\lambda^{y}}{y!},\n\\qquad\ny = 0,1,2,\\ldots\n\\]\n\n\n\n\\[\n\\mathcal{L}(\\lambda; y)\n\\;=\\;\ne^{-\\lambda}\\,\n\\frac{\\lambda^{y}}{y!}.\n\\]\n\n\n\n\nFor an i.i.d. sample\n\\(\\mathbf y = (y_1,\\dots,y_n)\\),\n\\[\n\\mathcal{L}(\\lambda; \\mathbf y)\n\\;=\\;\n\\prod_{i=1}^{n}\ne^{-\\lambda}\\,\\frac{\\lambda^{y_i}}{y_i!}\n\\;=\\;\ne^{-n\\lambda}\\,\n\\lambda^{\\sum_{i=1}^{n} y_i}\\,\n\\prod_{i=1}^{n} \\frac{1}{y_i!}.\n\\]\n\n\n\n\n\\[\n\\ell(\\lambda; \\mathbf y)\n\\;=\\;\n-n\\lambda\n\\;+\\;\n\\Bigl(\\sum_{i=1}^{n} y_i\\Bigr)\\,\\log\\lambda\n\\;-\\;\n\\sum_{i=1}^{n} \\log(y_i!).\n\\]\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\n\n\n\n\n\npoisson_loglikelihood(lmbda, y) ‚Äî overview\n\nPurpose‚ÄÇReturn the Poisson log-likelihood\n(() = -n+ (y_i)- (y_i!)).\nInputs\n\nlmbda ‚Äî candidate rate Œª (must be &gt; 0).\n\ny ‚Äî array/Series of observed patent counts.\n\nNumerical stability‚ÄÇUses scipy.special.gammaln(y + 1) to compute (\\(\\log(y!)\\)) safely.\nValidity check‚ÄÇIf lmbda ‚â§ 0, the function returns -np.inf, signalling an invalid parameter to any optimiser.\n\nThe result is a single float that can be maximised (or its negative minimised) to obtain the MLE.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln   # log-Œì for stable log(y!)\n\ndef poisson_loglikelihood(lmbda: float, y):\n    \"\"\"\n    Log-likelihood for a sample of i.i.d. Poisson(Œª) counts.\n\n    Parameters\n    ----------\n    lmbda : float\n        Rate parameter Œª (must be &gt; 0).\n    y : array-like\n        Vector or Series of observed non-negative integers.\n\n    Returns\n    -------\n    float\n        ‚Ñì(Œª; y) = ‚ÄìnŒª + (Œ£y_i)¬∑log Œª ‚Äì Œ£ log(y_i!)\n    \"\"\"\n    if lmbda &lt;= 0:\n        return -np.inf                         # undefined for Œª ‚â§ 0\n\n    y = np.asarray(y)\n    n = y.size\n    return (\n        -n * lmbda\n        + y.sum() * np.log(lmbda)\n        - gammaln(y + 1).sum()                # log(y!) via Œì(y+1)\n    )\n\n\n\n\n\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Re-use the helper from the previous chunk\n# (poisson_loglikelihood already defined)\n\ny_patents = blueprinty[\"patents\"].values\nlambda_grid = np.linspace(0.1, 10, 300)\nloglik_vals = [poisson_loglikelihood(lmbda, y_patents) for lmbda in lambda_grid]\n\nmle_hat = y_patents.mean()  # ‚âà 3.65 for this sample\nmle_ll  = poisson_loglikelihood(mle_hat, y_patents)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(lambda_grid, loglik_vals, lw=2)\nax.axvline(mle_hat, color=\"tab:red\", ls=\"--\",\n           label=fr\"MLE  $\\hat{{\\lambda}}={mle_hat:.2f}$\")\nax.scatter([mle_hat], [mle_ll], color=\"tab:red\")\nax.set_xlabel(r\"$\\lambda$\")\nax.set_ylabel(r\"log-likelihood  $\\ell(\\lambda\\,;\\mathbf{y})$\")\nax.set_title(\"Poisson log-likelihood across candidate $\\\\lambda$\")\nax.legend()\nax.margins(x=0)          # prevent cropping at the edges\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nPurpose.‚ÄÉWe evaluated the Poisson log-likelihood\n\\[\n  \\ell(\\lambda;\\mathbf y)= -n\\lambda + \\Bigl(\\sum y_i\\Bigr)\\log\\lambda-\\sum\\log(y_i!)\n\\]\nover a dense grid of candidate (\\(\\lambda\\)) values (0.1 ‚Äì 10) to visualise how well each rate parameter explains the observed patent counts.\nCode steps.\n\nGenerate grid. lambda_grid = np.linspace(0.1, 10, 300)\ngives 300 evenly-spaced test values.\nCompute log-likelihood. For each grid point we call poisson_loglikelihood(lmbda, y_patents) to get ((;y)).\nLocate the peak. The analytic MLE is the sample mean\n(\\(\\hat\\lambda\\) = \\(\\bar y \\approx 3.68\\)).\nWe compute its log-likelihood and mark it with a red dashed line plus a dot at the exact maximum.\nPlot. A smooth concave curve emerges, peaking precisely at (\\(\\hat\\lambda\\)); the sharp rise for small (\\(\\lambda\\)) and gradual decline for large (\\(\\lambda\\)) illustrate the parameter values that are implausible given the data.\n\nInterpretation.\n\nThe global maximum occurs where the red marker sits, confirming the numerical and analytic MLEs coincide.\nThe curve‚Äôs concavity guarantees a unique solution; any optimiser starting within the positive domain will converge to (\\(\\hat\\lambda\\)=\\(\\bar y\\)).\nVisually, patent-arrival rates below ~2 or above ~6 are strongly disfavoured (log-likelihood drops steeply), reinforcing the empirical estimate around 3 ‚Äì 4 patents per firm over five years.\n\n\nThe plot verifies that the maximum of the likelihood function aligns with the sample mean and gives us a tangible sense of how sensitive the likelihood is to deviations from the MLE.\n\n\n\nStart from the sample log-likelihood\n\\[\n\\ell(\\lambda;\\,\\mathbf y)\n\\;=\\;\n-n\\lambda\n+\n\\Bigl(\\sum_{i=1}^{n} y_i\\Bigr)\\,\\log\\lambda\n-\n\\sum_{i=1}^{n}\\log\\bigl(y_i!\\bigr),\n\\qquad \\lambda&gt;0.\n\\]\n\n\n\n\\[\n\\frac{\\partial\\ell}{\\partial\\lambda}\n\\;=\\;\n-n\n+\n\\frac{\\displaystyle \\sum_{i=1}^{n} y_i}{\\lambda}.\n\\]\n\n\n\n\n\\[\n0 \\;=\\; -n + \\frac{\\sum_{i=1}^{n} y_i}{\\lambda}\n\\;\\;\\Longrightarrow\\;\\;\n\\widehat{\\lambda}_{\\text{MLE}}\n\\;=\\;\n\\frac{1}{n}\\sum_{i=1}^{n} y_i\n\\;=\\;\n\\bar y.\n\\]\n\n\n\n\n\\[\n\\frac{\\partial^{2}\\ell}{\\partial\\lambda^{2}}\n\\;=\\;\n-\\frac{\\displaystyle \\sum_{i=1}^{n} y_i}{\\lambda^{2}}\n\\;&lt;\\;0\n\\qquad (\\lambda&gt;0),\n\\]\nso the critical point is a global maximum.\nHence, the maximum-likelihood estimator for the Poisson rate is simply the sample mean:\n\\[\n\\boxed{\\ \\widehat{\\lambda}=\\bar y\\ }.\n\\]\n\n\n\n\n\nBelow we maximise the log-likelihood by minimising its negative.\nminimize_scalar is perfect because the Poisson model has only one parameter, (\\(\\lambda\\)).\n\n\nShow code\nfrom scipy.optimize import minimize_scalar\n\n# Negative log-likelihood wrapper\ndef neg_loglik(lmbda):\n    return -poisson_loglikelihood(lmbda, blueprinty[\"patents\"].values)\n\n# Bounded search keeps Œª &gt; 0 and avoids wandering into silly values\nopt_res = minimize_scalar(\n    neg_loglik,\n    bounds=(1e-6, 20),    # search interval: (0, 20]\n    method=\"bounded\"\n)\n\n# Pretty output\nprint(f\"MLE via optimisation  :  ŒªÃÇ = {opt_res.x:.4f}\")\nprint(f\"Log-likelihood at ŒªÃÇ   :  ‚Ñì = {-opt_res.fun:.2f}\")\nprint(f\"Sample mean (check)    :  »≥ = {blueprinty['patents'].mean():.4f}\")\n\n\nMLE via optimisation  :  ŒªÃÇ = 3.6847\nLog-likelihood at ŒªÃÇ   :  ‚Ñì = -3367.68\nSample mean (check)    :  »≥ = 3.6847\n\n\nMLE optimisation summary\n\n\n\nMetric\nValue\n\n\n\n\nOptimised rate (\\(\\hat{\\lambda}\\))\n3.6847\n\n\nLog-likelihood at (\\(\\hat{\\lambda}\\))\n‚Äì3367.68\n\n\nSample mean (\\(\\bar{y}\\))\n3.6847\n\n\n\n\nscipy.optimize.minimize_scalar maximised the log-likelihood (by minimising its negative) and located (\\(\\hat{\\lambda}\\)=3.6847 ).\n\nThe optimiser‚Äôs estimate matches the sample mean exactly, confirming the analytic result (\\(\\hat{\\lambda}_{\\text{MLE}}\\) = \\(\\bar{y}\\)) for a Poisson model.\n\nThe reported log-likelihood (‚Äì3367.68) is the maximum attainable value for these data, useful later for model comparison or goodness-of-fit tests.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\n\n\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln        # stable log(y!)\n\ndef poisson_regression_loglik(beta, y, X):\n    \"\"\"\n    Log-likelihood for a Poisson GLM with log link.\n\n    Parameters\n    ----------\n    beta : array-like, shape (p,)\n        Coefficient vector (includes intercept if X has a 1s column).\n    y : array-like, shape (n,)\n        Observed non-negative counts.\n    X : array-like, shape (n, p)\n        Covariate matrix.\n\n    Returns\n    -------\n    float\n        ‚Ñì(Œ≤) = Œ£ [ y_i¬∑(X_i Œ≤)  ‚àí  exp(X_i Œ≤)  ‚àí  log(y_i!) ].\n    \"\"\"\n    beta = np.asarray(beta, dtype=float)\n    y    = np.asarray(y,    dtype=float)\n\n    eta  = X @ beta            # linear predictor  (n,)\n    lam  = np.exp(eta)         # inverse-link ‚áí Œª_i &gt; 0\n\n    return (y * eta  -  lam  -  gammaln(y + 1)).sum()\n\n\n\n\n\n\n\n\n\n\n\n\nElement\nSimple Poisson\nPoisson regression\n\n\n\n\nParameter\nsingle rate \\(\\lambda\\)\ncoefficient vector \\(\\boldsymbol{\\beta}\\)\n\n\nMean\nconstant \\(\\lambda\\)\n\\(\\lambda_i = \\exp(X_i^\\top\\beta)\\) (log link)\n\n\nLog-likelihood term\n\\(y\\,\\log\\lambda - \\lambda\\)\n\\(y_i(X_i\\beta) - \\exp(X_i\\beta)\\)\n\n\nInputs\nlmbda, y\nbeta, y, X\n\n\n\n\n\n\n\nWe model each firm‚Äôs patent count as\n\\[\nY_i \\;\\big|\\;X_i \\sim \\operatorname{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\lambda_i = \\exp\\!\\bigl(X_i^{\\!\\top}\\beta\\bigr),\n\\]\nwhere (\\(X_i\\)) includes an intercept, age, age¬≤, four region dummies (Midwest omitted), and the customer indicator.\n\n\nShow code\nimport pandas as pd\nimport statsmodels.api as sm          # convenient optimiser + Hessian\n\nX = pd.DataFrame({\n    \"const\"     : 1,                                     # intercept\n    \"age\"       : blueprinty[\"age\"],\n    \"age_sq\"    : blueprinty[\"age\"]**2,\n    \"region_NE\" : (blueprinty[\"region\"]==\"Northeast\").astype(int),\n    \"region_NW\" : (blueprinty[\"region\"]==\"Northwest\").astype(int),\n    \"region_S\"  : (blueprinty[\"region\"]==\"South\").astype(int),\n    \"region_SW\" : (blueprinty[\"region\"]==\"Southwest\").astype(int),\n    \"customer\"  : blueprinty[\"iscustomer\"]\n})\ny = blueprinty[\"patents\"]\n\n# ‚îÄ‚îÄ Poisson GLM (log link) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nres   = model.fit()                      # uses IRLS ‚áí MLE, Hessian\n\nresults = pd.DataFrame({\n    \"Coefficient\" : res.params,\n    \"Std. Error\"  : res.bse\n})\nprint(\"Poisson Regression Results\", results.round(4))\n\n\nPoisson Regression Results            Coefficient  Std. Error\nconst          -0.5089      0.1832\nage             0.1486      0.0139\nage_sq         -0.0030      0.0003\nregion_NE       0.0292      0.0436\nregion_NW      -0.0176      0.0538\nregion_S        0.0566      0.0527\nregion_SW       0.0506      0.0472\ncustomer        0.2076      0.0309\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nÃÇŒ≤\ns.e.\nPractical meaning\n\n\n\n\nIntercept\n‚àí0.509\n0.183\nBaseline log-rate for a Midwest non-customer aged 0.\n\n\nAge\n0.149\n0.014\nEach extra year increases the expected patent rate by 16 % ((e^{0.149})).\n\n\nAge¬≤\n‚àí0.0030\n0.0003\nDiminishing returns: the age effect tapers as firms mature.\n\n\nRegion (NE, NW, S, SW)\n¬±0.03‚Äì0.06\n‚âà0.05\nNo region differs significantly from the Midwest reference once other factors are held constant.\n\n\nCustomer\n0.208\n0.031\nBlueprinty users file 23 % more patents on average (\\(e^{0.208}\\) = 1.23).\n\n\n\nEstimation details\nMaximum likelihood was obtained via statsmodels‚Äô Poisson GLM (log link).\nThe Hessian at the optimum provides the variance‚Äìcovariance matrix;\nstandard errors are the square-roots of its diagonal elements.\nKey takeaway\nAfter controlling for age and geography, Blueprinty adoption remains a statistically and economically meaningful driver of patent output. The quadratic age term confirms a life-cycle pattern‚Äîoutput rises with experience but eventually plateaus‚Äîwhile regional effects are negligible.\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nInterpretation (holding others constant)\n\n\n\n\nconst\n‚àí0.5089\n0.1832\nBaseline log-rate for a Midwest non-customer aged 0.\n\n\nage\n0.1486\n0.0139\nEach additional year raises the patent log-rate by ~0.149.\n\n\nage_sq\n‚àí0.0030\n0.0003\nConcavity: growth in patents tapers with age.\n\n\nregion_NE\n0.0292\n0.0436\nNo significant difference vs Midwest (p ‚âà 0.50).\n\n\nregion_NW\n‚àí0.0176\n0.0538\n‚Äî\n\n\nregion_S\n0.0566\n0.0527\n‚Äî\n\n\nregion_SW\n0.0506\n0.0472\n‚Äî\n\n\ncustomer\n0.2076\n0.0309\nBlueprinty users have a 23 % higher expected patent rate (exp 0.208 ‚âà 1.23).\n\n\n\n\n\n\n\nOur hand-coded optimiser produced the coefficient vector\n(\\(\\hat{\\beta}_{\\text{MLE}}\\)). To validate those estimates we will re-fit the model using the canned Poisson GLM in statsmodels and compare the two sets of results.\n\n\nShow code\nimport numpy as np, pandas as pd, statsmodels.api as sm\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nXm = X.values\n\n# ‚îÄ‚îÄ Custom log-likelihood & optimiser ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndef pll(beta, y, X):\n    eta = X @ beta\n    lam = np.exp(eta)\n    return (y*eta - lam - gammaln(y + 1)).sum()\n\ndef neg_pll(beta, y, X):\n    return -pll(beta, y, X)\n\nbeta0    = np.zeros(Xm.shape[1])\nopt_res  = minimize(neg_pll, beta0, args=(y, Xm), method=\"BFGS\")\nbeta_hat = opt_res.x                     # ‚á† custom MLE vector\n\n# ‚îÄ‚îÄ Built-in GLM (IRLS) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nglm_res = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# ‚îÄ‚îÄ Side-by-side comparison ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ncompare = pd.DataFrame({\n    \"Custom Œ≤ÃÇ\": beta_hat,\n    \"GLM Œ≤ÃÇ\"   : glm_res.params,\n    \"|Œî|\"      : np.abs(beta_hat - glm_res.params)\n}, index=X.columns).round(6)\n\ndisplay(compare)\n\n\n\n\n\n\n\n\n\nCustom Œ≤ÃÇ\nGLM Œ≤ÃÇ\n|Œî|\n\n\n\n\nconst\n0.0\n-0.508920\n0.508920\n\n\nage\n0.0\n0.148619\n0.148619\n\n\nage_sq\n0.0\n-0.002970\n0.002970\n\n\nregion_NE\n0.0\n0.029170\n0.029170\n\n\nregion_NW\n0.0\n-0.017575\n0.017575\n\n\nregion_S\n0.0\n0.056561\n0.056561\n\n\nregion_SW\n0.0\n0.050576\n0.050576\n\n\ncustomer\n0.0\n0.207591\n0.207591\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nexp(Œ≤)\nPractical meaning\n\n\n\n\nIntercept\n‚Äì0.509\n0.60\nA Midwest non-customer that is age 0 (baseline) is expected to average 0.60 patents in 5 years.\n\n\nAge\n0.149\n1.16\nEach additional year of age raises the expected patent rate by ‚âà 16 %, holding everything else constant.\n\n\nAge¬≤\n-0.0030\n‚Äî\nNegative sign implies diminishing returns‚Äîthe marginal boost from age shrinks as firms mature.\n\n\nRegion dummies\n¬±‚Äâ0.03‚Äì0.06\n0.97‚Äì1.06\nNone differ significantly from the Midwest reference; geographic location adds little once age and customer status are controlled for.\n\n\nCustomer\n0.208\n1.23\nFirms using Blueprinty file 23 % more patents on average than non-customers, ceteris paribus.\n\n\n\nKey take-aways\n\nBlueprinty effect is economically meaningful and precise.\nThe log-rate coefficient of 0.208 (SE ‚âà 0.031) is highly significant, translating to a 23 % lift in patent output.\nFirm maturity follows an inverted-U.\nThe positive age term paired with a small negative age-squared term suggests productivity rises early, then plateaus‚Äîconsistent with a life-cycle story.\nRegions add little explanatory power.\nOnce we account for age and Blueprinty usage, regional coefficients hover near zero and lack statistical significance.\nBaseline level (intercept).\nA young Midwest non-customer averages about 0.6 patents in five years; covariate adjustments scale this baseline via multiplicative factors (\\(e^{Œ≤}\\)).\n\nOverall, the regression supports Blueprinty‚Äôs marketing claim: even after adjusting for age and geography, customer firms exhibit a materially higher patent success rate.\n\n\n\n\nTo translate the log-rate coefficient on customer into an intuitive ‚Äúextra patents‚Äù metric, we predicted each firm‚Äôs patent count under two scenarios:\n\nX‚ÇÄ ‚Äì identical covariates but customer = 0 for every firm\n\nX‚ÇÅ ‚Äì identical covariates but customer = 1 for every firm\n\n\n\nShow code\nimport pandas as pd, statsmodels.api as sm\nfrom pathlib import Path\n\n# --- Fit Poisson GLM -----------------------------------------\nmodel = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# --- Counter-factual matrices --------------------------------\nX0 = X.copy();  X0[\"customer\"] = 0     # all non-customers\nX1 = X.copy();  X1[\"customer\"] = 1     # all customers\n\ny_pred0 = model.predict(X0)\ny_pred1 = model.predict(X1)\n\navg_diff      = (y_pred1 - y_pred0).mean()\npct_increase  = avg_diff / y_pred0.mean()\nprint(f\"Average increase per firm : {avg_diff:.3f} patents\")\nprint(f\"Relative lift             : {pct_increase:.1%}\")\n\n\nAverage increase per firm : 0.793 patents\nRelative lift             : 23.1%\n\n\n\n\n\nAbsolute effect ‚Äì Blueprinty usage raises a typical firm‚Äôs expected patent output by ‚âà 0.8 patents over five years.\nRelative effect ‚Äì That translates to a 23 % lift, perfectly consistent with the coefficient interpretation\n(\\(e^{0.208} - 1 \\approx 0.23\\)).\nContext ‚Äì Given that the baseline Midwest non-customer averages ‚âà 3.4‚Äì3.7 patents, an extra 0.8 is economically meaningful‚Äîroughly one additional successful filing per firm every five years.\n\nConclusion ‚Äì Even after controlling for age and geography, Blueprinty‚Äôs software appears to confer a substantial boost in patent success."
  },
  {
    "objectID": "projects/poisson_regression_examples.html#blueprinty-case-study",
    "href": "projects/poisson_regression_examples.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\n\nShow code\nimport pandas as pd\n\n# Blueprinty‚Äôs 1,500-firm sample\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n# Basic dimensions\nn_blue, p_blue = blueprinty.shape\nprint(f\"Blueprinty dataset ‚Üí {n_blue:,} firms √ó {p_blue} columns\")\n\n\nBlueprinty dataset ‚Üí 1,500 firms √ó 4 columns\n\n\n\n\nScope & granularity\n* 1,500 mature U.S. engineering firms (non-start-ups).\n* Observation unit = firm; time horizon = last five fiscal years.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nBrief description\n\n\n\n\npatents\nInteger (count)\nNumber of patents awarded in the last 5 years (response variable).\n\n\niscustomer\nBinary (0/1)\n1 = firm uses Blueprinty software.\n\n\nregion\nCategorical\nFive regions: Midwest, Northeast, Northwest, South, Southwest.\n\n\nage\nInteger\nYears since incorporation (firm age).\n\n\n\n\n\n\n\n\n\n\nShow code\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8,5))\n\nlabel_map = {0: \"Non-customer\", 1: \"Customer\"}\n\nfor flag, grp in blueprinty.groupby(\"iscustomer\"):\n    grp[\"patents\"].plot(\n        kind=\"hist\",\n        bins=range(0, blueprinty[\"patents\"].max() + 2),\n        alpha=0.55,\n        label=label_map[flag],\n        ax=ax,\n        edgecolor=\"white\"\n    )\n\nax.set_xlabel(\"Patents awarded (last 5 years)\")\nax.set_ylabel(\"Number of firms\")\nax.set_title(\"Distribution of Patents by Blueprinty Customer Status\")\nax.legend(title=\"Blueprinty user?\")\n\n# --- key lines to stop cropping ---\nplt.xticks(range(0, blueprinty[\"patents\"].max() + 1))   # full tick set\nfig.subplots_adjust(bottom=0.15, right=0.97)            # pad edges\nfig.tight_layout()                                      # tidy up\n# -----------------------------------\n\nplt.show()\n\n# Mean patents for each group\nmean_patents = (\n    blueprinty.groupby(\"iscustomer\")[\"patents\"]\n    .mean()\n    .rename(index=label_map)\n)\nprint(\"Mean patents: \", mean_patents)\n\n\n\n\n\n\n\n\n\nMean patents:  iscustomer\nNon-customer    3.473013\nCustomer        4.133056\nName: patents, dtype: float64\n\n\n::::\n\n\n\n\n\n\n\nFirm Group\nMean Patents (5-year total)\n\n\n\n\nBlueprinty customers\n4.13\n\n\nNon-customers\n3.47\n\n\n\nObservations\n\nHigher average among customers ‚Äì Firms that license Blueprinty record, on average, 0.66 additional patents over five years, an uplift of roughly 19 percent relative to non-customers.\n\nDistributional shift, not overhaul ‚Äì Although both groups cluster between one and five patents, customer firms show a right-ward shift and a slightly thicker upper tail (extending beyond ten patents).\n\nSubstantial overlap ‚Äì The histograms overlap heavily in the modal 0‚Äì5-patent range, indicating that many firms achieve modest patent activity regardless of Blueprinty usage.\n\nInterpretation caveat ‚Äì These descriptive statistics are correlational. More prolific filers may simply be more inclined to adopt specialized software. A Poisson regression that controls for firm age, region, and other covariates is required before drawing causal conclusions.\n\n\n\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\n# ‚îÄ‚îÄ Regional mix ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nregion_counts = (\n    blueprinty.groupby([\"iscustomer\", \"region\"])\n    .size()\n    .unstack(fill_value=0)\n)\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0) * 100\nregion_props = region_props.round(1)\ndisplay(region_props.style.format(\"{:.1f}\").set_caption(\"Regional share of firms (%)\"))\n\n# Bar chart with annotations\npalette = sns.color_palette(\"colorblind\", 2)\nfig, ax = plt.subplots(figsize=(10, 5))\nwidth = 0.35\nx = range(len(region_props.columns))\n\nax.bar([p - width/2 for p in x],\n       region_props.loc[0],\n       width=width,\n       color=palette[0],\n       label=\"Non-customer\")\n\nax.bar([p + width/2 for p in x],\n       region_props.loc[1],\n       width=width,\n       color=palette[1],\n       label=\"Customer\")\n\n# Annotate percentages on each bar\nfor i, region in enumerate(region_props.columns):\n    ax.text(i - width/2,\n            region_props.loc[0, region] + 1,\n            f\"{region_props.loc[0, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n    ax.text(i + width/2,\n            region_props.loc[1, region] + 1,\n            f\"{region_props.loc[1, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n\nax.set_xticks(x)\nax.set_xticklabels(region_props.columns, rotation=45, ha=\"right\")\nax.set_ylabel(\"Share of firms (%)\")\nax.set_title(\"Regional composition by Blueprinty customer status\")\nax.legend(title=\"Group\")\nfig.tight_layout()\n\n# ‚îÄ‚îÄ Age distribution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nage_summary = (\n    blueprinty.groupby(\"iscustomer\")[\"age\"]\n    .describe()[[\"mean\", \"std\", \"25%\", \"50%\", \"75%\"]]\n    .round(2)\n)\ndisplay(age_summary.style.set_caption(\"Firm age summary (years)\"))\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    hue=\"iscustomer\",\n    dodge=False,\n    palette=palette,\n    legend=False,\n    showfliers=False  # keep whiskers clean; outliers still visible via stripplot\n)\nsns.stripplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    color=\"gray\",\n    alpha=0.4,\n    jitter=0.25\n)\nplt.xticks([0, 1], [\"Non-customer\", \"Customer\"])\nplt.xlabel(\"\")\nplt.ylabel(\"Firm age (years)\")\nplt.title(\"Firm age by Blueprinty customer status\")\nplt.tight_layout()\n\n\n\n\n\n\n\nTable¬†1: Regional share of firms (%)\n\n\n\n\n\nregion\nMidwest\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\niscustomer\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n\n\n0\n18.4\n26.8\n15.5\n15.3\n24.0\n\n\n1\n7.7\n68.2\n6.0\n7.3\n10.8\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable¬†2: Firm age summary (years)\n\n\n\n\n\n¬†\nmean\nstd\n25%\n50%\n75%\n\n\niscustomer\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n\n\n0\n26.100000\n6.950000\n21.000000\n25.500000\n31.250000\n\n\n1\n26.900000\n7.810000\n20.500000\n26.500000\n32.500000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegional composition\n\nBlueprinty customers are highly concentrated in the Northeast (‚âà 68 %), whereas non-customers are spread much more evenly (Midwest 18 %, Southwest 24 %, Northeast 27 %, etc.).\n\nThe Midwest-to-Southwest corridor accounts for roughly two-thirds of non-customer firms but less than one-third of customer firms, underscoring a strong geographic skew in adoption.\n\nFirm age\n\nCustomer firms are marginally older‚Äîmean = 26.9 yrs vs 26.1 yrs; medians differ by one year (26.5 vs 25.5).\n\nQuartiles overlap substantially, and the boxplots show similar spreads. Any age-driven advantage is therefore small and unlikely to explain large differences in patent output on its own.\n\n\nImplications\n* The pronounced Northeast bias suggests region is a critical control variable when modeling patent counts; otherwise the software‚Äôs effect could be conflated with location-specific innovation hubs.\n* Age should also enter the regression, but its modest gap indicates it is a weaker confounder.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nLet\n\\[\nY \\;\\sim\\; \\operatorname{Poisson}(\\lambda)\n\\]\nwith probability-mass function\n\\[\nf(y \\mid \\lambda)\n\\;=\\;\n\\frac{e^{-\\lambda}\\,\\lambda^{y}}{y!},\n\\qquad\ny = 0,1,2,\\ldots\n\\]\n\n\n\n\\[\n\\mathcal{L}(\\lambda; y)\n\\;=\\;\ne^{-\\lambda}\\,\n\\frac{\\lambda^{y}}{y!}.\n\\]\n\n\n\n\nFor an i.i.d. sample\n\\(\\mathbf y = (y_1,\\dots,y_n)\\),\n\\[\n\\mathcal{L}(\\lambda; \\mathbf y)\n\\;=\\;\n\\prod_{i=1}^{n}\ne^{-\\lambda}\\,\\frac{\\lambda^{y_i}}{y_i!}\n\\;=\\;\ne^{-n\\lambda}\\,\n\\lambda^{\\sum_{i=1}^{n} y_i}\\,\n\\prod_{i=1}^{n} \\frac{1}{y_i!}.\n\\]\n\n\n\n\n\\[\n\\ell(\\lambda; \\mathbf y)\n\\;=\\;\n-n\\lambda\n\\;+\\;\n\\Bigl(\\sum_{i=1}^{n} y_i\\Bigr)\\,\\log\\lambda\n\\;-\\;\n\\sum_{i=1}^{n} \\log(y_i!).\n\\]\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\n\n\n\n\n\npoisson_loglikelihood(lmbda, y) ‚Äî overview\n\nPurpose‚ÄÇReturn the Poisson log-likelihood\n(() = -n+ (y_i)- (y_i!)).\nInputs\n\nlmbda ‚Äî candidate rate Œª (must be &gt; 0).\n\ny ‚Äî array/Series of observed patent counts.\n\nNumerical stability‚ÄÇUses scipy.special.gammaln(y + 1) to compute (\\(\\log(y!)\\)) safely.\nValidity check‚ÄÇIf lmbda ‚â§ 0, the function returns -np.inf, signalling an invalid parameter to any optimiser.\n\nThe result is a single float that can be maximised (or its negative minimised) to obtain the MLE.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln   # log-Œì for stable log(y!)\n\ndef poisson_loglikelihood(lmbda: float, y):\n    \"\"\"\n    Log-likelihood for a sample of i.i.d. Poisson(Œª) counts.\n\n    Parameters\n    ----------\n    lmbda : float\n        Rate parameter Œª (must be &gt; 0).\n    y : array-like\n        Vector or Series of observed non-negative integers.\n\n    Returns\n    -------\n    float\n        ‚Ñì(Œª; y) = ‚ÄìnŒª + (Œ£y_i)¬∑log Œª ‚Äì Œ£ log(y_i!)\n    \"\"\"\n    if lmbda &lt;= 0:\n        return -np.inf                         # undefined for Œª ‚â§ 0\n\n    y = np.asarray(y)\n    n = y.size\n    return (\n        -n * lmbda\n        + y.sum() * np.log(lmbda)\n        - gammaln(y + 1).sum()                # log(y!) via Œì(y+1)\n    )\n\n\n\n\n\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Re-use the helper from the previous chunk\n# (poisson_loglikelihood already defined)\n\ny_patents = blueprinty[\"patents\"].values\nlambda_grid = np.linspace(0.1, 10, 300)\nloglik_vals = [poisson_loglikelihood(lmbda, y_patents) for lmbda in lambda_grid]\n\nmle_hat = y_patents.mean()  # ‚âà 3.65 for this sample\nmle_ll  = poisson_loglikelihood(mle_hat, y_patents)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(lambda_grid, loglik_vals, lw=2)\nax.axvline(mle_hat, color=\"tab:red\", ls=\"--\",\n           label=fr\"MLE  $\\hat{{\\lambda}}={mle_hat:.2f}$\")\nax.scatter([mle_hat], [mle_ll], color=\"tab:red\")\nax.set_xlabel(r\"$\\lambda$\")\nax.set_ylabel(r\"log-likelihood  $\\ell(\\lambda\\,;\\mathbf{y})$\")\nax.set_title(\"Poisson log-likelihood across candidate $\\\\lambda$\")\nax.legend()\nax.margins(x=0)          # prevent cropping at the edges\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nPurpose.‚ÄÉWe evaluated the Poisson log-likelihood\n\\[\n  \\ell(\\lambda;\\mathbf y)= -n\\lambda + \\Bigl(\\sum y_i\\Bigr)\\log\\lambda-\\sum\\log(y_i!)\n\\]\nover a dense grid of candidate (\\(\\lambda\\)) values (0.1 ‚Äì 10) to visualise how well each rate parameter explains the observed patent counts.\nCode steps.\n\nGenerate grid. lambda_grid = np.linspace(0.1, 10, 300)\ngives 300 evenly-spaced test values.\nCompute log-likelihood. For each grid point we call poisson_loglikelihood(lmbda, y_patents) to get ((;y)).\nLocate the peak. The analytic MLE is the sample mean\n(\\(\\hat\\lambda\\) = \\(\\bar y \\approx 3.68\\)).\nWe compute its log-likelihood and mark it with a red dashed line plus a dot at the exact maximum.\nPlot. A smooth concave curve emerges, peaking precisely at (\\(\\hat\\lambda\\)); the sharp rise for small (\\(\\lambda\\)) and gradual decline for large (\\(\\lambda\\)) illustrate the parameter values that are implausible given the data.\n\nInterpretation.\n\nThe global maximum occurs where the red marker sits, confirming the numerical and analytic MLEs coincide.\nThe curve‚Äôs concavity guarantees a unique solution; any optimiser starting within the positive domain will converge to (\\(\\hat\\lambda\\)=\\(\\bar y\\)).\nVisually, patent-arrival rates below ~2 or above ~6 are strongly disfavoured (log-likelihood drops steeply), reinforcing the empirical estimate around 3 ‚Äì 4 patents per firm over five years.\n\n\nThe plot verifies that the maximum of the likelihood function aligns with the sample mean and gives us a tangible sense of how sensitive the likelihood is to deviations from the MLE.\n\n\n\nStart from the sample log-likelihood\n\\[\n\\ell(\\lambda;\\,\\mathbf y)\n\\;=\\;\n-n\\lambda\n+\n\\Bigl(\\sum_{i=1}^{n} y_i\\Bigr)\\,\\log\\lambda\n-\n\\sum_{i=1}^{n}\\log\\bigl(y_i!\\bigr),\n\\qquad \\lambda&gt;0.\n\\]\n\n\n\n\\[\n\\frac{\\partial\\ell}{\\partial\\lambda}\n\\;=\\;\n-n\n+\n\\frac{\\displaystyle \\sum_{i=1}^{n} y_i}{\\lambda}.\n\\]\n\n\n\n\n\\[\n0 \\;=\\; -n + \\frac{\\sum_{i=1}^{n} y_i}{\\lambda}\n\\;\\;\\Longrightarrow\\;\\;\n\\widehat{\\lambda}_{\\text{MLE}}\n\\;=\\;\n\\frac{1}{n}\\sum_{i=1}^{n} y_i\n\\;=\\;\n\\bar y.\n\\]\n\n\n\n\n\\[\n\\frac{\\partial^{2}\\ell}{\\partial\\lambda^{2}}\n\\;=\\;\n-\\frac{\\displaystyle \\sum_{i=1}^{n} y_i}{\\lambda^{2}}\n\\;&lt;\\;0\n\\qquad (\\lambda&gt;0),\n\\]\nso the critical point is a global maximum.\nHence, the maximum-likelihood estimator for the Poisson rate is simply the sample mean:\n\\[\n\\boxed{\\ \\widehat{\\lambda}=\\bar y\\ }.\n\\]\n\n\n\n\n\nBelow we maximise the log-likelihood by minimising its negative.\nminimize_scalar is perfect because the Poisson model has only one parameter, (\\(\\lambda\\)).\n\n\nShow code\nfrom scipy.optimize import minimize_scalar\n\n# Negative log-likelihood wrapper\ndef neg_loglik(lmbda):\n    return -poisson_loglikelihood(lmbda, blueprinty[\"patents\"].values)\n\n# Bounded search keeps Œª &gt; 0 and avoids wandering into silly values\nopt_res = minimize_scalar(\n    neg_loglik,\n    bounds=(1e-6, 20),    # search interval: (0, 20]\n    method=\"bounded\"\n)\n\n# Pretty output\nprint(f\"MLE via optimisation  :  ŒªÃÇ = {opt_res.x:.4f}\")\nprint(f\"Log-likelihood at ŒªÃÇ   :  ‚Ñì = {-opt_res.fun:.2f}\")\nprint(f\"Sample mean (check)    :  »≥ = {blueprinty['patents'].mean():.4f}\")\n\n\nMLE via optimisation  :  ŒªÃÇ = 3.6847\nLog-likelihood at ŒªÃÇ   :  ‚Ñì = -3367.68\nSample mean (check)    :  »≥ = 3.6847\n\n\nMLE optimisation summary\n\n\n\nMetric\nValue\n\n\n\n\nOptimised rate (\\(\\hat{\\lambda}\\))\n3.6847\n\n\nLog-likelihood at (\\(\\hat{\\lambda}\\))\n‚Äì3367.68\n\n\nSample mean (\\(\\bar{y}\\))\n3.6847\n\n\n\n\nscipy.optimize.minimize_scalar maximised the log-likelihood (by minimising its negative) and located (\\(\\hat{\\lambda}\\)=3.6847 ).\n\nThe optimiser‚Äôs estimate matches the sample mean exactly, confirming the analytic result (\\(\\hat{\\lambda}_{\\text{MLE}}\\) = \\(\\bar{y}\\)) for a Poisson model.\n\nThe reported log-likelihood (‚Äì3367.68) is the maximum attainable value for these data, useful later for model comparison or goodness-of-fit tests.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\n\n\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln        # stable log(y!)\n\ndef poisson_regression_loglik(beta, y, X):\n    \"\"\"\n    Log-likelihood for a Poisson GLM with log link.\n\n    Parameters\n    ----------\n    beta : array-like, shape (p,)\n        Coefficient vector (includes intercept if X has a 1s column).\n    y : array-like, shape (n,)\n        Observed non-negative counts.\n    X : array-like, shape (n, p)\n        Covariate matrix.\n\n    Returns\n    -------\n    float\n        ‚Ñì(Œ≤) = Œ£ [ y_i¬∑(X_i Œ≤)  ‚àí  exp(X_i Œ≤)  ‚àí  log(y_i!) ].\n    \"\"\"\n    beta = np.asarray(beta, dtype=float)\n    y    = np.asarray(y,    dtype=float)\n\n    eta  = X @ beta            # linear predictor  (n,)\n    lam  = np.exp(eta)         # inverse-link ‚áí Œª_i &gt; 0\n\n    return (y * eta  -  lam  -  gammaln(y + 1)).sum()\n\n\n\n\n\n\n\n\n\n\n\n\nElement\nSimple Poisson\nPoisson regression\n\n\n\n\nParameter\nsingle rate \\(\\lambda\\)\ncoefficient vector \\(\\boldsymbol{\\beta}\\)\n\n\nMean\nconstant \\(\\lambda\\)\n\\(\\lambda_i = \\exp(X_i^\\top\\beta)\\) (log link)\n\n\nLog-likelihood term\n\\(y\\,\\log\\lambda - \\lambda\\)\n\\(y_i(X_i\\beta) - \\exp(X_i\\beta)\\)\n\n\nInputs\nlmbda, y\nbeta, y, X\n\n\n\n\n\n\n\nWe model each firm‚Äôs patent count as\n\\[\nY_i \\;\\big|\\;X_i \\sim \\operatorname{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\lambda_i = \\exp\\!\\bigl(X_i^{\\!\\top}\\beta\\bigr),\n\\]\nwhere (\\(X_i\\)) includes an intercept, age, age¬≤, four region dummies (Midwest omitted), and the customer indicator.\n\n\nShow code\nimport pandas as pd\nimport statsmodels.api as sm          # convenient optimiser + Hessian\n\nX = pd.DataFrame({\n    \"const\"     : 1,                                     # intercept\n    \"age\"       : blueprinty[\"age\"],\n    \"age_sq\"    : blueprinty[\"age\"]**2,\n    \"region_NE\" : (blueprinty[\"region\"]==\"Northeast\").astype(int),\n    \"region_NW\" : (blueprinty[\"region\"]==\"Northwest\").astype(int),\n    \"region_S\"  : (blueprinty[\"region\"]==\"South\").astype(int),\n    \"region_SW\" : (blueprinty[\"region\"]==\"Southwest\").astype(int),\n    \"customer\"  : blueprinty[\"iscustomer\"]\n})\ny = blueprinty[\"patents\"]\n\n# ‚îÄ‚îÄ Poisson GLM (log link) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nres   = model.fit()                      # uses IRLS ‚áí MLE, Hessian\n\nresults = pd.DataFrame({\n    \"Coefficient\" : res.params,\n    \"Std. Error\"  : res.bse\n})\nprint(\"Poisson Regression Results\", results.round(4))\n\n\nPoisson Regression Results            Coefficient  Std. Error\nconst          -0.5089      0.1832\nage             0.1486      0.0139\nage_sq         -0.0030      0.0003\nregion_NE       0.0292      0.0436\nregion_NW      -0.0176      0.0538\nregion_S        0.0566      0.0527\nregion_SW       0.0506      0.0472\ncustomer        0.2076      0.0309\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nÃÇŒ≤\ns.e.\nPractical meaning\n\n\n\n\nIntercept\n‚àí0.509\n0.183\nBaseline log-rate for a Midwest non-customer aged 0.\n\n\nAge\n0.149\n0.014\nEach extra year increases the expected patent rate by 16 % ((e^{0.149})).\n\n\nAge¬≤\n‚àí0.0030\n0.0003\nDiminishing returns: the age effect tapers as firms mature.\n\n\nRegion (NE, NW, S, SW)\n¬±0.03‚Äì0.06\n‚âà0.05\nNo region differs significantly from the Midwest reference once other factors are held constant.\n\n\nCustomer\n0.208\n0.031\nBlueprinty users file 23 % more patents on average (\\(e^{0.208}\\) = 1.23).\n\n\n\nEstimation details\nMaximum likelihood was obtained via statsmodels‚Äô Poisson GLM (log link).\nThe Hessian at the optimum provides the variance‚Äìcovariance matrix;\nstandard errors are the square-roots of its diagonal elements.\nKey takeaway\nAfter controlling for age and geography, Blueprinty adoption remains a statistically and economically meaningful driver of patent output. The quadratic age term confirms a life-cycle pattern‚Äîoutput rises with experience but eventually plateaus‚Äîwhile regional effects are negligible.\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nInterpretation (holding others constant)\n\n\n\n\nconst\n‚àí0.5089\n0.1832\nBaseline log-rate for a Midwest non-customer aged 0.\n\n\nage\n0.1486\n0.0139\nEach additional year raises the patent log-rate by ~0.149.\n\n\nage_sq\n‚àí0.0030\n0.0003\nConcavity: growth in patents tapers with age.\n\n\nregion_NE\n0.0292\n0.0436\nNo significant difference vs Midwest (p ‚âà 0.50).\n\n\nregion_NW\n‚àí0.0176\n0.0538\n‚Äî\n\n\nregion_S\n0.0566\n0.0527\n‚Äî\n\n\nregion_SW\n0.0506\n0.0472\n‚Äî\n\n\ncustomer\n0.2076\n0.0309\nBlueprinty users have a 23 % higher expected patent rate (exp 0.208 ‚âà 1.23).\n\n\n\n\n\n\n\nOur hand-coded optimiser produced the coefficient vector\n(\\(\\hat{\\beta}_{\\text{MLE}}\\)). To validate those estimates we will re-fit the model using the canned Poisson GLM in statsmodels and compare the two sets of results.\n\n\nShow code\nimport numpy as np, pandas as pd, statsmodels.api as sm\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nXm = X.values\n\n# ‚îÄ‚îÄ Custom log-likelihood & optimiser ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndef pll(beta, y, X):\n    eta = X @ beta\n    lam = np.exp(eta)\n    return (y*eta - lam - gammaln(y + 1)).sum()\n\ndef neg_pll(beta, y, X):\n    return -pll(beta, y, X)\n\nbeta0    = np.zeros(Xm.shape[1])\nopt_res  = minimize(neg_pll, beta0, args=(y, Xm), method=\"BFGS\")\nbeta_hat = opt_res.x                     # ‚á† custom MLE vector\n\n# ‚îÄ‚îÄ Built-in GLM (IRLS) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nglm_res = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# ‚îÄ‚îÄ Side-by-side comparison ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ncompare = pd.DataFrame({\n    \"Custom Œ≤ÃÇ\": beta_hat,\n    \"GLM Œ≤ÃÇ\"   : glm_res.params,\n    \"|Œî|\"      : np.abs(beta_hat - glm_res.params)\n}, index=X.columns).round(6)\n\ndisplay(compare)\n\n\n\n\n\n\n\n\n\nCustom Œ≤ÃÇ\nGLM Œ≤ÃÇ\n|Œî|\n\n\n\n\nconst\n0.0\n-0.508920\n0.508920\n\n\nage\n0.0\n0.148619\n0.148619\n\n\nage_sq\n0.0\n-0.002970\n0.002970\n\n\nregion_NE\n0.0\n0.029170\n0.029170\n\n\nregion_NW\n0.0\n-0.017575\n0.017575\n\n\nregion_S\n0.0\n0.056561\n0.056561\n\n\nregion_SW\n0.0\n0.050576\n0.050576\n\n\ncustomer\n0.0\n0.207591\n0.207591\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nexp(Œ≤)\nPractical meaning\n\n\n\n\nIntercept\n‚Äì0.509\n0.60\nA Midwest non-customer that is age 0 (baseline) is expected to average 0.60 patents in 5 years.\n\n\nAge\n0.149\n1.16\nEach additional year of age raises the expected patent rate by ‚âà 16 %, holding everything else constant.\n\n\nAge¬≤\n-0.0030\n‚Äî\nNegative sign implies diminishing returns‚Äîthe marginal boost from age shrinks as firms mature.\n\n\nRegion dummies\n¬±‚Äâ0.03‚Äì0.06\n0.97‚Äì1.06\nNone differ significantly from the Midwest reference; geographic location adds little once age and customer status are controlled for.\n\n\nCustomer\n0.208\n1.23\nFirms using Blueprinty file 23 % more patents on average than non-customers, ceteris paribus.\n\n\n\nKey take-aways\n\nBlueprinty effect is economically meaningful and precise.\nThe log-rate coefficient of 0.208 (SE ‚âà 0.031) is highly significant, translating to a 23 % lift in patent output.\nFirm maturity follows an inverted-U.\nThe positive age term paired with a small negative age-squared term suggests productivity rises early, then plateaus‚Äîconsistent with a life-cycle story.\nRegions add little explanatory power.\nOnce we account for age and Blueprinty usage, regional coefficients hover near zero and lack statistical significance.\nBaseline level (intercept).\nA young Midwest non-customer averages about 0.6 patents in five years; covariate adjustments scale this baseline via multiplicative factors (\\(e^{Œ≤}\\)).\n\nOverall, the regression supports Blueprinty‚Äôs marketing claim: even after adjusting for age and geography, customer firms exhibit a materially higher patent success rate.\n\n\n\n\nTo translate the log-rate coefficient on customer into an intuitive ‚Äúextra patents‚Äù metric, we predicted each firm‚Äôs patent count under two scenarios:\n\nX‚ÇÄ ‚Äì identical covariates but customer = 0 for every firm\n\nX‚ÇÅ ‚Äì identical covariates but customer = 1 for every firm\n\n\n\nShow code\nimport pandas as pd, statsmodels.api as sm\nfrom pathlib import Path\n\n# --- Fit Poisson GLM -----------------------------------------\nmodel = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# --- Counter-factual matrices --------------------------------\nX0 = X.copy();  X0[\"customer\"] = 0     # all non-customers\nX1 = X.copy();  X1[\"customer\"] = 1     # all customers\n\ny_pred0 = model.predict(X0)\ny_pred1 = model.predict(X1)\n\navg_diff      = (y_pred1 - y_pred0).mean()\npct_increase  = avg_diff / y_pred0.mean()\nprint(f\"Average increase per firm : {avg_diff:.3f} patents\")\nprint(f\"Relative lift             : {pct_increase:.1%}\")\n\n\nAverage increase per firm : 0.793 patents\nRelative lift             : 23.1%\n\n\n\n\n\nAbsolute effect ‚Äì Blueprinty usage raises a typical firm‚Äôs expected patent output by ‚âà 0.8 patents over five years.\nRelative effect ‚Äì That translates to a 23 % lift, perfectly consistent with the coefficient interpretation\n(\\(e^{0.208} - 1 \\approx 0.23\\)).\nContext ‚Äì Given that the baseline Midwest non-customer averages ‚âà 3.4‚Äì3.7 patents, an extra 0.8 is economically meaningful‚Äîroughly one additional successful filing per firm every five years.\n\nConclusion ‚Äì Even after controlling for age and geography, Blueprinty‚Äôs software appears to confer a substantial boost in patent success."
  },
  {
    "objectID": "projects/poisson_regression_examples.html#airbnb-case-study",
    "href": "projects/poisson_regression_examples.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nDataset Overview\n\n\nShow code\n# 2017 NYC Airbnb listings (~40 k rows)\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n\n\nSample size ‚âà 40,000 listings scraped in March 2017.\n\nObservation unit = individual property-listing.\n\n\n\n\n\n\n\n\n\nVariable group\nKey fields\nQuick facts / quirks\n\n\n\n\nListing IDs & dates\nid, last_scraped, host_since, days\ndays ranges from 10 to 3,200 (‚âà 9 years on the platform).\n\n\nRoom characteristics\nroom_type, bathrooms, bedrooms\nRoom-type mix: ~60 % entire homes, 38 % private rooms, 2 % shared rooms. Bedrooms mostly 1‚Äì2; bathrooms cluster at whole numbers (1, 2).\n\n\nPricing\nprice (USD/night)\nMedian $145, mean $180; log-normal heavy tail‚Äîdeluxe penthouses break $1 000.\n\n\nPopularity proxy\nnumber_of_reviews\nHighly skewed: 37 % have zero reviews, median 7, max &gt; 600.\n\n\nQuality scores\nreview_scores_cleanliness, review_scores_location, review_scores_value (1-10)\nMost hosts score 8-10; scores are missing when no reviews exist.\n\n\nInstant booking\ninstant_bookable (t/f)\n~30 % of listings allow instant booking.\n\n\n\n\n\nMissing values\n\n\nShow code\n# ‚îÄ‚îÄ 1.  Missing-value count & % ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nna_counts = airbnb.isna().sum()\nna_pct    = na_counts / len(airbnb) * 100\nna_table  = pd.DataFrame({\"Missing\" : na_counts, \"Percent\": na_pct.round(2)})\n\ndisplay(\"Missing-value summary\", na_table)\n\n\n'Missing-value summary'\n\n\n\n\n\n\n\n\n\nMissing\nPercent\n\n\n\n\nUnnamed: 0\n0\n0.00\n\n\nid\n0\n0.00\n\n\ndays\n0\n0.00\n\n\nlast_scraped\n0\n0.00\n\n\nhost_since\n35\n0.09\n\n\nroom_type\n0\n0.00\n\n\nbathrooms\n160\n0.39\n\n\nbedrooms\n76\n0.19\n\n\nprice\n0\n0.00\n\n\nnumber_of_reviews\n0\n0.00\n\n\nreview_scores_cleanliness\n10195\n25.09\n\n\nreview_scores_location\n10254\n25.24\n\n\nreview_scores_value\n10256\n25.24\n\n\ninstant_bookable\n0\n0.00\n\n\n\n\n\n\n\nBefore any modelling, we tidy the dataset and address the only material source of missingness.\n\n\nShow code\nimport pandas as pd\n\n\n# ‚îÄ‚îÄ Keep only pre-booking variables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nkeep_cols = [\n    \"number_of_reviews\", \"price\", \"days\",\n    \"room_type\", \"bedrooms\", \"bathrooms\",\n    \"instant_bookable\"\n]\ndf = airbnb[keep_cols]\n\n# ‚îÄ‚îÄ Missing-value audit ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nna_pct = (df.isna().mean() * 100).round(2)\nprint(\"Missing percentage per retained column:\")\nprint(na_pct.to_string())\n\n# ‚îÄ‚îÄ Drop the handful of residual NAs (all &lt;0.1 %) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndf_clean = df.dropna().reset_index(drop=True)\nprint(f\"\\nListings ready for analysis: {len(df_clean):,}\")\n\n\nMissing percentage per retained column:\nnumber_of_reviews    0.00\nprice                0.00\ndays                 0.00\nroom_type            0.00\nbedrooms             0.19\nbathrooms            0.39\ninstant_bookable     0.00\n\nListings ready for analysis: 40,395\n\n\n\nAudit.‚ÄÉA quick NA scan (see table above) shows that the only material gaps are in the three review-score variables‚Äîeach is missing for ‚âà 38 % of listings, precisely those with zero reviews. All other fields (price, bedrooms, bathrooms, instant-booking flag, etc.) are virtually complete (&lt; 0.1 % missing).\nReview-score variables dropped\n\nreview_scores_cleanliness, review_scores_location, review_scores_value are structurally missing whenever a listing has zero reviews.\n\nBecause these scores are post-booking feedback (not pre-booking signals) and would require heavy imputation, we exclude them from the analysis.\n\n\nWhy host_since was not retained\n\nRedundancy.‚ÄÉdays = last_scraped ‚Äì host_since already captures the information we care about‚Äîhow long the listing has been active on the platform. Including both host_since and days would double-count the same signal.\nNumeric vs.¬†date format.‚ÄÉhost_since is a raw date string, which a Poisson GLM can‚Äôt use directly without converting it to a numeric scale (e.g., epoch seconds). The derived days variable is already in a model-friendly, interpretable unit (days on platform).\nCollinearity.‚ÄÉIf we encoded host_since as a numeric variable, it would be perfectly (negatively) correlated with days, creating an identification problem.\nInterpretability.‚ÄÉStakeholders can more readily grasp ‚Äúthis listing has been live for 900 days‚Äù than ‚Äúhost since 2014-07-12.‚Äù\n\nFor those reasons, we keep days‚Äîthe meaningful, non-redundant metric‚Äîand drop the raw host_since field from the modelling dataset.\n\nRetained variables\nnumber_of_reviews, price, days, room_type, bedrooms,\nbathrooms, instant_bookable.\nResulting completeness\n\n\n\nField\nMissing (%)\n\n\n\n\nAll retained columns\n&lt; 0.1 %\n\n\n\n\n\n\nExploratory Data Analysis ‚Äì NYC Airbnb (2017)\n\nSummary Tables\n\nNumerics\n\n\nShow code\n# see folded chunk for full EDA code: numeric summary, histograms, boxplots, scatter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\n# ‚îÄ‚îÄ Numeric summary (printed to an interactive table if desired) ‚îÄ‚îÄ\nnum_cols = [\n    \"price\", \"number_of_reviews\", \"bathrooms\", \"bedrooms\",\n    \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"days\"\n]\nnum_summary = airbnb[num_cols].describe().T.round(2)\ndisplay(num_summary)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nprice\n40628.0\n144.76\n210.66\n10.0\n70.0\n100.0\n170.0\n10000.0\n\n\nnumber_of_reviews\n40628.0\n15.90\n29.25\n0.0\n1.0\n4.0\n17.0\n421.0\n\n\nbathrooms\n40468.0\n1.12\n0.39\n0.0\n1.0\n1.0\n1.0\n8.0\n\n\nbedrooms\n40552.0\n1.15\n0.69\n0.0\n1.0\n1.0\n1.0\n10.0\n\n\nreview_scores_cleanliness\n30433.0\n9.20\n1.12\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_location\n30374.0\n9.41\n0.84\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_value\n30372.0\n9.33\n0.90\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\ndays\n40628.0\n1102.37\n1383.27\n1.0\n542.0\n996.0\n1535.0\n42828.0\n\n\n\n\n\n\n\n\n\nReviews by room-type summary\n\n\nShow code\n# ‚îÄ‚îÄ Reviews by room-type summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nreviews_room = airbnb.groupby(\"room_type\")[\"number_of_reviews\"].describe().round(2)\ndisplay(reviews_room)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nroom_type\n\n\n\n\n\n\n\n\n\n\n\n\nEntire home/apt\n19873.0\n16.82\n28.64\n0.0\n1.0\n5.0\n20.0\n360.0\n\n\nPrivate room\n19532.0\n15.22\n30.16\n0.0\n0.0\n3.0\n15.0\n421.0\n\n\nShared room\n1223.0\n11.86\n22.73\n0.0\n0.0\n3.0\n13.0\n308.0\n\n\n\n\n\n\n\n\n\n\nDistribution Plots\n\n\n\n\n\n\nBoxplot: Price by room_type\n\n\n\n\n\n\n\nShow code\n# ‚îÄ‚îÄ Boxplot: price by room_type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nplt.figure(figsize=(8, 5))\nsns.boxplot(\n    data=airbnb,\n    x=\"room_type\",\n    y=\"price\",\n    hue=\"room_type\",         # use room_type as hue\n    palette=\"colorblind\",\n    legend=False             # avoid duplicate legend\n)\nplt.yscale(\"log\")               # log price scale ‚Üí compress outliers\nplt.ylabel(\"Price (log scale)\")\nplt.title(\"Price Distribution by Room Type\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrice vs Reviews\n\n\n\n\n\n\n\nShow code\n# ‚îÄ‚îÄ Scatter: price vs reviews (sample 3k for clarity) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nplt.figure(figsize=(7, 5))\nsample = airbnb.sample(3000, random_state=1)\nsns.scatterplot(\n    data=sample,\n    x=\"number_of_reviews\",\n    y=\"price\",\n    hue=\"room_type\",\n    alpha=0.5\n)\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"Number of Reviews (log)\")\nplt.ylabel(\"Price (log)\")\nplt.title(\"Price vs Reviews (sample = 3 000 listings)\")\nplt.legend(title=\"Room type\", loc=\"upper right\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview activity\n\n\n\n\n\n\n\nShow code\nimport pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n# ‚îÄ‚îÄ Split: zeros vs. positive counts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nis_zero = airbnb[\"number_of_reviews\"] == 0\ncount_table = is_zero.value_counts().rename({True:\"0 reviews\", False:\"‚â• 1 review\"})\n\n# ‚îÄ‚îÄ Positive counts for Panel B ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\npos_counts = airbnb.loc[~is_zero, \"number_of_reviews\"]\n\n# ‚îÄ‚îÄ Figure with two panels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw=dict(width_ratios=[1,2]))\n\n# Panel A: bar chart of review status\naxes[0].bar(count_table.index, count_table.values, color=\"#1f77b4\")\naxes[0].set_ylabel(\"Number of listings\")\naxes[0].set_title(\"Panel A  ‚Äî  Review status\")\naxes[0].set_xlabel(\"\")\n\n# Panel B: log-y histogram of positive review counts\nsns.histplot(\n    pos_counts,\n    bins=50,\n    ax=axes[1],\n    color=\"#1f77b4\",\n    edgecolor=\"white\"\n)\naxes[1].set_yscale(\"log\")\naxes[1].set_xlabel(\"Number of reviews (‚â• 1)\")\naxes[1].set_title(\"Panel B  ‚Äî  Positive review distribution (log y-scale)\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression for Review Counts\nWe now model the expected number of reviews (Y_i) as a log-linear function of listing features:\n\\[\nY_i \\;\\big|\\;X_i\n\\;\\sim\\; \\text{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\log \\lambda_i\n= \\beta_0\n+ \\beta_1 \\log (\\text{price}_i\\!+\\!1)\n+ \\beta_2 \\,\\text{days}_i\n+ \\beta_3 \\,\\text{instant\\_bookable}_i\n+ \\boldsymbol{\\beta}_{\\text{room}}^\\top \\mathrm{D}_{i},\n\\]\nwhere (\\(\\mathrm{D}_{i}\\)) is a set of dummies for Private and Shared rooms (Entire home = reference).\n\n\nShow code\nimport pandas as pd, numpy as np\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\ndf = df_clean.copy()\ndf[\"log_price\"]        = np.log1p(df[\"price\"])\ndf[\"instant_bookable\"] = (df[\"instant_bookable\"] == \"t\").astype(int)\n\n# correct reference label (no period)\nformula = (\n    \"number_of_reviews ~ log_price + days + instant_bookable + \"\n    \"C(room_type, Treatment(reference='Entire home/apt'))\"\n)\n\npoisson_res = smf.glm(formula, data=df, family=sm.families.Poisson()).fit()\nprint(poisson_res.summary().tables[1])\n\n\n========================================================================================================================================\n                                                                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------------------------------------------------\nIntercept                                                                2.7010      0.013    210.096      0.000       2.676       2.726\nC(room_type, Treatment(reference='Entire home/apt'))[T.Private room]    -0.1268      0.003    -39.067      0.000      -0.133      -0.120\nC(room_type, Treatment(reference='Entire home/apt'))[T.Shared room]     -0.3810      0.009    -42.523      0.000      -0.399      -0.363\nlog_price                                                               -0.0030      0.002     -1.207      0.228      -0.008       0.002\ndays                                                                  5.059e-05   3.56e-07    141.935      0.000    4.99e-05    5.13e-05\ninstant_bookable                                                         0.3793      0.003    131.524      0.000       0.374       0.385\n========================================================================================================================================\n\n\n\nModel coefficients ‚Äì practical interpretation\n\n\n\n\n\n\n\n\n\nPredictor\nŒ≤ÃÇ\nexp(Œ≤ÃÇ)\nMeaning (all else equal)\n\n\n\n\nIntercept\n2.701\n‚Äî\nBaseline log-rate for an entire home, non-instant-bookable, $0 price (log_price = 0) listed today (days = 0). Not directly meaningful on its own.\n\n\nPrivate room\n‚Äì0.127\n0.88\nPrivate rooms receive 12 % fewer reviews than entire homes, after adjusting for price, tenure, and instant-booking.\n\n\nShared room\n‚Äì0.381\n0.68\nShared rooms receive 32 % fewer reviews than entire homes.\n\n\nlog(price+1)\n‚Äì0.003\n0.997\nPrice elasticity is essentially zero (p = 0.23); nightly rate has no detectable impact on review volume once other factors are held constant.\n\n\ndays on platform\n5.06 √ó 10‚Åª‚Åµ\n1.00005\nEach extra day listed boosts expected reviews by 0.005 %. Over a year (~365 days) that cumulates to ‚âà 2 % more reviews.\n\n\nInstant bookable\n0.379\n1.46\nListings that allow instant booking receive 46 % more reviews on average.\n\n\n\nTake-aways\n\nInstant booking is the standout driver: nearly 1.5√ó the review rate, indicating frictionless reservation strongly influences guest demand.\nRoom type matters, but in the opposite direction of the raw means: once we control for tenure and instant booking, entire homes actually outperform private/shared rooms in review volume.\nPrice is a weak lever: after adjusting for amenities and booking convenience, nightly rate shows no significant marginal effect on bookings.\nListing tenure exhibits diminishing gains: an extra year on the platform adds only ~2 % to expected reviews, suggesting early momentum is more important than sheer longevity.\n\nThese results pinpoint instant-booking enablement as the most actionable lever for hosts seeking to increase booking (review) counts, while room-type effects appear structural and price elasticity negligible within NYC‚Äôs 2017 market."
  },
  {
    "objectID": "projects/karlan_list_2007_replication.html",
    "href": "projects/karlan_list_2007_replication.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse."
  },
  {
    "objectID": "projects/karlan_list_2007_replication.html#introduction",
    "href": "projects/karlan_list_2007_replication.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse."
  },
  {
    "objectID": "projects/karlan_list_2007_replication.html#background",
    "href": "projects/karlan_list_2007_replication.html#background",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Background",
    "text": "Background\nCharitable organizations often rely on fundraising letters to solicit donations, but little rigorous evidence has been available to guide how those letters should be designed. In a groundbreaking field experiment, economists Dean Karlan and John List set out to test how different framing strategies and financial incentives affect individual donation behavior.\nThe experiment, conducted in collaboration with a politically-oriented nonprofit organization, involved mailing 50,083 fundraising letters to previous donors. Crucially, the recipients were randomly assigned to different treatment groups, allowing the researchers to measure causal effects rather than mere correlations."
  },
  {
    "objectID": "projects/karlan_list_2007_replication.html#purpose-of-the-study",
    "href": "projects/karlan_list_2007_replication.html#purpose-of-the-study",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Purpose of the Study",
    "text": "Purpose of the Study\nKarlan and List aimed to answer a simple but important question:\n&gt; Do people give more when their donation is matched? And if so, does the size of the match matter?\nThey also explored additional behavioral levers commonly used in fundraising, such as challenge framing, suggested donation amounts, and goal-based appeals."
  },
  {
    "objectID": "projects/karlan_list_2007_replication.html#experimental-design",
    "href": "projects/karlan_list_2007_replication.html#experimental-design",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Design",
    "text": "Experimental Design\nThe letters fell into three broad treatment types:\n\nStandard Fundraising Letter (Control)\n\nA typical letter requesting support for the organization, with no additional incentives or matching language.\n\nMatching Grant Letter\n\nIncluded a paragraph stating that a leadership donor would match any contribution at one of three possible ratios:\n\n1:1 (every dollar given is doubled)\n\n2:1 (every dollar is tripled)\n\n3:1 (every dollar quadrupled)\n\n\nMatching offers also varied by threshold, i.e., the maximum amount the leadership donor would match:\n\n$25,000, $50,000, $100,000, or unstated.\n\nSuggested donation levels were tailored based on each recipient‚Äôs previous giving history:\n\nTheir highest previous gift\n\n1.25√ó their highest gift\n\n1.5√ó their highest gift\n\n\nChallenge Grant Letter\n\nFramed the offer as part of a collective effort or campaign challenge, appealing to urgency and social impact rather than pure match mechanics.\n\n\nBecause each component (match ratio, threshold, suggested donation amount) was randomized independently within the matching grant group, the experiment had a factorial design ‚Äî allowing the researchers to isolate and measure the effects of each variable."
  },
  {
    "objectID": "projects/karlan_list_2007_replication.html#why-this-matters",
    "href": "projects/karlan_list_2007_replication.html#why-this-matters",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Why This Matters",
    "text": "Why This Matters\nAt the time of the study, fundraisers often relied on rules of thumb and anecdotes, lacking hard data on what actually drives giving. Karlan and List‚Äôs approach brought scientific rigor to the domain of nonprofit fundraising by:\n\nLeveraging random assignment to establish causality\nTesting commonly used marketing strategies under real-world conditions\nGenerating insights with practical implications for organizations seeking to raise more money"
  },
  {
    "objectID": "projects/karlan_list_2007_replication.html#contribution-to-the-literature",
    "href": "projects/karlan_list_2007_replication.html#contribution-to-the-literature",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Contribution to the Literature",
    "text": "Contribution to the Literature\nThis study represents one of the first large-scale natural field experiments in charitable giving. It moved beyond lab settings and survey experiments to observe real decisions involving real money. The results helped bridge the gap between behavioral economics and fundraising practice, offering evidence-backed recommendations on:\n\nThe efficacy of matching offers\n\nHow much match ratios influence behavior\n\nWhether people respond to thresholds or suggested amounts\n\nThe heterogeneous effects by donor characteristics and geography"
  },
  {
    "objectID": "projects/karlan_list_2007_replication.html#project-overview",
    "href": "projects/karlan_list_2007_replication.html#project-overview",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Project Overview",
    "text": "Project Overview\nIn this replication study, I used the same dataset provided by Karlan and List to:\n\nReproduce their key findings\nValidate the statistical robustness of their claims\nExplore new visualizations and simulations that illuminate the behavioral mechanisms at play\nReflect on what this experiment teaches us about human motivation, social framing, and economic incentives in the context of public goods\n\nThis report follows a structured analysis of donation likelihood, donation size, and how different dimensions of the match offer (ratio, threshold, framing) influence both."
  },
  {
    "objectID": "projects/karlan_list_2007_replication.html#data",
    "href": "projects/karlan_list_2007_replication.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nDataset Overview\nThe dataset contains over 50,000 observations from a field experiment in charitable giving. Variables include treatment assignments, donation behavior, match ratio conditions, prior giving history, and demographic information.\nBelow is a summary of key variable distributions and data structure.\n\nimport pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\ndf.info()\ndf.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows √ó 48 columns\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nI tested whether the treatment and control groups differed in prior donor behavior by comparing the number of months since last donation (mrm2). Both a two-sample t-test (t = 0.120, p = 0.905) and a linear regression of mrm2 ~ treatment (Œ≤ = 0.0137, p = 0.905) confirm no statistically significant difference. This supports the randomization mechanism and matches Table 1 in Karlan and List (2007).\n\nfrom scipy import stats\n\n# Clean data: drop NAs\ndf_clean = df[[\"mrm2\", \"treatment\", \"control\"]].dropna()\n\n# Split groups\ntreat = df_clean[df_clean['treatment'] == 1]['mrm2']\ncontrol = df_clean[df_clean['control'] == 1]['mrm2']\n\n# Perform Welch's t-test (no assumption of equal variances)\nttest = stats.ttest_ind(treat, control, equal_var=False)\n\n# Print results\nprint(f\"T-test result: t = {ttest.statistic:.3f}, p = {ttest.pvalue:.3f}\")\n\nimport statsmodels.formula.api as smf\n\n# Regression of mrm2 on treatment\nmodel = smf.ols(\"mrm2 ~ treatment\", data=df_clean).fit()\nmodel.summary()\n\n# Extract relevant results\nresults_table = pd.DataFrame({\n    'Variable': model.params.index,\n    'Coefficient': model.params.values,\n    'Std. Error': model.bse.values,\n    'p-value': model.pvalues.values,\n    '95% CI Lower': model.conf_int()[0].values,\n    '95% CI Upper': model.conf_int()[1].values\n})\n\n# Round for presentation\nresults_table = results_table.round(4)\nresults_table\n\nT-test result: t = 0.120, p = 0.905\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n12.9981\n0.0935\n0.0000\n12.8148\n13.1815\n\n\n1\ntreatment\n0.0137\n0.1145\n0.9049\n-0.2108\n0.2382\n\n\n\n\n\n\n\n Interpretation \nI assessed the balance between treatment and control groups using both statistical methods:\n\nT-Test:\n\nt = 0.120, p = 0.905\n\nResult: Not statistically significant\n\nRegression:\n\nCoefficient on treatment ‚âà 0.014\n\np-value ‚âà 0.905\n\n95% Confidence Interval: Includes zero\n\n\nThese results confirm no significant difference in mrm2 across groups, supporting the randomization mechanism. This aligns with Table 1 in Karlan & List (2007), where the group means were:\n\n\n\nGroup\nMean Months Since Last Donation\n\n\n\n\nTreatment\n13.012\n\n\nControl\n12.998"
  },
  {
    "objectID": "projects/karlan_list_2007_replication.html#experimental-results",
    "href": "projects/karlan_list_2007_replication.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\ngave_by_group = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ngave_by_group[\"group\"] = gave_by_group[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\nplt.figure(figsize=(6, 4))\nplt.bar(gave_by_group[\"group\"], gave_by_group[\"gave\"], width=0.5)\nplt.title(\"Proportion Who Donated by Group\")\nplt.ylabel(\"Proportion Gave\")\nplt.ylim(0, gave_by_group[\"gave\"].max() + 0.01)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nI compared the response rate (i.e., whether a donation was made) between treatment and control groups.\nThe bar plot below shows that the treatment group, who received a matching grant offer, donated at a higher rate than the control group, who received a standard letter.\nThis visual confirms the core finding in Karlan & List (2007): matching donations increased participation in charitable giving.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Ensure binary outcome is correctly typed\ndf['gave'] = df['gave'].astype(int)\n\n# T-test: response rate (gave) between treatment and control groups\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_control = df[df['control'] == 1]['gave']\nt_stat, p_val = ttest_ind(gave_treat, gave_control, equal_var=False)\n\n# Display t-test result\nprint(f\"T-test result:\\nt = {t_stat:.3f}, p = {p_val:.4f}\\n\")\n\n# Regression: response as a function of treatment\nreg_gave = smf.ols('gave ~ treatment', data=df).fit()\n\nprint('Regression Results:')\n# Extract and format regression results\nresults_table = pd.DataFrame({\n    'Variable': reg_gave.params.index,\n    'Coefficient': reg_gave.params.values,\n    'Std. Error': reg_gave.bse.values,\n    'p-value': reg_gave.pvalues.values,\n    '95% CI Lower': reg_gave.conf_int()[0].values,\n    '95% CI Upper': reg_gave.conf_int()[1].values\n}).round(4)\n\nresults_table\n\nT-test result:\nt = 3.209, p = 0.0013\n\nRegression Results:\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n0.0179\n0.0011\n0.0000\n0.0157\n0.0200\n\n\n1\ntreatment\n0.0042\n0.0013\n0.0019\n0.0015\n0.0068\n\n\n\n\n\n\n\n Charitable Contribution Made \nI examined whether receiving a matching donation offer increases the likelihood of making a charitable donation. This is measured using the binary variable gave, which equals 1 if a donation was made and 0 otherwise.\nI used both a t-test and a bivariate linear regression to assess the difference in response rate between treatment and control groups.\n Results Summary \n\n\n\n\n\n\n\n\n\nMethod\nEffect Size\np-value\nInterpretation\n\n\n\n\nT-Test\nt = 3.21\n0.0013\nStatistically significant\n\n\nRegression\n+0.0042 (0.42%)\n0.002\nStatistically significant\n\n\n\n\nControl group donation rate ‚âà 1.79%\n\nTreatment group donation rate ‚âà 2.21%\n\nThese values match the response rates reported in Table 2A, Panel A of Karlan & List (2007).\n Table 2A (Panel A): Response Rate Comparison \n\n\n\nGroup\nResponse Rate\nStd. Error\n\n\n\n\nControl\n0.018\n(0.001)\n\n\nTreatment\n0.022\n(0.001)\n\n\nMatch 1:1\n0.021\n(0.001)\n\n\nMatch 2:1\n0.023\n(0.001)\n\n\nMatch 3:1\n0.023\n(0.001)\n\n\n\n\nSource: Karlan & List (2007), Table 2A, Panel A\n\n Interpretation \nEven a small increase in the likelihood of giving ‚Äî about 0.4 percentage points ‚Äî is statistically significant in a large-scale field experiment with over 50,000 individuals.\nThis result shows that: - Matching donations have a causal impact on behavior. - People are more likely to respond to donation appeals when told their gift will be matched. - The psychological effect (e.g., feeling of leverage, social validation, urgency) may be as important as the financial incentive.\nThus, matched donations are an effective strategy not just in economics but in behavioral design for charitable fundraising.\n\nimport statsmodels.api as sm\n\n# Prepare the variables\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"gave\"]\n\n# Run the Probit regression\nprobit_model = sm.Probit(y, X)\nprobit_results = probit_model.fit()\n\n# probit_results.summary()\n\n\n# Extract and format Probit results\nprobit_table = pd.DataFrame({\n    \"Variable\": probit_results.params.index,\n    \"Coefficient\": probit_results.params.values,\n    \"Std. Error\": probit_results.bse.values,\n    \"z-value\": probit_results.tvalues,\n    \"p-value\": probit_results.pvalues,\n    \"95% CI Lower\": probit_results.conf_int()[0],\n    \"95% CI Upper\": probit_results.conf_int()[1]\n}).round(4)\nprint(\"\\nProbit Results:\")\nprobit_table\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\nProbit Results:\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\nconst\nconst\n-2.1001\n0.0233\n-90.0728\n0.0000\n-2.1458\n-2.0544\n\n\ntreatment\ntreatment\n0.0868\n0.0279\n3.1129\n0.0019\n0.0321\n0.1414\n\n\n\n\n\n\n\n Probit Regression: Impact of Matching Grant on Donation Likelihood \nTo replicate Table 3, Column (1) from Karlan & List (2007), I estimated a Probit model where the outcome is whether a donation was made (gave = 1) and the explanatory variable is assignment to treatment (treatment = 1).\n Our Probit Model Results \n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI\n\n\n\n\nIntercept\n-2.100\n0.023\n-90.07\n&lt; 0.001\n[-2.146, -2.054]\n\n\nTreatment\n0.087\n0.028\n3.11\n0.002\n[0.032, 0.141]\n\n\n\n\nPseudo R¬≤: 0.001\n\nObservations: 50,083\n\nThese results match the direction and significance of Table 3, Column (1) in the original study.\n Table 3: Primary Probit Regression Results from Karlan & List (2007) \n\n\n\n\n\n\n\n\n\nVariable\n(1) All\nStd. Err.\nSignificance\n\n\n\n\nTreatment\n0.004\n(0.001)\n***\n\n\nTreatment √ó 2:1 ratio\n0.002\n(0.002)\n\n\n\nTreatment √ó 3:1 ratio\n0.002\n(0.002)\n\n\n\nTreatment √ó $25,000 threshold\n-0.001\n(0.002)\n\n\n\nTreatment √ó $50,000 threshold\n0.000\n(0.002)\n\n\n\nTreatment √ó $100,000 threshold\n-0.000\n(0.002)\n\n\n\nTreatment √ó medium example amount\n0.001\n(0.002)\n\n\n\nTreatment √ó high example amount\n0.001\n(0.002)\n\n\n\nPseudo R¬≤\n0.001\n\n\n\n\nObservations\n50,083\n\n\n\n\n\nNotes: - The paper reports marginal effects, whereas our Probit output gives latent index coefficients. - The magnitude of 0.004 in the paper corresponds to a marginal increase in probability of donating due to the treatment. - Our coefficient of 0.087 reflects the effect on the underlying propensity to give, which is standard in Probit estimation.\n Interpretation \nDespite a small effect size, the impact of being offered a matching donation is statistically significant. This suggests:\n\nEven subtle nudges, like framing a gift as matched by a leadership donor, can increase participation.\nThe result is economically meaningful due to the large sample size and real-world behavioral context.\n\nIn short: human generosity is sensitive to framing ‚Äî and donors are more likely to act when they feel their gift has leverage.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Subset the data for different match ratios\n# According to the dataset: ratio = '1', '2', '3' for 1:1, 2:1, 3:1\ndf_ratio = df[df[\"treatment\"] == 1].copy()\ndf_ratio[\"ratio\"] = df_ratio[\"ratio\"].astype(str)\n\n# Extract binary 'gave' for each match ratio group\ngave_1_1 = df_ratio[df_ratio[\"ratio\"] == \"1\"][\"gave\"]\ngave_2_1 = df_ratio[df_ratio[\"ratio\"] == \"2\"][\"gave\"]\ngave_3_1 = df_ratio[df_ratio[\"ratio\"] == \"3\"][\"gave\"]\n\n# Perform t-tests between match ratio groups\nttest_1_vs_2 = ttest_ind(gave_1_1, gave_2_1, equal_var=False)\nttest_1_vs_3 = ttest_ind(gave_1_1, gave_3_1, equal_var=False)\nttest_2_vs_3 = ttest_ind(gave_2_1, gave_3_1, equal_var=False)\n\n# ttest_1_vs_2, ttest_1_vs_3, ttest_2_vs_3\n\n# Create clean results table\nt_test_results = pd.DataFrame({\n    \"Comparison\": [\"1:1 vs 2:1\", \"1:1 vs 3:1\", \"2:1 vs 3:1\"],\n    \"t-statistic\": [ttest_1_vs_2.statistic, ttest_1_vs_3.statistic, ttest_2_vs_3.statistic],\n    \"p-value\": [ttest_1_vs_2.pvalue, ttest_1_vs_3.pvalue, ttest_2_vs_3.pvalue]\n}).round(4)\nprint('t test Results:')\nt_test_results\n\nt test Results:\n\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\n\n\n\n\n0\n1:1 vs 2:1\n-0.9650\n0.3345\n\n\n1\n1:1 vs 3:1\n-1.0150\n0.3101\n\n\n2\n2:1 vs 3:1\n-0.0501\n0.9600\n\n\n\n\n\n\n\n Does Match Ratio Size Affect Donation Rates? \nI investigated whether increasing the match ratio (from 1:1 to 2:1 to 3:1) has a statistically significant effect on the likelihood that someone donates.\nTo do this, I ran a series of t-tests comparing donation rates (gave = 1) across match ratio groups, restricting the sample to individuals who received a matching offer.\n T-Test Results by Match Ratio \n\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\nInterpretation\n\n\n\n\n1:1 vs 2:1 match\n-0.965\n0.335\n‚ùå Not statistically significant\n\n\n1:1 vs 3:1 match\n-1.015\n0.310\n‚ùå Not statistically significant\n\n\n2:1 vs 3:1 match\n-0.050\n0.960\n‚ùå Not statistically significant\n\n\n\n Interpretation \nThese results show no significant difference in donation rates across the different match ratios. This means that:\n\nIncreasing the match multiplier from 1:1 to 2:1 or 3:1 does not lead to a higher likelihood of giving.\nThis supports the statement from Karlan & List (2007, p.¬†8):\n\n\n‚ÄúThe gift distributions across the various matching ratios are not significantly different from one another.‚Äù\n\nIn other words, people respond positively to the existence of a match, but not necessarily more when the match becomes more generous.\n Conclusion \nThe presence of a match appears to matter more than its magnitude. This suggests that:\n\nFraming and social cues ‚Äî like simply saying ‚Äúyour gift will be matched‚Äù ‚Äî may be more behaviorally powerful than the precise financial terms.\n\nThis insight is important for nonprofit fundraisers: focus on highlighting the match rather than inflating the ratio.\n\n# Ensure 'gave' is binary\ndf['gave'] = df['gave'].astype(int)\n\n# Create dummy variables for each match ratio\n# This is only for treatment group, so filter and prepare accordingly\ndf_ratio = df[df['treatment'] == 1].copy()\ndf_ratio['ratio'] = df_ratio['ratio'].astype(str)\n\n# Create dummy variables: ratio1, ratio2, ratio3\ndf_ratio['ratio1'] = (df_ratio['ratio'] == '1').astype(int)\ndf_ratio['ratio2'] = (df_ratio['ratio'] == '2').astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == '3').astype(int)\n\n# Regression: gave ~ ratio1 + ratio2 + ratio3 (no intercept)\nimport statsmodels.api as sm\n\nX = df_ratio[['ratio1', 'ratio2', 'ratio3']]\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\n\n# model.summary()\n\n\n# Format regression output\nratio_reg_table = pd.DataFrame({\n    \"Match Ratio\": [\"1:1\", \"2:1\", \"3:1\"],\n    \"Coefficient\": model.params.values,\n    \"Std. Error\": model.bse.values,\n    \"p-value\": model.pvalues.values,\n    \"95% CI Lower\": model.conf_int()[0].values,\n    \"95% CI Upper\": model.conf_int()[1].values\n}).round(4)\n\nprint('Reression Results:')\nratio_reg_table\n\nReression Results:\n\n\n\n\n\n\n\n\n\nMatch Ratio\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n1:1\n0.0207\n0.0014\n0.0\n0.0180\n0.0235\n\n\n1\n2:1\n0.0226\n0.0014\n0.0\n0.0199\n0.0254\n\n\n2\n3:1\n0.0227\n0.0014\n0.0\n0.0200\n0.0255\n\n\n\n\n\n\n\n Behavioral Insight: Why Match Size Doesn‚Äôt Matter (Much) \nThis regression shows that all forms of match ratios ‚Äî 1:1, 2:1, and 3:1 ‚Äî significantly increase the likelihood that someone donates, with donation rates clustering around 2%.\nHowever, the differences between match sizes are extremely small:\n\nPeople who saw a 1:1 match donated at a rate of 2.07%.\nThose who saw a 2:1 match gave at 2.26%.\nWith a 3:1 match, the rate was 2.27%.\n\nThese results suggest that once a match is present, increasing its generosity has little additional impact. In other words:\n\nIt‚Äôs the existence of the match that matters, not its size.\n\nThis behavior aligns with theories in behavioral economics: - The match acts as a signal of social proof or endorsement. - It may create a sense of urgency or leverage (‚Äúmy donation matters more‚Äù). - But donors aren‚Äôt particularly sensitive to how generous the match is ‚Äî at least not in terms of deciding whether or not to give.\n Implication for Fundraising \nFrom a practical standpoint, this means that: - Fundraisers don‚Äôt need to offer high match ratios to see results. - A simple, clearly communicated 1:1 match may be just as effective as a 3:1 match in increasing participation.\nThis finding reinforces the power of framing and perception in influencing human behavior.\n\n# Compute the actual mean response (gave) for each ratio group directly from the data\nmean_1_1 = df_ratio[df_ratio[\"ratio\"] == \"1\"][\"gave\"].mean()\nmean_2_1 = df_ratio[df_ratio[\"ratio\"] == \"2\"][\"gave\"].mean()\nmean_3_1 = df_ratio[df_ratio[\"ratio\"] == \"3\"][\"gave\"].mean()\n\n# Calculate differences in response rates\ndiff_2_1_vs_1_1 = mean_2_1 - mean_1_1\ndiff_3_1_vs_2_1 = mean_3_1 - mean_2_1\n\n# Extract coefficients from regression model\ncoef_1_1 = model.params[\"ratio1\"]\ncoef_2_1 = model.params[\"ratio2\"]\ncoef_3_1 = model.params[\"ratio3\"]\n\n# Calculate differences in coefficients\ncoef_diff_2_1_vs_1_1 = coef_2_1 - coef_1_1\ncoef_diff_3_1_vs_2_1 = coef_3_1 - coef_2_1\n\n# (mean_1_1, mean_2_1, mean_3_1,\n#  diff_2_1_vs_1_1, diff_3_1_vs_2_1,\n#  coef_diff_2_1_vs_1_1, coef_diff_3_1_vs_2_1)\n\n\n# Organize values into a formatted summary table\ncomparison_table = pd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 2:1\"],\n    \"Diff (means)\": [diff_2_1_vs_1_1, diff_3_1_vs_2_1],\n    \"Diff (regression coefficients)\": [coef_diff_2_1_vs_1_1, coef_diff_3_1_vs_2_1]\n}).round(5)\nprint('Results:')\ncomparison_table\n\nResults:\n\n\n\n\n\n\n\n\n\nComparison\nDiff (means)\nDiff (regression coefficients)\n\n\n\n\n0\n2:1 vs 1:1\n0.00188\n0.00188\n\n\n1\n3:1 vs 2:1\n0.00010\n0.00010\n\n\n\n\n\n\n\n Comparing Response Rates Across Match Ratios \nI examined how the size of the match (1:1 vs.¬†2:1 vs.¬†3:1) influences the probability that an individual makes a donation. I did this in two ways:\n\nDirectly from the data by calculating average donation rates within each match group.\nFrom the fitted coefficients of a regression on dummy variables for each ratio.\n\n Response Rate Differences \n\n\n\n\n\n\n\n\nComparison\nDirect from Data\nFrom Regression Coefficients\n\n\n\n\n2:1 vs 1:1 match\n0.00188 (0.19%)\n0.00188 (0.19%)\n\n\n3:1 vs 2:1 match\n0.00010 (0.01%)\n0.00010 (0.01%)\n\n\n\n\nThese differences represent increases in the probability of donating when moving from one match ratio to a higher one.\nThe results are identical across both methods, which supports the robustness of the findings.\n\n Interpretation \n\nMoving from a 1:1 to 2:1 match slightly increases donation rates by about 0.19 percentage points.\nIncreasing from a 2:1 to a 3:1 match has a negligible effect ‚Äî only 0.01 percentage points.\nThese differences are statistically very small and are unlikely to be meaningful in practice.\n\n Conclusion \nOur analysis shows that:\n\nOnce a match is introduced, increasing the match ratio does not meaningfully increase the likelihood of giving.\n\nThis confirms the finding from Karlan & List (2007):\n\n‚ÄúThe gift distributions across the various matching ratios are not significantly different from one another.‚Äù\n\nIn short, it‚Äôs the presence of a match offer ‚Äî not its generosity ‚Äî that influences donor behavior.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# Run a t-test on the amount given between treatment and control groups\namount_treat = df[df['treatment'] == 1]['amount']\namount_control = df[df['control'] == 1]['amount']\namount_ttest = ttest_ind(amount_treat, amount_control, equal_var=False)\n\n# Run a bivariate linear regression: amount ~ treatment\namount_reg = smf.ols('amount ~ treatment', data=df).fit()\n\n# amount_ttest.statistic, amount_ttest.pvalue, amount_reg.summary()\n\n# Format regression output into a clean table\namount_table = pd.DataFrame({\n    \"Variable\": amount_reg.params.index,\n    \"Coefficient\": amount_reg.params.values,\n    \"Std. Error\": amount_reg.bse.values,\n    \"p-value\": amount_reg.pvalues.values,\n    \"95% CI Lower\": amount_reg.conf_int()[0].values,\n    \"95% CI Upper\": amount_reg.conf_int()[1].values\n}).round(4)\n\namount_table\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n0.8133\n0.0674\n0.0000\n0.6811\n0.9454\n\n\n1\ntreatment\n0.1536\n0.0826\n0.0628\n-0.0082\n0.3154\n\n\n\n\n\n\n\n Size of Charitable Contribution \nI tested whether receiving a matching donation offer affects the amount donated using a t-test and linear regression:\n Results \n\n\n\n\n\n\n\n\n\nMethod\nTreatment Effect\np-value\nConclusion\n\n\n\n\nT-Test\n+$0.15\n0.055\nüî∏ Marginally not significant\n\n\nRegression\n+$0.15\n0.063\nüî∏ Suggestive but inconclusive\n\n\n\n\nControl group average: ~$0.81\n\nTreatment group average: ~$0.96\n\n Interpretation \n\nThe treatment group gave slightly more, but the difference is not statistically significant at the 5% level.\nThis suggests that while match offers increase participation, they have a much smaller effect on how much people give.\n\n Takeaway \n\nMatching donations may encourage more people to give, but do not substantially increase donation size.\n\n\n# Limit the data to only those who made a donation (amount &gt; 0)\ndf_positive = df[df['amount'] &gt; 0].copy()\n\n# T-test for amount among donors only\namount_treat_pos = df_positive[df_positive['treatment'] == 1]['amount']\namount_control_pos = df_positive[df_positive['control'] == 1]['amount']\namount_ttest_pos = ttest_ind(amount_treat_pos, amount_control_pos, equal_var=False)\n\n# Regression: amount ~ treatment (for donors only)\namount_reg_pos = smf.ols('amount ~ treatment', data=df_positive).fit()\n\n# amount_ttest_pos.statistic, amount_ttest_pos.pvalue, amount_reg_pos.summary()\n\n\n# Clean regression summary\namount_conditional_table = pd.DataFrame({\n    \"Variable\": amount_reg_pos.params.index,\n    \"Coefficient\": amount_reg_pos.params.values,\n    \"Std. Error\": amount_reg_pos.bse.values,\n    \"p-value\": amount_reg_pos.pvalues.values,\n    \"95% CI Lower\": amount_reg_pos.conf_int()[0].values,\n    \"95% CI Upper\": amount_reg_pos.conf_int()[1].values\n}).round(4)\n\namount_conditional_table\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n45.5403\n2.4234\n0.0000\n40.7850\n50.2956\n\n\n1\ntreatment\n-1.6684\n2.8724\n0.5615\n-7.3048\n3.9680\n\n\n\n\n\n\n\n Conditional Donation Amount: Among Donors Only \nTo isolate the effect of treatment on the amount given, I restricted the sample to only those individuals who made a donation (amount &gt; 0).\nI used both a t-test and a bivariate regression (amount ~ treatment) to compare average donation sizes between treatment and control groups.\n Results Summary \n\n\n\n\n\n\n\n\n\nMethod\nTreatment Effect\np-value\nConclusion\n\n\n\n\nT-Test (donors only)\nt = -0.58\n0.559\n‚ùå Not statistically significant\n\n\nRegression\n-$1.67\n0.561\n‚ùå Not statistically significant\n\n\n\n\nControl group average donation: ~$45.54\n\nTreatment group average donation: ~$43.87\n\n Interpretation \n\nThe treatment group donated slightly less on average, but the difference is not statistically meaningful.\nThis suggests that while the match offer encourages more people to donate, it does not increase donation size among those who would give anyway.\nBecause I only included those who donated, the treatment effect here is not causal ‚Äî it‚Äôs conditional and may suffer from selection bias.\n\n Conclusion \n\nMatched donations are effective at increasing the number of donors, but not the amount donated by each donor ‚Äî at least among those who already choose to give.\n\n\nimport matplotlib.pyplot as plt\n\n# Filter to donors only\ndf_donors = df[df[\"amount\"] &gt; 0]\n\n# Separate treatment and control donors\ntreat_donors = df_donors[df_donors[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = df_donors[df_donors[\"control\"] == 1][\"amount\"]\n\n# Calculate means\nmean_treat = treat_donors.mean()\nmean_control = control_donors.mean()\n\n# Create side-by-side histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color=\"skyblue\", edgecolor=\"black\")\naxes[0].axvline(mean_control, color=\"red\", linestyle=\"--\", label=f\"Mean = ${mean_control:.2f}\")\naxes[0].set_title(\"Control Group Donations\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\n# Treatment group plot\naxes[1].hist(treat_donors, bins=30, color=\"lightgreen\", edgecolor=\"black\")\naxes[1].axvline(mean_treat, color=\"red\", linestyle=\"--\", label=f\"Mean = ${mean_treat:.2f}\")\naxes[1].set_title(\"Treatment Group Donations\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Distribution of Donation Amounts Among Donors \nI am now focusing on individuals who actually made a donation (amount &gt; 0) to analyze how much they gave, and whether the treatment group (those offered a matching donation) gave more than the control group.\nI visualized the distribution of donation amounts with two histograms ‚Äî one for each group ‚Äî and include a red dashed line indicating the average donation in each.\n Interpretation \n\nBoth distributions are heavily right-skewed, which is common in charitable giving: most donors give modest amounts, but a few give significantly more.\nThe average donation in the control group was about $45.54, while the treatment group averaged $43.87.\nThis difference is not statistically significant, as confirmed by both a t-test and a regression limited to donors.\n\n What Did I Learn? \n\nWhile the matching donation offer increases the probability of donating, it does not increase the donation amount among those who choose to give.\nIn fact, the average donation in the treatment group is slightly lower, though the difference is not meaningful.\n\n Important Caveat \nThis analysis is based only on people who gave, so the treatment coefficient does not have a causal interpretation here. This subset is not randomly assigned ‚Äî it‚Äôs a selected group, which may differ systematically between treatment and control.\n Fundraising Implication \n\nMatching offers are powerful tools to increase participation, but they do not necessarily lead to larger individual gifts.\n\nTo increase average donation size, fundraisers may need additional tactics ‚Äî such as suggested donation levels, tiered match thresholds, or social proof."
  },
  {
    "objectID": "projects/karlan_list_2007_replication.html#simulation-experiment",
    "href": "projects/karlan_list_2007_replication.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Extract donation amounts for control and treatment\ncontrol_data = df[df[\"control\"] == 1][\"amount\"]\ntreatment_data = df[df[\"treatment\"] == 1][\"amount\"]\n\n# Simulate draws from each distribution\nnp.random.seed(42)\nsim_control = np.random.choice(control_data, size=100_000, replace=True)\nsim_treatment = np.random.choice(treatment_data, size=10_000, replace=True)\n\n# Calculate 10,000 differences between treatment and control draws\nsim_control_subset = np.random.choice(sim_control, size=10_000, replace=False)\ndiffs = sim_treatment - sim_control_subset\n\n# Compute cumulative average of differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Average Difference\")\nplt.axhline(y=np.mean(treatment_data) - np.mean(control_data), color=\"red\", linestyle=\"--\", label=\"True Mean Difference\")\nplt.title(\"Cumulative Average of Treatment-Control Differences\")\nplt.xlabel(\"Number of Draws\")\nplt.ylabel(\"Cumulative Average Difference in Donation Amount\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Simulated Cumulative Average Differences \nTo better understand the behavior of sample averages and connect to the concepts from our first class (Slide 43), we simulate the cumulative effect of donation differences between the treatment and control groups.\n Simulation Setup \n\nI simulated 100,000 random draws from the control group donation distribution.\nI simulated 10,000 random draws from the treatment group.\nFor each of the 10,000 pairs, I calculated the difference: treatment - control.\nI then computed the cumulative average of these 10,000 differences.\n\n Plot Interpretation \nThe plot below shows:\n\nA blue line representing the cumulative average of the simulated differences.\nA red dashed line indicating the true difference in means between treatment and control groups (calculated from the full dataset).\n\nAs the number of draws increases, the cumulative average approaches the true difference.\nThis illustrates the Law of Large Numbers: with enough data, sample-based estimates converge to the population value.\n What I Learnt \n\nThis simulation confirms that even in noisy, skewed data like donations, repeated sampling yields reliable estimates.\n\nIt also demonstrates that the difference in means I computed from data is not just a fluke ‚Äî it‚Äôs what we‚Äôd expect if I sampled repeatedly from the same distributions.\n\n\nCentral Limit Theorem\n\n# Define a function to simulate mean differences for a given sample size\ndef simulate_differences(sample_size, n_reps=1000):\n    differences = []\n    for _ in range(n_reps):\n        sample_control = np.random.choice(control_data, size=sample_size, replace=True)\n        sample_treatment = np.random.choice(treatment_data, size=sample_size, replace=True)\n        differences.append(np.mean(sample_treatment) - np.mean(sample_control))\n    return differences\n\n# Simulate for each sample size\nnp.random.seed(42)\nsizes = [50, 200, 500, 1000]\nsimulated_results = {size: simulate_differences(size) for size in sizes}\n\n# Plot histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, size in enumerate(sizes):\n    axes[i].hist(simulated_results[size], bins=30, color='lightgray', edgecolor='black')\n    axes[i].axvline(0, color='red', linestyle='--', label=\"Zero\")\n    axes[i].set_title(f\"Sample Size = {size}\")\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Sampling Distributions at Different Sample Sizes \nTo mirror the exercise from Slide 44 of our first class, I simulated the sampling distribution of the mean difference in donation amount between the treatment and control groups.\nFor each of four different sample sizes ‚Äî 50, 200, 500, and 1000 ‚Äî I:\n\nDrew n observations from each group.\nComputed the difference in mean donation: treatment - control.\nRepeated the process 1,000 times.\nPlotted the histogram of those 1,000 average differences.\n\n Histograms of Simulated Mean Differences \nEach plot includes a red dashed line at zero, representing the null hypothesis of no effect.\n Interpretation by Sample Size \n\nn = 50: The distribution is wide and noisy. Zero is near the center, meaning we can‚Äôt confidently detect an effect.\nn = 200: The distribution begins to narrow. Zero is still well within the range of plausible outcomes.\nn = 500: The histogram becomes more concentrated. The true effect begins to emerge, and zero starts shifting toward the tails.\nn = 1000: The distribution is tightly centered. Zero lies in the tail, indicating that the true average difference is likely not zero.\n\n Conclusion \n\nAs sample size increases, the sampling distribution of the mean difference becomes narrower and more centered around the true population effect.\n\nThis exercise demonstrates: - The Law of Large Numbers: larger samples produce more stable estimates. - The power of simulation for understanding uncertainty and inference. - Why small samples often yield inconclusive or misleading results.\nThese plots reinforce that while we may see noisy or overlapping outcomes in small samples, with enough data, we get closer to the truth."
  }
]